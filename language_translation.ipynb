{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Translation Model \n",
    "\n",
    "This notebook attempts to perform an effective language translation from Hindi to English language<br/>\n",
    "The dataset being used in this project is the IIT Bombay Hindi English Corpus which has been provided for free use on the internet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we would start with the imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now go ahead and import the required datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration cfilt--iitb-english-hindi-4e9610d2608dd062\n",
      "Reusing dataset parquet (C:\\Users\\parth\\.cache\\huggingface\\datasets\\parquet\\cfilt--iitb-english-hindi-4e9610d2608dd062\\0.0.0\\0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
      "100%|██████████| 3/3 [00:00<00:00, 13.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2507\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 520\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 1659083\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "corpus_data = load_dataset('cfilt/iitb-english-hindi')\n",
    "print(corpus_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the corpus has the train, test and validation sets prepared by default<br/>\n",
    "We can now go ahead and store them individually for further processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = corpus_data[\"train\"][\"translation\"]\n",
    "test_data = corpus_data[\"test\"][\"translation\"]\n",
    "validation_data = corpus_data[\"validation\"][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_en = [train_datum['en'] for train_datum in train_data]\n",
    "train_data_hi = [train_datum['hi'] for train_datum in train_data]\n",
    "\n",
    "test_data_en = [test_datum['en'] for test_datum in test_data]\n",
    "test_data_hi = [test_datum['hi'] for test_datum in test_data]\n",
    "\n",
    "validation_data_en = [validation_datum['en'] for validation_datum in validation_data]\n",
    "validation_data_hi = [validation_datum['hi'] for validation_datum in validation_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Text preprocessing on the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try to perform preprocessing on both English and Hindi sentences.<br/>\n",
    "This includes removing unwanted characters, URLs and numbers from the sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' is the number  website on this planet'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def purge_unwanted_characters(data):\n",
    "    \"\"\"To remove the unwanted characters from the input data\"\"\"\n",
    "\n",
    "    #Removing URLs with a regular expression\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    data = url_pattern.sub(r'', data)\n",
    "    \n",
    "    # Remove Emails\n",
    "    data = re.sub(r'\\S*@\\S*\\s?', '', data)\n",
    "    \n",
    "    # Remove new line characters\n",
    "    data = re.sub(r'\\s+', ' ', data)\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = re.sub(r\"\\'\", \"\", data)\n",
    "\n",
    "    # Remove numbers from text \n",
    "    data = re.sub(r'\\d', '', data)\n",
    "\n",
    "    # Remove underscores and other special characters from text \n",
    "    data = re.sub(r'[_#$%]', '', data)\n",
    "        \n",
    "    return data\n",
    "\n",
    "string = \"www.example.com 'is' the number 1_ website on this planet\"\n",
    "purge_unwanted_characters(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first proceed to perform preprocessing on English sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_english(data):\n",
    "    processed_data = []\n",
    "    for sentence in data: \n",
    "        processed_data.append(purge_unwanted_characters(sentence).lower())\n",
    "    return processed_data\n",
    "\n",
    "train_data_en = preprocess_english(train_data_en)\n",
    "test_data_en = preprocess_english(test_data_en)\n",
    "validation_data_en = preprocess_english(validation_data_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will proceed onto preprocessing of Hindi sentences. Note that here we will be trying to not only remove the unwanted characters as mentioned earlier, we will also try to remove english characters as they won't help the model in training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_hindi(data):\n",
    "    processed_data = []\n",
    "    for sentence in data:\n",
    "        processed_sentence = purge_unwanted_characters(sentence)\n",
    "        processed_sentence.replace(\"।\", '') # remove the hindi full stop (purn viram)\n",
    "        processed_data.append(re.sub(r'[a-zA-Z]', '', processed_sentence))\n",
    "    return processed_data\n",
    "\n",
    "train_data_hi = preprocess_hindi(train_data_hi)\n",
    "test_data_hi = preprocess_hindi(test_data_hi)\n",
    "validation_data_hi = preprocess_hindi(validation_data_hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will move onto the word embedding part for which we will be using FastText embeddings<br/>\n",
    "This is because fasttext offers embeddings for both english and hindi languages.<br/>\n",
    "Using this would be better as the vectors carry more meaning as compared to the default implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors \n",
    "\n",
    "english_embeddings = KeyedVectors.load_word2vec_format('cc.en.300.vec')\n",
    "hindi_embeddings = KeyedVectors.load_word2vec_format('cc.hi.300.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_for_english(text):\n",
    "    text_embedding = []\n",
    "    oov_count = 0\n",
    "    for sentence in text:\n",
    "        sentence_embedding = []\n",
    "        for word in sentence.split():\n",
    "            try:\n",
    "                sentence_embedding.append(english_embeddings.word_vec(word))\n",
    "            except KeyError:\n",
    "                sentence_embedding.append(np.zeros(300,)) # OOV words\n",
    "                oov_count += 1\n",
    "        text_embedding.append(sentence_embedding)\n",
    "    return text_embedding, oov_count\n",
    "\n",
    "def embeddings_for_hindi(text):\n",
    "    text_embedding = []\n",
    "    for sentence in text:\n",
    "        sentence_embedding = []\n",
    "        for word in sentence.split():\n",
    "            try:\n",
    "                sentence_embedding.append(hindi_embeddings.word_vec(word))\n",
    "            except KeyError:\n",
    "                sentence_embedding.append(np.zeros(300,)) # OOV words \n",
    "                oov_count += 1\n",
    "        text_embedding.append(sentence_embedding)\n",
    "    return text_embedding, oov_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parth\\AppData\\Local\\Temp/ipykernel_8432/1736097581.py:8: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  sentence_embedding.append(english_embeddings.word_vec(word))\n"
     ]
    }
   ],
   "source": [
    "en_train_data_embeddings, en_train_oov_count = embeddings_for_english(train_data_en)\n",
    "en_test_data_embeddings, en_test_oov_count = embeddings_for_english(test_data_en)\n",
    "en_validation_data_embeddings, en_validation_oov_count = embeddings_for_english(validation_data_en)\n",
    "\n",
    "hi_train_data_embeddings, hi_train_oov_count = embeddings_for_hindi(train_data_hi)\n",
    "hi_test_data_embeddings, hi_test_oov_count = embeddings_for_hindi(test_data_hi)\n",
    "hi_validation_data_embeddings, hi_validation_oov_count = embeddings_for_hindi(validation_data_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'en_train_data_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8432/248617866.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0msent_embed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0men_train_data_embeddings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword_embed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent_embed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'en_train_data_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for sent_embed in en_train_data_embeddings:\n",
    "    for word_embed in sent_embed: \n",
    "        count += 1\n",
    "\n",
    "print(\"English train data OOV token proportion: \", en_train_oov_count/count)\n",
    "\n",
    "count = 0\n",
    "for sent_embed in en_test_data_embeddings:\n",
    "    for word_embed in sent_embed: \n",
    "        count += 1\n",
    "\n",
    "print(\"English test data OOV token proportion: \", en_test_oov_count/count)\n",
    "\n",
    "count = 0\n",
    "for sent_embed in en_validation_data_embeddings:\n",
    "    for word_embed in sent_embed: \n",
    "        count += 1\n",
    "\n",
    "print(\"English validation data OOV token proportion: \", en_validation_oov_count/count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count = 0\n",
    "for sent_embed in en_train_data_embeddings:\n",
    "    for word_embed in sent_embed: \n",
    "        count += 1\n",
    "\n",
    "print(\"Hindi train data OOV token proportion: \", hi_train_oov_count/count)\n",
    "\n",
    "count = 0\n",
    "for sent_embed in en_train_data_embeddings:\n",
    "    for word_embed in sent_embed: \n",
    "        count += 1\n",
    "\n",
    "print(\"Hindi test data OOV token proportion: \", hi_test_oov_count/count)\n",
    "\n",
    "count = 0\n",
    "for sent_embed in en_train_data_embeddings:\n",
    "    for word_embed in sent_embed: \n",
    "        count += 1\n",
    "\n",
    "print(\"Hindi validation data OOV token proportion: \", hi_validation_oov_count/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data preprocessing done, we should now proceed towards building the transformer which will perform the language translation task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
