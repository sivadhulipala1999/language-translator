{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Translation Model \n",
    "\n",
    "This notebook attempts to perform an effective language translation from Hindi to English language<br/>\n",
    "The dataset being used in this project is the IIT Bombay Hindi English Corpus which has been provided for free use on the internet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we would start with the imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now go ahead and import the required datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration cfilt--iitb-english-hindi-911387c6837f8b91\n",
      "Reusing dataset parquet (C:\\Users\\parth\\.cache\\huggingface\\datasets\\parquet\\cfilt--iitb-english-hindi-911387c6837f8b91\\0.0.0\\0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
      "100%|██████████| 3/3 [00:00<00:00, 13.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 1659083\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2507\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 520\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "corpus_data = load_dataset('cfilt/iitb-english-hindi')\n",
    "print(corpus_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the corpus has the train, test and validation sets prepared by default<br/>\n",
    "We can now go ahead and store them individually for further processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = corpus_data[\"train\"][\"translation\"]\n",
    "test_data = corpus_data[\"test\"][\"translation\"]\n",
    "validation_data = corpus_data[\"validation\"][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_en = [train_datum['en'] for train_datum in train_data]\n",
    "train_data_hi = [train_datum['hi'] for train_datum in train_data]\n",
    "\n",
    "test_data_en = [test_datum['en'] for test_datum in test_data]\n",
    "test_data_hi = [test_datum['hi'] for test_datum in test_data]\n",
    "\n",
    "validation_data_en = [validation_datum['en'] for validation_datum in validation_data]\n",
    "validation_data_hi = [validation_datum['hi'] for validation_datum in validation_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Text preprocessing on the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try to perform preprocessing on both English and Hindi sentences.<br/>\n",
    "This includes\n",
    "<ul>\n",
    "    <li>Removing sentences whose length is more than a defined value</li>\n",
    "    <li>Removing unwanted characters from the remaining sentences</li>\n",
    "    <li>Converting the sentences to a sequence of numbers (embeddings) so that they can be fed to the model</li>\n",
    "    <li>Padding the sequences so that they are all of uniform length</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 30\n",
    "\n",
    "en_sent_list = []\n",
    "for en_sent in train_data_en:\n",
    "    if len(en_sent) <= maxlen:\n",
    "        en_sent_list.append(en_sent)\n",
    "train_data_en = en_sent_list\n",
    "en_sent_list = []\n",
    "for en_sent in test_data_en:\n",
    "    if len(en_sent) <= maxlen:\n",
    "        en_sent_list.append(en_sent)\n",
    "test_data_en = en_sent_list\n",
    "en_sent_list = []\n",
    "for en_sent in validation_data_en:\n",
    "    if len(en_sent) <= maxlen:\n",
    "        en_sent_list.append(en_sent)\n",
    "validation_data_en = en_sent_list\n",
    "\n",
    "\n",
    "hi_sent_list = []\n",
    "for hi_sent in train_data_hi:\n",
    "    if len(hi_sent) <= maxlen:\n",
    "        hi_sent_list.append(hi_sent)\n",
    "train_data_hi = hi_sent_list\n",
    "hi_sent_list = []\n",
    "for hi_sent in test_data_hi:\n",
    "    if len(hi_sent) <= maxlen:\n",
    "        hi_sent_list.append(hi_sent)\n",
    "test_data_hi = hi_sent_list\n",
    "hi_sent_list = []\n",
    "for hi_sent in validation_data_hi:\n",
    "    if len(hi_sent) <= maxlen:\n",
    "        hi_sent_list.append(hi_sent)\n",
    "validation_data_hi = hi_sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' is the number  website on this planet'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def purge_unwanted_characters(data):\n",
    "    \"\"\"To remove the unwanted characters from the input data\"\"\"\n",
    "\n",
    "    #Removing URLs with a regular expression\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    data = url_pattern.sub(r'', data)\n",
    "    \n",
    "    # Remove Emails\n",
    "    data = re.sub(r'\\S*@\\S*\\s?', '', data)\n",
    "    \n",
    "    # Remove new line characters\n",
    "    data = re.sub(r'\\s+', ' ', data)\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = re.sub(r\"\\'\", \"\", data)\n",
    "\n",
    "    # Remove numbers from text \n",
    "    data = re.sub(r'\\d', '', data)\n",
    "\n",
    "    # Remove underscores and other special characters from text \n",
    "    data = re.sub(r'[_#$%]', '', data)\n",
    "        \n",
    "    return data\n",
    "\n",
    "string = \"www.example.com 'is' the number 1_ website on this planet\"\n",
    "purge_unwanted_characters(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first proceed to perform preprocessing on English sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_english(data):\n",
    "    processed_data = []\n",
    "    for sentence in data: \n",
    "        processed_data.append(purge_unwanted_characters(sentence).lower())\n",
    "    return processed_data\n",
    "\n",
    "train_data_en = preprocess_english(train_data_en)\n",
    "test_data_en = preprocess_english(test_data_en)\n",
    "validation_data_en = preprocess_english(validation_data_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will proceed onto preprocessing of Hindi sentences. Note that here we will be trying to not only remove the unwanted characters as mentioned earlier, we will also try to remove english characters as they won't help the model in training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_hindi(data):\n",
    "    processed_data = []\n",
    "    for sentence in data:\n",
    "        processed_sentence = purge_unwanted_characters(sentence)\n",
    "        processed_sentence.replace(\"।\", '') # remove the hindi full stop (purn viram)\n",
    "        processed_data.append(re.sub(r'[a-zA-Z]', '', processed_sentence))\n",
    "    return processed_data\n",
    "\n",
    "train_data_hi = preprocess_hindi(train_data_hi)\n",
    "test_data_hi = preprocess_hindi(test_data_hi)\n",
    "validation_data_hi = preprocess_hindi(validation_data_hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will move onto the word embedding part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the texts and convert to sequences\n",
    "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', lower=False)\n",
    "en_tokenizer.fit_on_texts(train_data_en)\n",
    "en_train_sequences = en_tokenizer.texts_to_sequences(train_data_en)\n",
    "\n",
    "hi_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', lower=False)\n",
    "hi_tokenizer.fit_on_texts(train_data_hi)\n",
    "hi_train_sequences = hi_tokenizer.texts_to_sequences(train_data_hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do the same for test and validation datasets.<br/>\n",
    "After creating the embeddings for all data, we are going to fetch the final size of the vocabulary which can be fetched from the tokenizer object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size of english data:  107958\n",
      "Vocab size of hindi data:  107958\n"
     ]
    }
   ],
   "source": [
    "en_tokenizer.fit_on_texts(test_data_en)\n",
    "en_test_sequences = en_tokenizer.texts_to_sequences(test_data_en)\n",
    "en_tokenizer.fit_on_texts(validation_data_en)\n",
    "en_validation_sequences = en_tokenizer.texts_to_sequences(validation_data_en)\n",
    "\n",
    "hi_tokenizer.fit_on_texts(test_data_hi)\n",
    "hi_test_sequences = hi_tokenizer.texts_to_sequences(test_data_hi)\n",
    "hi_tokenizer.fit_on_texts(validation_data_hi)\n",
    "hi_validation_sequences = hi_tokenizer.texts_to_sequences(validation_data_hi)\n",
    "\n",
    "\n",
    "english_vocab_size = len(en_tokenizer.word_index) + 1\n",
    "hindi_vocab_size = len(en_tokenizer.word_index) + 1\n",
    "\n",
    "print(\"Vocab size of english data: \", english_vocab_size)\n",
    "print(\"Vocab size of hindi data: \", hindi_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple look at one of the embeddings will help us understand how the embedding was done by keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[670, 1303]\n",
      "highlight duration\n"
     ]
    }
   ],
   "source": [
    "print(en_train_sequences[0])\n",
    "print(train_data_en[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pad all the sequences so that they are all of uniform length<br/>\n",
    "Note that we are using English sentences as inputs to the encoder part of our transformer model and the hindi ones as both the outputs and inputs for the decoder of the transformer<br/>\n",
    "The latter is because we need the translated output from the decoder and we need to train it with the target language sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad english sentences for using them as encoder inputs  \n",
    "en_train_sequences = tf.keras.preprocessing.sequence.pad_sequences(en_train_sequences, maxlen=maxlen, padding='post')\n",
    "en_test_sequences = tf.keras.preprocessing.sequence.pad_sequences(en_test_sequences, maxlen=maxlen, padding='post')\n",
    "en_validation_sequences = tf.keras.preprocessing.sequence.pad_sequences(en_validation_sequences, maxlen=maxlen, padding='post')\n",
    "\n",
    "#Pad hindi sentences for using them as decoder outputs and inputs \n",
    "decoder_inputs_train = []\n",
    "decoder_outputs_train = []\n",
    "\n",
    "# remove the last token in input and first token in the output. This is because, decoder should be trained such that for every input token\n",
    "# the next token in sequence should be presented as the output. The training will be done accordingly.\n",
    "for hi in hi_train_sequences:\n",
    "  decoder_inputs_train.append(hi[:-1]) \n",
    "  decoder_outputs_train.append(hi[1:])\n",
    "\n",
    "decoder_inputs_train = tf.keras.preprocessing.sequence.pad_sequences(decoder_inputs_train, maxlen=maxlen, padding='post')\n",
    "decoder_outputs_train = tf.keras.preprocessing.sequence.pad_sequences(decoder_outputs_train, maxlen=maxlen, padding='post')\n",
    "\n",
    "\n",
    "decoder_inputs_test = []\n",
    "decoder_outputs_test = []\n",
    "\n",
    "for hi in hi_test_sequences:\n",
    "  decoder_inputs_test.append(hi[:-1])\n",
    "  decoder_outputs_test.append(hi[1:])\n",
    "\n",
    "decoder_inputs_test = tf.keras.preprocessing.sequence.pad_sequences(decoder_inputs_test, maxlen=maxlen, padding='post')\n",
    "decoder_outputs_test = tf.keras.preprocessing.sequence.pad_sequences(decoder_outputs_test, maxlen=maxlen, padding='post')\n",
    "\n",
    "decoder_inputs_validation = []\n",
    "decoder_outputs_validation = []\n",
    "\n",
    "for hi in hi_validation_sequences:\n",
    "  decoder_inputs_validation.append(hi[:-1])\n",
    "  decoder_outputs_validation.append(hi[1:])\n",
    "\n",
    "decoder_inputs_validation = tf.keras.preprocessing.sequence.pad_sequences(decoder_inputs_validation, maxlen=maxlen, padding='post')\n",
    "decoder_outputs_validation = tf.keras.preprocessing.sequence.pad_sequences(decoder_outputs_validation, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence:  highlight duration\n",
      "Encoding:  [ 670 1303    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(\"English sentence: \", train_data_en[0])\n",
    "print(\"Encoding: \", en_train_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data preprocessing done, we should now proceed towards building the transformer which will perform the language translation task<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us prepare the embedding layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add the positional encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch \n",
    "from torch.autograd import Variable\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(0)\n",
    "        \n",
    "        x = x + Variable(self.pe[:seq_len], requires_grad=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 80])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = torch.zeros(80, 512)\n",
    "game.transpose(0, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi head attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        \n",
    "        k = self.k_linear(k).view(batch_size, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "\n",
    "        # calculate attention using function we will define next\n",
    "        scores = self.attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output\n",
    "    \n",
    "    def attention(self, q, k, v, d_k, mask=None, dropout=None):\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "\n",
    "        # if mask is not None:\n",
    "        #     mask = mask.unsqueeze(1)\n",
    "        #     scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "\n",
    "        output = torch.matmul(scores, v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various masks are created here \n",
    "\n",
    "<ul>\n",
    "<li>Padding mask for English</li>\n",
    "<li>Padding mask for Hindi</li>\n",
    "<li>No peek mask for masking future tokens in order to avoid decoder from seeing the inputs early</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 30, 30])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = maxlen\n",
    "\n",
    "def no_peek_mask():\n",
    "    nopeek_mask = np.triu(np.ones((1, size, size)), k=1).astype('uint8')\n",
    "    nopeek_mask = torch.from_numpy(nopeek_mask) == 0\n",
    "\n",
    "    return nopeek_mask\n",
    "\n",
    "def padding_mask(text):\n",
    "    pad_mask = text != 0\n",
    "    return pad_mask\n",
    "\n",
    "def create_masks(src_text, target_text):\n",
    "    return padding_mask(src_text), no_peek_mask() & padding_mask(target_text)\n",
    "\n",
    "\n",
    "no_peek_mask().unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True))/(x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# build an encoder layer with one multi-head attention layer and one feed-forward layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x\n",
    "    \n",
    "# build a decoder layer with two multi-head attention layers and one feed-forward layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
    "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "\n",
    "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "            x2 = self.norm_1(x)\n",
    "            x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "            x2 = self.norm_2(x)\n",
    "            x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, src_mask))\n",
    "            x2 = self.norm_3(x)\n",
    "            x = x + self.dropout_3(self.ff(x2))\n",
    "            return x\n",
    "# We can then build a convenient cloning function that can generate multiple layers:\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
    "        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        e_outputs = self.encoder.forward(src, src_mask)\n",
    "        d_output = self.decoder.forward(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "heads = 8\n",
    "N = 2\n",
    "\n",
    "#Save model after each epoch\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"best_model1.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto', save_weights_only=False)\n",
    "\n",
    "model = Transformer(english_vocab_size, hindi_vocab_size, d_model, N, heads)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "    \n",
    "# this code is very important! It initialises the parameters with a\n",
    "# range of values that stops the signal fading or getting too big.\n",
    "# See this blog for a mathematical explanation.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([900, 107958]) torch.Size([30])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (900) to match target batch_size (30).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14192/2275413178.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14192/2275413178.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(epochs, print_every)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_lang_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_lang_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   2994\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2995\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2996\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (900) to match target batch_size (30)."
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "def train_model(epochs, print_every=100):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    start = time.time()\n",
    "    temp = start\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for en_train_sequence, target_lang_input, target_lang_output in zip(en_train_sequences, decoder_inputs_train, decoder_outputs_train):\n",
    "            source = torch.from_numpy(en_train_sequence)\n",
    "            target_lang_input = torch.from_numpy(target_lang_input)\n",
    "            target_lang_output = torch.from_numpy(target_lang_output)\n",
    "            \n",
    "            # create function to make masks using mask code above\n",
    "            src_mask, trg_mask = create_masks(source, target_lang_input)\n",
    "            \n",
    "            preds = model(source, target_lang_input, src_mask, trg_mask)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            print(preds.view(-1, preds.size(-1)).shape, target_lang_output.shape)\n",
    "\n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), target_lang_output, ignore_index=0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data[0]\n",
    "            if (epoch + 1) % print_every == 0:\n",
    "                loss_avg = total_loss / print_every\n",
    "                print(\"time = %dm, epoch %d, loss = %.3f, %ds per %d iters\" % ((time.time() - start) // 60,\n",
    "                epoch + 1, loss_avg, time.time() - temp, print_every))\n",
    "                total_loss = 0\n",
    "                temp = time.time()\n",
    "\n",
    "train_model(1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
