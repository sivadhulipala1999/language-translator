{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Translation Model \n",
    "\n",
    "This notebook attempts to perform an effective language translation from Hindi to English language<br/>\n",
    "The dataset being used in this project is the IIT Bombay Hindi English Corpus which has been provided for free use on the internet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we would start with the imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now go ahead and import the required datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration cfilt--iitb-english-hindi-911387c6837f8b91\n",
      "Reusing dataset parquet (C:\\Users\\parth\\.cache\\huggingface\\datasets\\cfilt___parquet\\cfilt--iitb-english-hindi-911387c6837f8b91\\0.0.0\\7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n",
      "100%|██████████| 3/3 [00:00<00:00,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 1659083\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2507\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 520\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "corpus_data = load_dataset('cfilt/iitb-english-hindi')\n",
    "print(corpus_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the corpus has the train, test and validation sets prepared by default<br/>\n",
    "We can now go ahead and store them individually for further processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = corpus_data[\"train\"][\"translation\"]\n",
    "test_data = corpus_data[\"test\"][\"translation\"]\n",
    "validation_data = corpus_data[\"validation\"][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[:len(train_data)//100]\n",
    "test_data = test_data[:len(test_data)//100]\n",
    "validation_data = validation_data[:len(validation_data)//100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_en = [train_datum['en'] for train_datum in train_data]\n",
    "train_data_hi = [train_datum['hi'] for train_datum in train_data]\n",
    "\n",
    "test_data_en = [test_datum['en'] for test_datum in test_data]\n",
    "test_data_hi = [test_datum['hi'] for test_datum in test_data]\n",
    "\n",
    "validation_data_en = [validation_datum['en'] for validation_datum in validation_data]\n",
    "validation_data_hi = [validation_datum['hi'] for validation_datum in validation_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Text preprocessing on the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try to perform preprocessing on both English and Hindi sentences.<br/>\n",
    "This includes\n",
    "<ul>\n",
    "    <li>Removing sentences whose length is more than a defined value</li>\n",
    "    <li>Removing unwanted characters from the remaining sentences</li>\n",
    "    <li>Converting the sentences to a sequence of numbers (embeddings) so that they can be fed to the model</li>\n",
    "    <li>Padding the sequences so that they are all of uniform length</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first remove unwanted characters like URL characters, new line characters, single quotes, numbers etc. from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' is the number  website on this planet'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def purge_unwanted_characters(data):\n",
    "    \"\"\"To remove the unwanted characters from the input data\"\"\"\n",
    "\n",
    "    #Removing URLs with a regular expression\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    data = url_pattern.sub(r'', data)\n",
    "    \n",
    "    # Remove Emails\n",
    "    data = re.sub(r'\\S*@\\S*\\s?', '', data)\n",
    "    \n",
    "    # Remove new line characters\n",
    "    data = re.sub(r'\\s+', ' ', data)\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = re.sub(r\"\\'\", \"\", data)\n",
    "\n",
    "    # Remove numbers from text \n",
    "    data = re.sub(r'\\d', '', data)\n",
    "\n",
    "    # Remove underscores and other special characters from text \n",
    "    data = re.sub(r'[_#$%]', '', data)\n",
    "        \n",
    "    return data\n",
    "\n",
    "string = \"www.example.com 'is' the number 1_ website on this planet\"\n",
    "purge_unwanted_characters(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_english(data):\n",
    "    processed_data = []\n",
    "    for sentence in data: \n",
    "        processed_data.append(purge_unwanted_characters(sentence).lower())\n",
    "    return processed_data\n",
    "\n",
    "train_data_en = preprocess_english(train_data_en)\n",
    "test_data_en = preprocess_english(test_data_en)\n",
    "validation_data_en = preprocess_english(validation_data_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will proceed onto preprocessing of Hindi sentences. Note that here we will be trying to not only remove the unwanted characters as mentioned earlier, we will also try to remove english characters as they won't help the model in training. <br/>\n",
    "Also note that we will be adding the 'START' and 'END' tokens to each hindi sentence to make sure that the decoder knows about the start and end of the sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_hindi(data):\n",
    "    processed_data = []\n",
    "    for sentence in data:\n",
    "        processed_sentence = purge_unwanted_characters(sentence)\n",
    "        processed_sentence.replace(\"।\", '') # remove the hindi full stop (purn viram)\n",
    "        processed_sentence = re.sub(r'[a-zA-Z]', '', processed_sentence)\n",
    "        processed_sentence = '<START> ' + processed_sentence + ' <END>'\n",
    "        processed_data.append(processed_sentence)\n",
    "    return processed_data\n",
    "\n",
    "train_data_hi = preprocess_hindi(train_data_hi)\n",
    "test_data_hi = preprocess_hindi(test_data_hi)\n",
    "validation_data_hi = preprocess_hindi(validation_data_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START> अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें <END>'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_hi[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_train = train_data_en\n",
    "out_train = train_data_hi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give your application an accessibility workout <START> अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें <END>\n",
      "accerciser accessibility explorer <START> एक्सेर्साइसर पहुंचनीयता अन्वेषक <END>\n",
      "the default plugin layout for the bottom panel <START> निचले पटल के लिए डिफोल्ट प्लग-इन खाका <END>\n",
      "the default plugin layout for the top panel <START> ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका <END>\n",
      "a list of plugins that are disabled by default <START> उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से निष्क्रिय किया गया है <END>\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(in_train[i], out_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', lower=False)\n",
    "hi_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', lower=False)\n",
    "\n",
    "en_tokenizer.fit_on_texts(in_train)\n",
    "in_train_s = en_tokenizer.texts_to_sequences(in_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_tokenizer.fit_on_texts(out_train)\n",
    "out_train_s = hi_tokenizer.texts_to_sequences(out_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready for the word embedding phase of preprocessing where we can convert our texts to sequences of integers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the texts and convert to sequences\n",
    "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', lower=False)\n",
    "en_tokenizer.fit_on_texts(train_data_en)\n",
    "en_train_sequences = en_tokenizer.texts_to_sequences(train_data_en)\n",
    "\n",
    "hi_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', lower=False)\n",
    "hi_tokenizer.fit_on_texts(train_data_hi)\n",
    "hi_train_sequences = hi_tokenizer.texts_to_sequences(train_data_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16590"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_train_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do the same for test and validation datasets.<br/>\n",
    "After creating the embeddings for all data, we are going to fetch the final size of the vocabulary which can be fetched from the tokenizer object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size of english data:  2881\n",
      "Vocab size of hindi data:  2881\n"
     ]
    }
   ],
   "source": [
    "en_tokenizer.fit_on_texts(test_data_en)\n",
    "en_test_sequences = en_tokenizer.texts_to_sequences(test_data_en)\n",
    "en_tokenizer.fit_on_texts(validation_data_en)\n",
    "en_validation_sequences = en_tokenizer.texts_to_sequences(validation_data_en)\n",
    "\n",
    "hi_tokenizer.fit_on_texts(test_data_hi)\n",
    "hi_test_sequences = hi_tokenizer.texts_to_sequences(test_data_hi)\n",
    "hi_tokenizer.fit_on_texts(validation_data_hi)\n",
    "hi_validation_sequences = hi_tokenizer.texts_to_sequences(validation_data_hi)\n",
    "\n",
    "\n",
    "english_vocab_size = len(en_tokenizer.word_index) + 1\n",
    "hindi_vocab_size = len(en_tokenizer.word_index) + 1\n",
    "\n",
    "print(\"Vocab size of english data: \", english_vocab_size)\n",
    "print(\"Vocab size of hindi data: \", hindi_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple look at one of the embeddings will help us understand how the embedding was done by keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[526, 106, 184, 27, 224, 991]\n",
      "give your application an accessibility workout\n"
     ]
    }
   ],
   "source": [
    "print(en_train_sequences[0])\n",
    "print(train_data_en[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pad all the sequences so that they are all of uniform length<br/>\n",
    "Note that we are using English sentences as inputs to the encoder part of our transformer model and the hindi ones as both the outputs and inputs for the decoder of the transformer<br/>\n",
    "The latter is because we need the translated output from the decoder and we need to train it with the target language sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad english sentences for using them as encoder inputs  \n",
    "maxlen = 200\n",
    "\n",
    "en_train_sequences = tf.keras.preprocessing.sequence.pad_sequences(en_train_sequences, maxlen=maxlen, padding='post')\n",
    "en_test_sequences = tf.keras.preprocessing.sequence.pad_sequences(en_test_sequences, maxlen=maxlen, padding='post')\n",
    "en_validation_sequences = tf.keras.preprocessing.sequence.pad_sequences(en_validation_sequences, maxlen=maxlen, padding='post')\n",
    "\n",
    "#Pad hindi sentences for using them as decoder outputs and inputs \n",
    "decoder_inputs_train = []\n",
    "decoder_outputs_train = []\n",
    "\n",
    "# remove the last token in input and first token in the output. This is because, decoder should be trained such that for every input token\n",
    "# the next token in sequence should be presented as the output. The training will be done accordingly.\n",
    "for hi in hi_train_sequences:\n",
    "  decoder_inputs_train.append(hi[:-1]) \n",
    "  decoder_outputs_train.append(hi[1:])\n",
    "\n",
    "decoder_inputs_train = tf.keras.preprocessing.sequence.pad_sequences(decoder_inputs_train, maxlen=maxlen, padding='post')\n",
    "decoder_outputs_train = tf.keras.preprocessing.sequence.pad_sequences(decoder_outputs_train, maxlen=maxlen, padding='post')\n",
    "\n",
    "\n",
    "decoder_inputs_test = []\n",
    "decoder_outputs_test = []\n",
    "\n",
    "for hi in hi_test_sequences:\n",
    "  decoder_inputs_test.append(hi[:-1])\n",
    "  decoder_outputs_test.append(hi[1:])\n",
    "\n",
    "decoder_inputs_test = tf.keras.preprocessing.sequence.pad_sequences(decoder_inputs_test, maxlen=maxlen, padding='post')\n",
    "decoder_outputs_test = tf.keras.preprocessing.sequence.pad_sequences(decoder_outputs_test, maxlen=maxlen, padding='post')\n",
    "\n",
    "decoder_inputs_validation = []\n",
    "decoder_outputs_validation = []\n",
    "\n",
    "for hi in hi_validation_sequences:\n",
    "  decoder_inputs_validation.append(hi[:-1])\n",
    "  decoder_outputs_validation.append(hi[1:])\n",
    "\n",
    "decoder_inputs_validation = tf.keras.preprocessing.sequence.pad_sequences(decoder_inputs_validation, maxlen=maxlen, padding='post')\n",
    "decoder_outputs_validation = tf.keras.preprocessing.sequence.pad_sequences(decoder_outputs_validation, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence:  give your application an accessibility workout\n",
      "Encoding:  [526 106 184  27 224 991   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(\"English sentence: \", train_data_en[0])\n",
    "print(\"Encoding: \", en_train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_23 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_24 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, None, 256)    737536      ['input_23[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, None, 256)    737536      ['input_24[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 256),        525312      ['embedding_2[0][0]']            \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 256),  525312      ['embedding_3[0][0]',            \n",
      "                                 (None, 256),                     'lstm_2[0][1]',                 \n",
      "                                 (None, 256)]                     'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, None, 2881)   740417      ['lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,266,113\n",
      "Trainable params: 3,266,113\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define LSTM model\n",
    "d_model = 256    #dimension of the internal vector representations (h,c and embedding vectors)\n",
    "\n",
    "#Encoder\n",
    "inputs = tf.keras.layers.Input(shape=(None,))\n",
    "x = tf.keras.layers.Embedding(english_vocab_size, d_model, mask_zero=True)(inputs)\n",
    "_, state_h, state_c = tf.keras.layers.LSTM(d_model,activation='relu',return_state=True)(x)\n",
    "\n",
    "#Decoder\n",
    "targets = tf.keras.layers.Input(shape=(None,))\n",
    "embedding_layer = tf.keras.layers.Embedding(hindi_vocab_size, d_model, mask_zero=True)\n",
    "x = embedding_layer(targets)\n",
    "decoder_lstm = tf.keras.layers.LSTM(d_model,activation='relu',return_sequences=True, return_state=True)\n",
    "x,_,_ = decoder_lstm(x, initial_state=[state_h, state_c])\n",
    "dense1 = tf.keras.layers.Dense(hindi_vocab_size, activation='softmax')\n",
    "x = dense1(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[inputs, targets],outputs=x)\n",
    "model.summary()\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "model.compile(optimizer='rmsprop', loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model after each epoch\n",
    "save_model_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='./en-hi.h5',\n",
    "    monitor='val_accuracy',\n",
    "    mode='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "519/519 [==============================] - 3308s 6s/step - loss: 0.1124 - accuracy: 0.2710\n",
      "Epoch 2/20\n",
      "519/519 [==============================] - 1159s 2s/step - loss: 0.0809 - accuracy: 0.3852\n",
      "Epoch 3/20\n",
      "519/519 [==============================] - 1111s 2s/step - loss: 10.1473 - accuracy: 0.4825\n",
      "Epoch 4/20\n",
      "519/519 [==============================] - 1119s 2s/step - loss: 0.0856 - accuracy: 0.5815\n",
      "Epoch 5/20\n",
      "519/519 [==============================] - 1123s 2s/step - loss: 0.1016 - accuracy: 0.6826\n",
      "Epoch 6/20\n",
      "519/519 [==============================] - 1176s 2s/step - loss: 0.0267 - accuracy: 0.7659\n",
      "Epoch 7/20\n",
      "519/519 [==============================] - 1202s 2s/step - loss: 0.0196 - accuracy: 0.8268\n",
      "Epoch 8/20\n",
      "519/519 [==============================] - 1166s 2s/step - loss: 0.0150 - accuracy: 0.8649\n",
      "Epoch 9/20\n",
      "519/519 [==============================] - 1179s 2s/step - loss: 0.0124 - accuracy: 0.8881\n",
      "Epoch 10/20\n",
      "519/519 [==============================] - 1154s 2s/step - loss: 0.0100 - accuracy: 0.9046\n",
      "Epoch 11/20\n",
      "519/519 [==============================] - 1139s 2s/step - loss: 0.0085 - accuracy: 0.9171\n",
      "Epoch 12/20\n",
      "519/519 [==============================] - 1140s 2s/step - loss: 0.0075 - accuracy: 0.9259\n",
      "Epoch 13/20\n",
      "519/519 [==============================] - 1127s 2s/step - loss: 0.0067 - accuracy: 0.9327\n",
      "Epoch 14/20\n",
      "519/519 [==============================] - 1133s 2s/step - loss: 0.0061 - accuracy: 0.9369\n",
      "Epoch 15/20\n",
      "519/519 [==============================] - 1131s 2s/step - loss: 0.0056 - accuracy: 0.9404\n",
      "Epoch 16/20\n",
      "519/519 [==============================] - 1120s 2s/step - loss: 0.0052 - accuracy: 0.9440\n",
      "Epoch 17/20\n",
      "519/519 [==============================] - 1124s 2s/step - loss: 0.0049 - accuracy: 0.9465\n",
      "Epoch 18/20\n",
      "519/519 [==============================] - 1128s 2s/step - loss: 0.0047 - accuracy: 0.9476\n",
      "Epoch 19/20\n",
      "519/519 [==============================] - 1128s 2s/step - loss: 0.0045 - accuracy: 0.9489\n",
      "Epoch 20/20\n",
      "519/519 [==============================] - 1205s 2s/step - loss: 0.0044 - accuracy: 0.9508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x244b2490460>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = [en_train_sequences, decoder_inputs_train]\n",
    "y_train = decoder_outputs_train\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, callbacks=[save_model_callback, tf.keras.callbacks.TerminateOnNaN()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_23 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_24 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, None, 256)    737536      ['input_23[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, None, 256)    737536      ['input_24[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 256),        525312      ['embedding_2[0][0]']            \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 256),  525312      ['embedding_3[0][0]',            \n",
      "                                 (None, 256),                     'lstm_2[0][1]',                 \n",
      "                                 (None, 256)]                     'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, None, 2881)   740417      ['lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,266,113\n",
      "Trainable params: 3,266,113\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Retrieve previously saved stuff\n",
    "saved_model = tf.keras.models.load_model('./en-hi.h5')\n",
    "\n",
    "saved_model.summary()\n",
    "\n",
    "inputs = saved_model.get_layer('input_23').output\n",
    "_,state_h,state_c = saved_model.get_layer('lstm_2').output\n",
    "targets = saved_model.get_layer('input_24').output\n",
    "embedding_layer = saved_model.get_layer('embedding_3')\n",
    "decoder_lstm = saved_model.get_layer('lstm_3')\n",
    "dense1 = saved_model.get_layer('dense_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference Model\n",
    "d_model=256\n",
    "\n",
    "#Encoder\n",
    "encoder = tf.keras.models.Model(inputs, [state_h, state_c])\n",
    "\n",
    "#Decoder\n",
    "decoder_input_h = tf.keras.layers.Input(shape=(d_model,))\n",
    "decoder_input_c = tf.keras.layers.Input(shape=(d_model,))\n",
    "x = embedding_layer(targets)\n",
    "x, decoder_output_h, decoder_output_c = decoder_lstm(x, initial_state=[decoder_input_h, decoder_input_c])\n",
    "x = dense1(x)\n",
    "decoder = tf.keras.models.Model([targets] + [decoder_input_h, decoder_input_c], [x] + [decoder_output_h, decoder_output_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(en_input):\n",
    "  input_seq = en_tokenizer.texts_to_sequences([en_input])\n",
    "\n",
    "  next_h, next_c = encoder.predict(input_seq)\n",
    "\n",
    "  curr_token = np.zeros((1,1))\n",
    "  curr_token[0] = hi_tokenizer.word_index['<START>']\n",
    "\n",
    "  print(\"current token: \", curr_token[0])\n",
    "  print(\"current token shape: \", curr_token.shape)\n",
    "\n",
    "  pred_sentence = ''\n",
    "\n",
    "  for i in range(maxlen):\n",
    "    output, next_h, next_c = decoder.predict([curr_token] + [next_h, next_c])\n",
    "    next_token = np.argmax(output[0, 0, :])\n",
    "    next_word = hi_tokenizer.index_word[next_token]\n",
    "    if next_word == '<END>':\n",
    "      break\n",
    "    else:\n",
    "      pred_sentence += ' ' + next_word\n",
    "      curr_token[0] = next_token\n",
    "\n",
    "  return pred_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "current token:  [2.]\n",
      "current token shape:  (1, 1)\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' ----. . . के . ) जोड़ा -----. . . (- ) ----वरीयता -----. . . (- ) ----वरीयता -----. --. . (- स्रोत जोड़ा ----. . . के . ) उत्पन्न'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ContextualVersionConflict",
     "evalue": "(google-auth 2.8.0 (d:\\data\\tutorials\\data science\\projects\\language-translator\\transformenv\\lib\\site-packages), Requirement.parse('google-auth<2.0dev,>=1.25.0'), {'google-api-core'})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mContextualVersionConflict\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Data\\tutorials\\Data Science\\Projects\\language-translator\\language_translation.ipynb Cell 39'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000060?line=0'>1</a>\u001b[0m \u001b[39m#Set up google translate as an additional reference\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000060?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloud\u001b[39;00m \u001b[39mimport\u001b[39;00m translate_v2 \u001b[39mas\u001b[39;00m translate\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000060?line=3'>4</a>\u001b[0m translate_client \u001b[39m=\u001b[39m translate\u001b[39m.\u001b[39mClient\u001b[39m.\u001b[39mfrom_service_account_json(\u001b[39m\"\u001b[39m\u001b[39m./drive/MyDrive/gcloud-auth-files/data-shard-330609-2669b6f2c900.json\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Data\\tutorials\\Data Science\\Projects\\language-translator\\transformenv\\lib\\site-packages\\google\\cloud\\translate_v2\\__init__.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Google Cloud Translation API wrapper.\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpkg_resources\u001b[39;00m \u001b[39mimport\u001b[39;00m get_distribution\n\u001b[1;32m---> 20\u001b[0m __version__ \u001b[39m=\u001b[39m get_distribution(\u001b[39m\"\u001b[39;49m\u001b[39mgoogle-cloud-translate\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mversion\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloud\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtranslate_v2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m Client\n\u001b[0;32m     25\u001b[0m __all__ \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m__version__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mClient\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Data\\tutorials\\Data Science\\Projects\\language-translator\\transformenv\\lib\\site-packages\\pkg_resources\\__init__.py:466\u001b[0m, in \u001b[0;36mget_distribution\u001b[1;34m(dist)\u001b[0m\n\u001b[0;32m    464\u001b[0m     dist \u001b[39m=\u001b[39m Requirement\u001b[39m.\u001b[39mparse(dist)\n\u001b[0;32m    465\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dist, Requirement):\n\u001b[1;32m--> 466\u001b[0m     dist \u001b[39m=\u001b[39m get_provider(dist)\n\u001b[0;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dist, Distribution):\n\u001b[0;32m    468\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected string, Requirement, or Distribution\u001b[39m\u001b[39m\"\u001b[39m, dist)\n",
      "File \u001b[1;32md:\\Data\\tutorials\\Data Science\\Projects\\language-translator\\transformenv\\lib\\site-packages\\pkg_resources\\__init__.py:342\u001b[0m, in \u001b[0;36mget_provider\u001b[1;34m(moduleOrReq)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[39m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(moduleOrReq, Requirement):\n\u001b[1;32m--> 342\u001b[0m     \u001b[39mreturn\u001b[39;00m working_set\u001b[39m.\u001b[39mfind(moduleOrReq) \u001b[39mor\u001b[39;00m require(\u001b[39mstr\u001b[39;49m(moduleOrReq))[\u001b[39m0\u001b[39m]\n\u001b[0;32m    343\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    344\u001b[0m     module \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mmodules[moduleOrReq]\n",
      "File \u001b[1;32md:\\Data\\tutorials\\Data Science\\Projects\\language-translator\\transformenv\\lib\\site-packages\\pkg_resources\\__init__.py:886\u001b[0m, in \u001b[0;36mWorkingSet.require\u001b[1;34m(self, *requirements)\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequire\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mrequirements):\n\u001b[0;32m    878\u001b[0m     \u001b[39m\"\"\"Ensure that distributions matching `requirements` are activated\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \n\u001b[0;32m    880\u001b[0m \u001b[39m    `requirements` must be a string or a (possibly-nested) sequence\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[39m    included, even if they were already activated in this working set.\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 886\u001b[0m     needed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresolve(parse_requirements(requirements))\n\u001b[0;32m    888\u001b[0m     \u001b[39mfor\u001b[39;00m dist \u001b[39min\u001b[39;00m needed:\n\u001b[0;32m    889\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd(dist)\n",
      "File \u001b[1;32md:\\Data\\tutorials\\Data Science\\Projects\\language-translator\\transformenv\\lib\\site-packages\\pkg_resources\\__init__.py:777\u001b[0m, in \u001b[0;36mWorkingSet.resolve\u001b[1;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[39mif\u001b[39;00m dist \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m req:\n\u001b[0;32m    775\u001b[0m     \u001b[39m# Oops, the \"best\" so far conflicts with a dependency\u001b[39;00m\n\u001b[0;32m    776\u001b[0m     dependent_req \u001b[39m=\u001b[39m required_by[req]\n\u001b[1;32m--> 777\u001b[0m     \u001b[39mraise\u001b[39;00m VersionConflict(dist, req)\u001b[39m.\u001b[39mwith_context(dependent_req)\n\u001b[0;32m    779\u001b[0m \u001b[39m# push the new requirements onto the stack\u001b[39;00m\n\u001b[0;32m    780\u001b[0m new_requirements \u001b[39m=\u001b[39m dist\u001b[39m.\u001b[39mrequires(req\u001b[39m.\u001b[39mextras)[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mContextualVersionConflict\u001b[0m: (google-auth 2.8.0 (d:\\data\\tutorials\\data science\\projects\\language-translator\\transformenv\\lib\\site-packages), Requirement.parse('google-auth<2.0dev,>=1.25.0'), {'google-api-core'})"
     ]
    }
   ],
   "source": [
    "#Set up google translate as an additional reference\n",
    "from google.cloud import translate_v2 as translate\n",
    "\n",
    "translate_client = translate.Client.from_service_account_json(\"./drive/MyDrive/gcloud-auth-files/data-shard-330609-2669b6f2c900.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Data\\tutorials\\Data Science\\Projects\\language-translator\\language_translation.ipynb Cell 42'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000065?line=0'>1</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Data\\tutorials\\Data Science\\Projects\\language-translator\\language_translation.ipynb Cell 39'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000058?line=7'>8</a>\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000058?line=9'>10</a>\u001b[0m \u001b[39mwhile\u001b[39;00m ctr\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000058?line=10'>11</a>\u001b[0m   l \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(X_test[i]\u001b[39m.\u001b[39msplit())\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000058?line=11'>12</a>\u001b[0m   \u001b[39mif\u001b[39;00m l\u001b[39m<\u001b[39m\u001b[39m=\u001b[39mmaxlen:   \u001b[39m#Choose only sentences of length in range [5,15]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000058?line=12'>13</a>\u001b[0m     pred_sentence \u001b[39m=\u001b[39m predict_sentence(X_test[i])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "#Testing and Analysis\n",
    "import nltk\n",
    "\n",
    "candidates = []\n",
    "references = []\n",
    "\n",
    "X_test = [test_datum['en'] for test_datum in test_data]\n",
    "y_test = [test_datum['hi'] for test_datum in test_data]\n",
    "\n",
    "ctr = 20 \n",
    "i = 0\n",
    "\n",
    "while ctr > 0:\n",
    "  l = len(X_test[i].split())\n",
    "  if l<=maxlen:   #Choose only sentences of length in range [5,15]\n",
    "    pred_sentence = predict_sentence(X_test[i])\n",
    "    candidates.append(pred_sentence.split())\n",
    "\n",
    "    print(\"Input: \", X_test[i])\n",
    "    print(\"Prediction: \", pred_sentence)\n",
    "\n",
    "    google_translated_sentence = translate_client.translate(X_test[i], target_language='hi')['translatedText']\n",
    "    \n",
    "    print(\"Google Translated Reference: \", google_translated_sentence)\n",
    "    print(\"Dataset Reference: \", ' '.join(y_test[i].split()[1:-1]))\n",
    "    print()\n",
    "    references.append([y_test[i].split()[1:-1], google_translated_sentence.split()])\n",
    "\n",
    "    ctr -= 1\n",
    "  i += 1\n",
    "\n",
    "print(nltk.translate.bleu_score.corpus_bleu(references, candidates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data preprocessing done, we should now proceed towards building the transformer which will perform the language translation task<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us prepare the embedding layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        embedding = self.embed(x)\n",
    "        return embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add the positional encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch \n",
    "from torch.autograd import Variable\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(0)\n",
    "        \n",
    "        x = x + Variable(self.pe[:seq_len], requires_grad=False)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi head attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        \n",
    "        k = self.k_linear(k).view(batch_size, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "\n",
    "        # calculate attention using function we will define next\n",
    "        scores = self.attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output\n",
    "    \n",
    "    def attention(self, q, k, v, d_k, mask=None, dropout=None):\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "\n",
    "        # if mask is not None:\n",
    "        #     mask = mask.unsqueeze(1)\n",
    "        #     scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "\n",
    "        output = torch.matmul(scores, v)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various masks are created here \n",
    "\n",
    "<ul>\n",
    "<li>Padding mask for English</li>\n",
    "<li>Padding mask for Hindi</li>\n",
    "<li>No peek mask for masking future tokens in order to avoid decoder from seeing the inputs early</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 200, 200])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = maxlen\n",
    "\n",
    "def no_peek_mask():\n",
    "    nopeek_mask = np.triu(np.ones((1, size, size)), k=1).astype('uint8')\n",
    "    nopeek_mask = torch.from_numpy(nopeek_mask) == 0\n",
    "\n",
    "    return nopeek_mask\n",
    "\n",
    "def padding_mask(text):\n",
    "    pad_mask = text != 0\n",
    "    return pad_mask\n",
    "\n",
    "def create_masks(src_text, target_text):\n",
    "    return padding_mask(src_text), no_peek_mask() & padding_mask(target_text)\n",
    "\n",
    "\n",
    "no_peek_mask().unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True))/(x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# build an encoder layer with one multi-head attention layer and one feed-forward layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout, inplace=True)\n",
    "        self.dropout_2 = nn.Dropout(dropout, inplace=True)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask).squeeze(1))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x\n",
    "    \n",
    "# build a decoder layer with two multi-head attention layers and one feed-forward layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout, inplace=True)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
    "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "\n",
    "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "            x2 = self.norm_1(x)\n",
    "            x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask).squeeze(1))\n",
    "            x2 = self.norm_2(x)\n",
    "            x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, src_mask).squeeze(1))\n",
    "            x2 = self.norm_3(x)\n",
    "            x = x + self.dropout_3(self.ff(x2))\n",
    "            return x\n",
    "# We can then build a convenient cloning function that can generate multiple layers:\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "        self.softmax_layer = tf.keras.layers.Softmax()\n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
    "        x =  self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
    "        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "        self.softmax = tf.keras.layers.Softmax()\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        e_outputs = self.encoder.forward(src, src_mask)\n",
    "        d_output = self.decoder.forward(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "heads = 8\n",
    "N = 2\n",
    "\n",
    "#Save model after each epoch\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"best_model1.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto', save_weights_only=False)\n",
    "\n",
    "model = Transformer(english_vocab_size, hindi_vocab_size, d_model, N, heads)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "    \n",
    "# this code is very important! It initialises the parameters with a\n",
    "# range of values that stops the signal fading or getting too big.\n",
    "# See this blog for a mathematical explanation.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('encoder.embed.embed.weight', Parameter containing:\n",
      "tensor([[-0.0405, -0.0304,  0.0337,  ...,  0.0133,  0.0366,  0.0052],\n",
      "        [-0.0140,  0.0131,  0.0412,  ...,  0.0167, -0.0336, -0.0154],\n",
      "        [-0.0121, -0.0016,  0.0107,  ...,  0.0089,  0.0157,  0.0324],\n",
      "        ...,\n",
      "        [-0.0094,  0.0102, -0.0073,  ...,  0.0301, -0.0217,  0.0237],\n",
      "        [-0.0170,  0.0170, -0.0046,  ..., -0.0211,  0.0331,  0.0063],\n",
      "        [-0.0338,  0.0267,  0.0009,  ...,  0.0194,  0.0139,  0.0278]],\n",
      "       requires_grad=True))\n",
      "('encoder.layers.0.norm_1.alpha', Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True))\n",
      "('encoder.layers.0.norm_1.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))\n",
      "('encoder.layers.0.norm_2.alpha', Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True))\n",
      "('encoder.layers.0.norm_2.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))\n",
      "('encoder.layers.0.attn.q_linear.weight', Parameter containing:\n",
      "tensor([[ 0.0569,  0.0150, -0.0552,  ...,  0.0558,  0.0167, -0.0444],\n",
      "        [-0.0566, -0.0560,  0.0669,  ..., -0.0349, -0.0529, -0.0119],\n",
      "        [-0.0418,  0.0592, -0.0416,  ...,  0.0665,  0.0202, -0.0174],\n",
      "        ...,\n",
      "        [-0.0039, -0.0080, -0.0302,  ..., -0.0090, -0.0298,  0.0193],\n",
      "        [-0.0483,  0.0592,  0.0466,  ...,  0.0597, -0.0087,  0.0494],\n",
      "        [-0.0298, -0.0097, -0.0584,  ...,  0.0612, -0.0647,  0.0519]],\n",
      "       requires_grad=True))\n",
      "('encoder.layers.0.attn.q_linear.bias', Parameter containing:\n",
      "tensor([ 0.0037, -0.0255,  0.0169, -0.0331,  0.0107,  0.0101,  0.0304,  0.0379,\n",
      "        -0.0097,  0.0260,  0.0218,  0.0417, -0.0303,  0.0176, -0.0309, -0.0430,\n",
      "         0.0238, -0.0074, -0.0114,  0.0167, -0.0358, -0.0360, -0.0149,  0.0043,\n",
      "        -0.0048,  0.0150, -0.0285, -0.0426,  0.0094, -0.0173, -0.0380, -0.0102,\n",
      "         0.0309,  0.0027, -0.0394,  0.0356, -0.0072, -0.0125, -0.0017,  0.0092,\n",
      "         0.0202,  0.0127,  0.0399,  0.0216, -0.0212,  0.0311,  0.0224, -0.0084,\n",
      "         0.0053,  0.0103,  0.0417, -0.0023, -0.0191, -0.0258, -0.0392,  0.0277,\n",
      "         0.0173,  0.0373, -0.0245, -0.0335,  0.0165,  0.0316,  0.0409, -0.0081,\n",
      "        -0.0322,  0.0059, -0.0097, -0.0315,  0.0275,  0.0417,  0.0401, -0.0423,\n",
      "        -0.0021,  0.0090,  0.0405,  0.0289,  0.0138, -0.0350, -0.0347, -0.0119,\n",
      "        -0.0327,  0.0427, -0.0294,  0.0197,  0.0057,  0.0230, -0.0014,  0.0094,\n",
      "        -0.0130,  0.0073, -0.0301,  0.0058,  0.0283, -0.0328, -0.0093, -0.0205,\n",
      "         0.0124, -0.0295, -0.0362,  0.0110,  0.0352, -0.0014, -0.0147,  0.0026,\n",
      "        -0.0021,  0.0373, -0.0169,  0.0221, -0.0183, -0.0347,  0.0166,  0.0411,\n",
      "         0.0097,  0.0383,  0.0105, -0.0187,  0.0292, -0.0311, -0.0054, -0.0376,\n",
      "        -0.0035, -0.0335, -0.0395, -0.0290,  0.0184,  0.0061, -0.0029, -0.0261,\n",
      "         0.0081, -0.0248,  0.0216,  0.0435,  0.0049, -0.0248,  0.0178, -0.0417,\n",
      "         0.0046, -0.0085, -0.0037,  0.0167,  0.0268,  0.0214, -0.0002,  0.0087,\n",
      "         0.0216, -0.0289, -0.0435,  0.0248,  0.0219,  0.0374, -0.0103,  0.0178,\n",
      "        -0.0196, -0.0183, -0.0039, -0.0123,  0.0441,  0.0344, -0.0170, -0.0327,\n",
      "         0.0031,  0.0403,  0.0421,  0.0155, -0.0193, -0.0438,  0.0279, -0.0390,\n",
      "        -0.0025,  0.0439, -0.0010,  0.0180, -0.0370, -0.0361, -0.0065, -0.0079,\n",
      "        -0.0313, -0.0135, -0.0324, -0.0410, -0.0143, -0.0335,  0.0306, -0.0280,\n",
      "        -0.0004, -0.0124, -0.0081,  0.0030, -0.0191, -0.0208, -0.0217, -0.0232,\n",
      "         0.0089, -0.0083, -0.0104, -0.0048,  0.0274, -0.0127, -0.0288,  0.0384,\n",
      "         0.0272, -0.0144, -0.0121, -0.0184,  0.0394,  0.0353,  0.0361,  0.0419,\n",
      "        -0.0144,  0.0428, -0.0371,  0.0312, -0.0044, -0.0170,  0.0262,  0.0397,\n",
      "         0.0410, -0.0280,  0.0066,  0.0355, -0.0214,  0.0383,  0.0049,  0.0150,\n",
      "        -0.0049, -0.0141, -0.0122,  0.0345, -0.0268, -0.0331,  0.0371,  0.0335,\n",
      "         0.0208, -0.0328, -0.0370,  0.0182,  0.0078, -0.0385, -0.0178, -0.0399,\n",
      "        -0.0116,  0.0287, -0.0139,  0.0084,  0.0230, -0.0364,  0.0236,  0.0218,\n",
      "        -0.0149, -0.0036, -0.0107, -0.0052, -0.0342, -0.0164, -0.0116,  0.0315,\n",
      "        -0.0323,  0.0271, -0.0114,  0.0366,  0.0167, -0.0101,  0.0063,  0.0352,\n",
      "         0.0366, -0.0245, -0.0193,  0.0115,  0.0005, -0.0404,  0.0032,  0.0237,\n",
      "        -0.0327,  0.0330, -0.0214, -0.0249,  0.0040,  0.0294, -0.0307,  0.0220,\n",
      "         0.0227, -0.0424,  0.0101,  0.0261, -0.0301,  0.0232, -0.0074, -0.0027,\n",
      "        -0.0110, -0.0421, -0.0341, -0.0375,  0.0367,  0.0201,  0.0216, -0.0221,\n",
      "         0.0121,  0.0123,  0.0011,  0.0357,  0.0343, -0.0065,  0.0032, -0.0132,\n",
      "         0.0298,  0.0157, -0.0166,  0.0048,  0.0338,  0.0369, -0.0315, -0.0320,\n",
      "        -0.0418, -0.0441,  0.0062,  0.0398, -0.0191,  0.0126,  0.0055, -0.0017,\n",
      "         0.0270,  0.0223, -0.0040,  0.0306, -0.0057,  0.0024, -0.0400, -0.0041,\n",
      "         0.0172, -0.0377, -0.0276, -0.0237,  0.0428, -0.0330,  0.0380,  0.0430,\n",
      "        -0.0216, -0.0205,  0.0130, -0.0268, -0.0084,  0.0393, -0.0429,  0.0063,\n",
      "         0.0284,  0.0134,  0.0224,  0.0202, -0.0012,  0.0131,  0.0151,  0.0341,\n",
      "         0.0066, -0.0254,  0.0018,  0.0264, -0.0141,  0.0182,  0.0022,  0.0299,\n",
      "        -0.0309,  0.0433, -0.0062, -0.0119,  0.0133,  0.0331,  0.0083, -0.0292,\n",
      "         0.0238,  0.0323, -0.0008, -0.0237, -0.0045,  0.0246, -0.0194, -0.0231,\n",
      "        -0.0327,  0.0414, -0.0289, -0.0342,  0.0324,  0.0251, -0.0156,  0.0079,\n",
      "        -0.0037, -0.0124, -0.0254,  0.0001,  0.0375, -0.0376,  0.0144, -0.0183,\n",
      "        -0.0313,  0.0146, -0.0395, -0.0202,  0.0293,  0.0299, -0.0205,  0.0363,\n",
      "        -0.0068,  0.0183, -0.0390, -0.0326,  0.0383,  0.0324,  0.0074, -0.0146,\n",
      "        -0.0050, -0.0101, -0.0414,  0.0151, -0.0316,  0.0319,  0.0188, -0.0413,\n",
      "        -0.0296,  0.0041, -0.0290,  0.0402, -0.0003,  0.0425, -0.0035,  0.0211,\n",
      "         0.0393, -0.0331,  0.0223,  0.0265, -0.0279, -0.0205, -0.0233, -0.0169,\n",
      "        -0.0118, -0.0391, -0.0327, -0.0169, -0.0170, -0.0298,  0.0085, -0.0274,\n",
      "         0.0216,  0.0299, -0.0331,  0.0376, -0.0394,  0.0259,  0.0022, -0.0055,\n",
      "         0.0033,  0.0048,  0.0219, -0.0085,  0.0388, -0.0075, -0.0249, -0.0128,\n",
      "         0.0300, -0.0179, -0.0366, -0.0409, -0.0308,  0.0340, -0.0272, -0.0118,\n",
      "         0.0388,  0.0419,  0.0168, -0.0420, -0.0360,  0.0100, -0.0208, -0.0411,\n",
      "         0.0340, -0.0215, -0.0098,  0.0176, -0.0420,  0.0214,  0.0045, -0.0278,\n",
      "        -0.0265,  0.0322,  0.0229, -0.0328,  0.0039, -0.0211,  0.0072,  0.0363,\n",
      "         0.0186,  0.0341, -0.0074, -0.0435, -0.0322,  0.0327, -0.0181,  0.0188,\n",
      "         0.0196,  0.0003,  0.0204, -0.0376,  0.0309, -0.0105,  0.0175, -0.0253,\n",
      "        -0.0263, -0.0417,  0.0046, -0.0128, -0.0029,  0.0165,  0.0078, -0.0064],\n",
      "       requires_grad=True))\n",
      "('encoder.layers.0.attn.v_linear.weight', Parameter containing:\n",
      "tensor([[-0.0255,  0.0457, -0.0230,  ...,  0.0689, -0.0587, -0.0631],\n",
      "        [-0.0068, -0.0250, -0.0115,  ...,  0.0640,  0.0476,  0.0427],\n",
      "        [-0.0584, -0.0470, -0.0367,  ..., -0.0294, -0.0311,  0.0606],\n",
      "        ...,\n",
      "        [ 0.0063, -0.0114, -0.0161,  ..., -0.0760, -0.0644,  0.0756],\n",
      "        [-0.0456, -0.0059,  0.0121,  ..., -0.0761, -0.0307,  0.0737],\n",
      "        [ 0.0126, -0.0518,  0.0272,  ...,  0.0763, -0.0128,  0.0283]],\n",
      "       requires_grad=True))\n",
      "('encoder.layers.0.attn.v_linear.bias', Parameter containing:\n",
      "tensor([ 4.2113e-02,  1.5851e-02, -1.7450e-02, -4.2895e-02, -6.4087e-03,\n",
      "        -5.0386e-03,  3.6729e-02, -3.2138e-02,  2.8940e-02,  1.4764e-02,\n",
      "         4.2312e-02,  3.6593e-02, -4.0668e-02,  3.9951e-02, -3.3939e-02,\n",
      "         8.0692e-03, -3.5519e-02,  1.5025e-02, -1.0814e-02,  3.9151e-02,\n",
      "         2.9167e-02,  2.4215e-03,  2.7750e-02, -4.8392e-03,  6.7985e-03,\n",
      "        -1.0831e-02, -3.8153e-02, -2.9198e-02, -2.9042e-02, -2.2730e-02,\n",
      "         2.2331e-03,  3.4279e-02, -2.1103e-02, -3.9359e-03, -2.3910e-02,\n",
      "         3.0465e-02, -5.5797e-03, -2.6073e-02, -5.4225e-03, -4.0814e-02,\n",
      "         2.1841e-02, -2.5196e-02,  6.4510e-03,  2.9795e-02, -3.5295e-02,\n",
      "         1.2397e-03, -3.0160e-02, -1.4703e-02, -1.5040e-02, -1.5722e-02,\n",
      "        -1.8178e-02,  1.6263e-02,  2.5149e-02, -3.4318e-02, -2.8332e-02,\n",
      "        -6.3897e-03, -2.7346e-02, -4.3754e-03,  2.9059e-02, -2.8649e-02,\n",
      "        -3.8368e-02,  2.4827e-02,  2.2584e-02, -2.3601e-02, -5.7184e-03,\n",
      "         1.4377e-02,  5.0146e-03,  1.3779e-02,  2.3479e-02,  7.6500e-03,\n",
      "         1.7557e-02,  3.7502e-02,  4.3230e-02,  4.9483e-03, -3.1885e-02,\n",
      "        -1.1206e-02, -3.2392e-02, -2.5045e-02, -3.1097e-03,  1.5950e-02,\n",
      "        -6.6972e-03, -2.0565e-02,  6.4086e-03,  2.3452e-02, -4.2593e-02,\n",
      "        -3.4376e-02,  2.4187e-02,  4.1653e-02,  2.3370e-02,  1.5631e-02,\n",
      "         2.3012e-02,  2.2945e-02,  3.1861e-02, -1.6558e-02,  4.3123e-02,\n",
      "         9.2278e-03, -2.7086e-03,  7.8744e-03,  8.0697e-03,  3.2652e-02,\n",
      "        -1.6191e-02,  3.4992e-02,  2.9780e-02,  3.4435e-02,  3.4536e-02,\n",
      "         1.2392e-02, -1.0935e-02, -2.3384e-02,  1.8899e-02,  2.3674e-02,\n",
      "        -4.2509e-02, -1.9072e-02, -1.4387e-02,  2.6450e-02, -2.0853e-02,\n",
      "         3.4762e-02,  3.5390e-02,  6.2467e-04,  2.8475e-02,  1.5546e-02,\n",
      "         2.3763e-03,  4.4156e-02,  3.5264e-02,  2.6563e-02,  4.0614e-02,\n",
      "         2.3431e-02,  2.8680e-02,  1.0449e-02, -3.4043e-02,  2.9406e-02,\n",
      "         2.7141e-02,  4.1027e-02, -3.9708e-02,  3.2239e-02,  1.1796e-02,\n",
      "        -3.3470e-02, -2.1774e-02,  3.9180e-02,  2.8233e-02, -1.6859e-02,\n",
      "        -3.9849e-03, -3.2929e-02, -1.8275e-02, -2.9675e-02, -2.9361e-02,\n",
      "         4.3643e-02, -2.0559e-02,  3.3972e-02,  2.3274e-02, -1.8623e-02,\n",
      "         1.4036e-02,  5.9390e-03, -3.9519e-02,  4.3914e-02, -1.4694e-02,\n",
      "         1.2145e-02,  3.0321e-02, -3.2974e-02, -2.2487e-02,  1.6057e-02,\n",
      "        -9.3509e-03, -1.1845e-02,  3.5449e-02, -3.3716e-02, -9.2715e-03,\n",
      "         3.1231e-02, -1.9629e-02, -1.8580e-02, -1.3048e-02,  2.2357e-02,\n",
      "        -3.0027e-02, -3.8051e-02,  1.7769e-02, -9.2919e-03, -2.8475e-03,\n",
      "        -1.3224e-02, -3.0929e-03,  2.5307e-02,  1.0416e-02,  7.1552e-03,\n",
      "         1.9898e-02,  2.6357e-02,  4.2880e-02,  3.2909e-02,  2.1154e-02,\n",
      "         3.4663e-02,  3.0058e-02, -4.1757e-02, -1.2631e-02,  1.7101e-02,\n",
      "        -3.4388e-02, -4.2318e-02,  2.4507e-02, -6.2388e-03, -2.6327e-02,\n",
      "        -3.9174e-02, -4.3351e-02,  8.0261e-03,  2.7526e-02,  3.2893e-02,\n",
      "         1.6539e-02, -4.2587e-02,  3.1007e-02, -9.6455e-03, -2.8560e-02,\n",
      "         4.2560e-02,  2.6695e-02,  1.9342e-02, -3.7470e-02,  1.8871e-02,\n",
      "         7.0978e-03,  3.9967e-02,  1.5689e-02,  2.8680e-02, -7.0472e-03,\n",
      "         8.1956e-03, -1.6854e-02, -2.6265e-02,  1.1372e-04, -8.6597e-03,\n",
      "         1.0542e-02, -1.5425e-02,  3.6033e-02,  7.3687e-03, -4.2952e-02,\n",
      "        -1.7665e-02, -2.8266e-02, -2.4220e-02, -2.0369e-02, -2.0754e-02,\n",
      "        -3.1167e-02, -3.2624e-02, -1.1506e-02, -3.4007e-02,  4.0915e-03,\n",
      "         4.0841e-02,  3.7551e-02,  1.1735e-02, -5.3644e-06, -3.9358e-02,\n",
      "         3.8169e-02, -1.6149e-02, -3.4050e-02, -7.3740e-03,  4.0952e-02,\n",
      "        -8.0734e-03, -1.5045e-03, -2.7533e-02, -1.1424e-02,  1.9700e-02,\n",
      "        -3.0426e-03,  1.1234e-02,  3.1455e-03,  8.9024e-04,  1.9487e-02,\n",
      "         3.3778e-02, -3.8216e-02, -3.1973e-02,  3.8842e-02, -3.6172e-02,\n",
      "         2.0288e-03,  2.8444e-02,  1.9681e-02, -3.5453e-03,  3.1852e-03,\n",
      "         1.4935e-02,  2.3969e-02,  2.1789e-02,  3.4326e-02, -3.5032e-02,\n",
      "         4.5000e-03, -3.2066e-02, -3.9639e-02, -3.1132e-02, -3.4109e-02,\n",
      "         1.2766e-02, -3.5620e-02, -1.1669e-02,  2.5384e-03, -2.9883e-02,\n",
      "         1.6426e-02, -1.2381e-02, -2.6539e-03, -2.6286e-02,  3.9408e-02,\n",
      "         4.1561e-02, -1.9339e-02, -2.8244e-02,  2.1043e-02, -3.1252e-02,\n",
      "         6.6334e-03, -2.5583e-02, -2.1440e-02, -7.2778e-03, -2.1829e-02,\n",
      "        -2.1525e-03,  3.8502e-02,  1.5328e-02, -1.9165e-02, -2.3123e-03,\n",
      "         4.2497e-02,  2.3590e-03,  3.0974e-02, -6.1708e-03, -7.3074e-03,\n",
      "         3.7644e-02,  2.7490e-02, -3.8924e-02, -3.7440e-02,  4.4168e-02,\n",
      "         3.1464e-02, -6.9982e-03,  1.6135e-02,  5.7168e-03, -2.0917e-02,\n",
      "         1.8525e-02,  3.0563e-02,  4.2785e-02,  3.5348e-02,  1.9730e-02,\n",
      "        -4.2467e-02,  3.7823e-02,  3.3418e-03,  7.1137e-03, -2.7478e-02,\n",
      "         1.6974e-02,  1.7053e-02,  3.2009e-02, -2.8223e-02, -1.7592e-02,\n",
      "        -2.0927e-02,  1.1907e-02, -2.9137e-02,  1.0654e-03, -1.3000e-02,\n",
      "         2.7631e-02,  1.7736e-02,  3.5504e-02,  7.3069e-03, -1.0474e-02,\n",
      "         1.0544e-02,  2.4495e-02,  2.0258e-02, -5.6692e-03,  1.0601e-03,\n",
      "        -3.7201e-02,  2.5732e-02, -5.1630e-04,  9.3522e-03, -3.4851e-02,\n",
      "        -1.7323e-02, -2.8997e-02, -3.9547e-02, -3.6293e-02, -1.3265e-02,\n",
      "        -1.2675e-02, -2.1615e-02,  1.4997e-02,  3.6799e-02, -2.1437e-02,\n",
      "         2.6789e-02,  2.6972e-02, -1.9737e-03, -1.4478e-02, -5.0306e-03,\n",
      "        -2.9115e-02, -1.7533e-02,  1.3315e-02,  8.5701e-03,  1.1155e-02,\n",
      "         1.8052e-02, -6.7017e-03,  2.7737e-03,  1.2492e-02, -1.4770e-02,\n",
      "        -9.7481e-03, -4.0069e-02,  2.8077e-02,  8.0499e-03,  3.1280e-02,\n",
      "        -1.1633e-02,  1.6202e-02,  2.2335e-02,  1.2777e-02,  4.0319e-02,\n",
      "         1.8177e-02,  2.4750e-02, -2.2074e-02, -3.6800e-02, -8.1532e-03,\n",
      "        -4.0069e-02,  1.2473e-02, -3.1786e-02, -1.7763e-02,  1.0229e-02,\n",
      "        -4.6579e-03, -3.9774e-02,  4.2938e-02,  5.1320e-03, -1.8982e-03,\n",
      "        -7.7134e-03, -1.8520e-02, -4.1662e-02, -1.0299e-02,  3.6774e-02,\n",
      "         9.1219e-03,  3.3715e-02, -3.8402e-02,  3.5109e-02, -1.4965e-02,\n",
      "        -3.5052e-02,  1.0958e-02,  3.9066e-02, -1.6442e-02, -6.0835e-03,\n",
      "         1.1498e-02, -6.7880e-03, -3.9587e-02,  2.0255e-03,  1.0725e-02,\n",
      "        -2.0047e-02, -3.1741e-02, -3.3663e-02, -1.8196e-02,  2.0481e-02,\n",
      "         2.6546e-02, -7.0763e-03,  1.5939e-02,  2.7903e-02,  1.6286e-02,\n",
      "        -4.2395e-02, -3.9589e-02,  2.6600e-02, -1.2837e-02, -2.7907e-02,\n",
      "        -2.2141e-02,  4.0994e-02,  3.6008e-02,  5.7013e-03, -4.1493e-02,\n",
      "        -1.9427e-02, -1.2082e-02, -3.3603e-02,  2.7140e-03,  3.0635e-02,\n",
      "         1.7389e-02, -1.4015e-02,  1.8548e-02, -3.0820e-02, -2.8175e-02,\n",
      "        -1.2806e-03,  3.4184e-02, -4.0249e-02,  1.9629e-02, -1.0646e-02,\n",
      "         6.7550e-03, -1.8758e-02,  4.1422e-02, -1.0605e-02,  1.8594e-02,\n",
      "         6.7012e-03,  2.5013e-02,  2.9761e-02, -4.3397e-02,  1.9559e-02,\n",
      "        -1.8084e-02, -2.3689e-02,  4.8354e-03, -2.1707e-02, -2.0083e-02,\n",
      "        -3.3928e-02,  3.9382e-02,  3.4075e-02, -1.7718e-02, -2.9083e-02,\n",
      "        -2.2975e-02,  2.7669e-02, -2.3874e-02,  8.4132e-03, -8.6920e-04,\n",
      "        -4.3509e-02, -1.7755e-02, -7.7514e-03, -2.2815e-02,  2.9627e-02,\n",
      "        -7.2082e-03,  4.3288e-03,  1.1265e-02, -2.2161e-02,  7.2518e-03,\n",
      "         2.7609e-02, -2.0242e-03, -3.0457e-02,  1.6602e-02, -5.7289e-03,\n",
      "         2.3394e-02, -1.2780e-03, -1.0862e-02,  1.4185e-02,  2.6841e-02,\n",
      "         4.0839e-02, -3.6305e-02,  3.3203e-03,  2.6790e-02, -1.6849e-02,\n",
      "        -7.3975e-03, -1.9108e-02,  4.0616e-02,  3.0909e-02, -4.7275e-03,\n",
      "        -3.0954e-02,  8.0627e-03], requires_grad=True))\n",
      "('encoder.layers.0.attn.k_linear.weight', Parameter containing:\n",
      "tensor([[-0.0254,  0.0289, -0.0191,  ..., -0.0435, -0.0354,  0.0101],\n",
      "        [ 0.0150,  0.0256,  0.0361,  ...,  0.0721, -0.0286, -0.0749],\n",
      "        [ 0.0162,  0.0721,  0.0274,  ..., -0.0475,  0.0210,  0.0590],\n",
      "        ...,\n",
      "        [ 0.0765, -0.0178, -0.0378,  ...,  0.0313, -0.0202, -0.0240],\n",
      "        [ 0.0741,  0.0450, -0.0444,  ...,  0.0338,  0.0319,  0.0212],\n",
      "        [-0.0735,  0.0110, -0.0332,  ...,  0.0698, -0.0413,  0.0500]],\n",
      "       requires_grad=True))\n",
      "('encoder.layers.0.attn.k_linear.bias', Parameter containing:\n",
      "tensor([ 2.8466e-02, -3.1576e-02, -2.1382e-02,  2.8040e-02, -1.9033e-02,\n",
      "        -2.3035e-02,  1.2237e-02,  4.3663e-02, -3.7604e-02, -4.3929e-02,\n",
      "         1.8038e-02,  4.1589e-02, -2.5162e-02, -3.8133e-02,  3.5641e-02,\n",
      "         4.1162e-03,  2.1725e-02, -2.6714e-02,  4.5525e-03, -1.3898e-02,\n",
      "         2.6634e-03, -2.1919e-02, -2.2364e-02, -3.7802e-02, -2.5131e-02,\n",
      "         1.1673e-02, -4.0482e-02, -4.8865e-03, -1.4762e-02, -8.6488e-03,\n",
      "        -3.3670e-02, -2.9190e-02,  3.3692e-02, -2.9774e-02, -2.1007e-03,\n",
      "         1.0623e-02,  4.2860e-02,  6.0571e-03, -3.8645e-02, -5.9925e-03,\n",
      "         4.2115e-04, -6.2222e-03,  1.0368e-02,  2.7856e-03,  3.3152e-02,\n",
      "        -7.2547e-03, -3.1418e-02, -4.5041e-03, -4.0171e-02, -2.3775e-02,\n",
      "        -1.1850e-05,  2.9324e-02, -9.2894e-03, -1.0094e-02,  8.1878e-03,\n",
      "        -7.3896e-03, -4.1529e-02, -4.0507e-02, -2.5653e-02, -2.4183e-02,\n",
      "        -1.8531e-02,  1.5281e-02, -1.5755e-02,  1.6397e-02, -3.7667e-02,\n",
      "        -1.7488e-02,  2.8644e-02,  5.3139e-03,  8.6694e-03,  3.8921e-02,\n",
      "        -2.6988e-02, -3.7142e-02, -7.0886e-03, -3.9489e-02, -1.6972e-02,\n",
      "         2.6430e-02, -7.1957e-03,  2.5814e-02, -2.5517e-02, -3.1044e-02,\n",
      "         6.6013e-03,  1.3303e-02, -3.3507e-03, -3.1976e-02,  3.7873e-02,\n",
      "        -4.3684e-02,  2.2735e-02, -4.3717e-02, -1.6612e-02,  1.4361e-02,\n",
      "         2.8796e-02,  1.5781e-02, -4.2693e-02, -2.6894e-02,  3.1873e-02,\n",
      "         2.0140e-02, -2.0802e-03, -2.5637e-03,  5.9422e-03,  3.4819e-02,\n",
      "         1.3108e-02,  3.3180e-03, -4.5628e-03,  3.6137e-02, -2.8060e-02,\n",
      "        -1.9545e-02,  7.0133e-03,  2.2358e-03,  2.4410e-02, -3.3672e-03,\n",
      "        -3.0463e-02,  1.8442e-02, -2.1668e-02, -1.0417e-02, -4.0111e-02,\n",
      "         1.3184e-03, -1.0552e-02,  3.1422e-02, -1.9723e-02, -1.7379e-02,\n",
      "        -1.4189e-02,  1.4435e-02, -3.9165e-02,  4.0357e-03,  2.1617e-02,\n",
      "         1.8650e-02,  1.2756e-02, -1.4909e-02, -1.9367e-02,  4.1250e-03,\n",
      "         7.2022e-03,  7.0841e-03, -9.6479e-03,  1.7322e-02,  1.9049e-03,\n",
      "         3.3038e-02, -3.6010e-02, -4.1597e-02,  4.2095e-02,  7.0053e-03,\n",
      "        -1.2476e-02,  1.3386e-02, -4.2721e-02,  1.7849e-02, -3.3959e-02,\n",
      "         2.0250e-02, -8.0928e-03,  7.9811e-03, -3.4161e-02, -2.8154e-02,\n",
      "        -2.4738e-02, -1.3604e-02, -3.0891e-02, -5.1802e-03,  1.6409e-02,\n",
      "         6.8439e-03, -2.7238e-02, -8.7744e-03, -1.4245e-02,  1.6753e-02,\n",
      "         1.5804e-02,  1.5015e-02,  3.1510e-02,  2.1790e-02, -1.7269e-02,\n",
      "         2.3606e-02, -1.2520e-02, -2.4243e-02,  3.9948e-02,  2.4889e-02,\n",
      "        -4.0360e-02,  1.7399e-03,  3.1816e-02,  4.1331e-02, -6.7625e-04,\n",
      "         3.6791e-02,  1.7415e-02, -5.1280e-03,  3.2169e-02, -1.9530e-02,\n",
      "         4.0514e-04,  3.7053e-02,  1.1483e-02,  3.8117e-03,  2.7934e-02,\n",
      "        -3.1959e-02, -4.3440e-02, -6.0972e-03, -2.3632e-02, -2.7036e-02,\n",
      "         3.6770e-02,  1.6995e-03, -2.0404e-02, -1.4274e-02,  4.1004e-02,\n",
      "        -1.4619e-02, -2.6548e-02, -3.4694e-02,  3.2412e-03, -3.8872e-02,\n",
      "        -2.3288e-03, -3.1133e-02, -4.9501e-03, -1.7422e-02, -6.3620e-03,\n",
      "         3.0451e-02, -2.2695e-02, -3.7003e-02, -1.9297e-03,  3.6891e-02,\n",
      "        -1.9107e-02,  9.7054e-04,  1.4439e-02,  3.8221e-02, -7.5843e-03,\n",
      "         2.0589e-02, -4.3379e-02, -3.7846e-02, -2.8062e-02, -2.3367e-02,\n",
      "         1.5211e-03, -2.6829e-02, -4.3265e-02,  1.7478e-02,  3.3655e-02,\n",
      "         2.7938e-02,  2.5354e-03,  2.8159e-02,  8.5770e-03, -3.7975e-02,\n",
      "         1.3817e-02,  4.3845e-02, -1.5784e-02,  1.7846e-02, -2.4755e-02,\n",
      "         1.3470e-02, -4.8110e-03,  6.0575e-03, -5.3527e-03,  8.3360e-03,\n",
      "        -1.5052e-02,  3.4404e-02,  1.3029e-02,  1.6181e-02,  2.9638e-02,\n",
      "         1.9958e-02,  2.3601e-02,  4.2157e-02,  2.9096e-02, -4.7397e-03,\n",
      "         1.3966e-02, -3.6908e-02,  2.1043e-02,  1.5177e-02, -9.8751e-03,\n",
      "         1.0005e-03, -1.3313e-02,  1.6878e-02, -4.0443e-02, -3.5311e-03,\n",
      "         2.8758e-02,  1.0232e-02, -7.5291e-03,  2.8563e-03, -4.7029e-03,\n",
      "         9.5207e-03, -1.8018e-02, -9.7055e-03, -4.1400e-02,  2.8311e-02,\n",
      "        -5.5068e-03,  2.9616e-02, -1.1018e-02,  2.1123e-02, -1.5897e-02,\n",
      "         2.2421e-02,  3.5948e-02,  2.9202e-02,  4.4113e-02,  1.5457e-02,\n",
      "        -3.0534e-02,  2.2483e-02, -1.5201e-02, -3.1705e-02,  2.4663e-02,\n",
      "         2.6117e-02, -3.1813e-02, -3.1776e-02,  2.8752e-02,  2.6732e-02,\n",
      "         1.7115e-03,  2.9238e-02, -2.2809e-02, -2.5481e-02,  2.1772e-03,\n",
      "        -1.7083e-02,  3.6425e-02, -2.3514e-02,  3.7942e-02,  2.9621e-02,\n",
      "        -1.3041e-02, -2.7971e-02,  3.1175e-02, -3.7329e-02, -3.2022e-02,\n",
      "         3.1949e-02, -1.8306e-02, -2.6754e-02, -1.0185e-02, -3.5084e-02,\n",
      "        -5.3606e-03,  2.1412e-02, -2.4348e-02,  3.2649e-02,  4.0224e-02,\n",
      "        -7.2036e-04, -3.7017e-02, -3.0335e-02, -4.2164e-02,  2.6888e-02,\n",
      "        -6.2750e-03, -2.1122e-02, -3.9755e-02,  3.4689e-02, -9.1327e-03,\n",
      "         4.2169e-02,  1.4085e-02, -5.6629e-03,  3.7312e-02, -2.8975e-02,\n",
      "        -2.2938e-02,  9.3918e-04,  2.8349e-02, -3.9844e-02,  2.0003e-02,\n",
      "        -1.0823e-02, -3.9257e-02,  2.4904e-02,  3.9756e-03,  3.5340e-02,\n",
      "        -3.7791e-02,  4.5258e-03, -1.6417e-02, -2.4745e-02,  2.7035e-02,\n",
      "         3.3336e-02, -1.4166e-02,  7.2056e-03, -2.6066e-02,  5.4514e-03,\n",
      "         2.2904e-02, -1.3844e-02, -4.2029e-03,  1.2859e-03, -1.9006e-02,\n",
      "        -1.9352e-02, -2.8439e-02,  4.3345e-02,  3.6851e-02,  2.2704e-02,\n",
      "         9.7455e-03, -3.7895e-02, -3.7768e-02, -1.8372e-03,  2.9151e-02,\n",
      "         4.1177e-02,  3.5148e-03, -9.4573e-03,  3.5877e-03, -2.0087e-02,\n",
      "         2.0176e-04, -3.7562e-02, -3.1867e-02, -2.1994e-02,  3.6004e-02,\n",
      "        -2.3605e-02,  1.2749e-02,  3.0475e-03, -3.3361e-02, -1.7551e-02,\n",
      "         1.7869e-02, -2.3366e-02, -9.8540e-03, -1.6827e-02, -1.3040e-02,\n",
      "        -3.8654e-02, -3.9348e-02,  2.3817e-02,  4.0248e-02, -3.7212e-02,\n",
      "         9.2285e-03,  1.8703e-02, -3.5633e-02,  3.6174e-02,  1.4531e-02,\n",
      "        -1.8495e-02,  2.7171e-02,  3.6090e-03, -8.9996e-03, -2.4432e-02,\n",
      "         1.1079e-02, -3.1874e-02, -3.4302e-02,  3.5858e-02,  2.0719e-02,\n",
      "        -3.1681e-03,  2.1033e-02, -6.0762e-03, -2.0588e-02,  2.8543e-03,\n",
      "         1.1513e-02, -7.6844e-03, -1.2826e-02,  3.3306e-02, -3.6754e-02,\n",
      "         3.6549e-02, -8.0794e-03, -3.3808e-02, -1.6690e-02, -2.1592e-02,\n",
      "         7.8664e-04,  2.6582e-02, -1.4578e-02, -3.1909e-02, -2.6587e-02,\n",
      "         2.3355e-02,  2.7460e-03,  1.4893e-02, -2.9185e-02,  6.4524e-03,\n",
      "        -3.3763e-02,  3.3056e-02,  3.8978e-02,  1.5069e-02,  2.0968e-02,\n",
      "         3.7773e-02,  2.1346e-02,  1.5759e-02,  1.4843e-02, -9.2096e-03,\n",
      "        -3.3041e-02, -1.9303e-02, -1.1495e-02,  1.9062e-02,  3.1878e-02,\n",
      "         9.8645e-03, -3.8840e-02, -3.0855e-02, -1.7385e-02,  7.1576e-03,\n",
      "         1.2452e-02, -4.2648e-02,  3.4358e-02,  4.3458e-03,  3.6405e-02,\n",
      "        -2.9515e-02,  3.5475e-02, -8.0282e-03, -3.2711e-02, -4.5234e-03,\n",
      "        -2.3730e-02, -9.8844e-03, -3.0309e-02, -3.5248e-02,  8.3474e-03,\n",
      "         4.1270e-02,  3.7590e-02,  2.6379e-02, -4.3542e-02, -3.3868e-02,\n",
      "        -1.9520e-02, -2.3184e-02,  3.1624e-02, -1.1634e-02, -2.9302e-02,\n",
      "        -3.6573e-02, -3.6630e-02, -2.9054e-02,  1.1941e-02, -2.6494e-02,\n",
      "        -4.3305e-02, -1.2661e-02, -3.5914e-02, -3.2297e-02, -2.2791e-02,\n",
      "         2.5692e-02,  4.0889e-02,  3.6008e-02, -2.5321e-02, -7.8863e-03,\n",
      "         1.2929e-02, -1.8719e-02,  4.0269e-02, -2.1128e-02,  3.4347e-02,\n",
      "        -4.2727e-02, -1.9288e-02,  6.7732e-03, -4.2316e-03,  7.3233e-03,\n",
      "        -1.4291e-02, -3.8192e-02,  2.7631e-02,  2.8169e-02,  1.8133e-02,\n",
      "         2.2899e-02, -4.5350e-04, -2.1982e-02, -1.6685e-02, -1.8313e-02,\n",
      "         3.6036e-02, -3.5458e-02], requires_grad=True))\n",
      "('encoder.layers.0.attn.out.weight', Parameter containing:\n",
      "tensor([[ 0.0146,  0.0031, -0.0407,  ...,  0.0683,  0.0161,  0.0178],\n",
      "        [ 0.0707,  0.0644,  0.0343,  ..., -0.0683,  0.0610,  0.0498],\n",
      "        [-0.0056, -0.0592, -0.0626,  ...,  0.0599, -0.0723,  0.0106],\n",
      "        ...,\n",
      "        [ 0.0437,  0.0700, -0.0309,  ...,  0.0575,  0.0054, -0.0250],\n",
      "        [-0.0679,  0.0452,  0.0119,  ...,  0.0543,  0.0336,  0.0684],\n",
      "        [-0.0753, -0.0083,  0.0318,  ...,  0.0226,  0.0393, -0.0019]],\n",
      "       requires_grad=True))\n",
      "('encoder.layers.0.attn.out.bias', Parameter containing:\n",
      "tensor([ 3.6173e-02,  1.6211e-03,  2.4914e-02,  3.2639e-02,  2.9335e-02,\n",
      "         1.1391e-02, -1.1946e-02,  1.0488e-02, -3.1338e-02,  2.9513e-02,\n",
      "         5.4303e-03, -4.1162e-02, -3.9499e-02,  1.8841e-02, -2.7576e-02,\n",
      "         7.8569e-03, -3.4326e-02,  2.3920e-03,  1.9933e-02, -5.1718e-03,\n",
      "         2.4679e-02,  3.1648e-02,  3.3778e-02,  2.8280e-02, -3.0736e-02,\n",
      "         1.4679e-02, -2.1490e-02, -1.5910e-02, -3.4204e-02, -3.6788e-02,\n",
      "         3.3354e-02, -2.9069e-03,  2.8820e-02,  3.8679e-02, -2.8400e-02,\n",
      "         1.1979e-02,  2.6168e-02,  3.9946e-02,  4.0310e-02,  1.8609e-02,\n",
      "        -4.1103e-02,  2.7008e-02,  4.2743e-02, -3.1683e-03, -3.0279e-02,\n",
      "         2.5710e-02,  3.7383e-02, -2.1197e-02,  2.2708e-02, -4.2422e-02,\n",
      "         3.7280e-02, -2.0359e-02, -3.0157e-03,  3.4316e-02,  6.7439e-03,\n",
      "         2.4612e-04, -3.5880e-02, -4.1755e-02, -3.8700e-02, -4.1624e-02,\n",
      "         1.8720e-02,  4.2409e-02,  2.5815e-02,  1.1577e-02, -4.1043e-02,\n",
      "        -2.3002e-03, -3.4853e-02,  3.8617e-02, -2.4050e-02, -2.0056e-02,\n",
      "         2.8247e-02,  3.3182e-02,  2.1098e-02,  2.9222e-02, -1.8749e-02,\n",
      "        -2.4707e-02,  5.5640e-03,  3.5089e-02, -1.1001e-02, -3.8201e-02,\n",
      "         3.1635e-02,  2.3236e-02,  2.4273e-02, -3.1115e-03, -1.5420e-02,\n",
      "         4.3964e-02, -1.4401e-02, -6.0152e-03,  3.2525e-02, -2.7180e-02,\n",
      "        -2.2690e-02,  4.3622e-02,  2.5715e-02,  3.6379e-02, -4.1415e-02,\n",
      "        -3.8484e-02, -4.3007e-02, -3.9885e-02, -1.2271e-02, -3.7029e-02,\n",
      "         2.4156e-02,  1.5509e-02,  7.1704e-03, -4.0322e-02,  3.4739e-02,\n",
      "        -3.2848e-02,  1.3377e-02, -3.5581e-03,  1.7323e-02,  1.5545e-03,\n",
      "         4.1564e-02,  3.3363e-02,  4.0752e-02, -1.7113e-02,  1.2511e-02,\n",
      "         1.6957e-02, -5.8650e-03,  2.2142e-02,  3.5512e-02, -4.3793e-02,\n",
      "         9.2476e-03, -4.3238e-02,  4.2163e-02,  4.2876e-02, -1.6486e-02,\n",
      "         5.7445e-03, -1.6908e-02,  1.7725e-02, -2.1374e-02,  3.3253e-02,\n",
      "        -4.2019e-02,  4.2907e-02, -2.9750e-02, -1.1331e-02, -4.3203e-02,\n",
      "         2.7376e-02, -5.8672e-03, -2.2327e-02, -2.1135e-03, -7.7616e-04,\n",
      "        -3.3093e-02, -1.2653e-03, -2.1780e-02,  2.4166e-02,  3.9501e-02,\n",
      "         2.5473e-02,  2.6114e-02, -7.1739e-03,  3.4049e-02, -2.8074e-02,\n",
      "         3.0200e-02, -4.3572e-02, -3.6449e-02, -4.1135e-02, -4.0824e-02,\n",
      "         9.1651e-03, -1.6268e-03,  3.2338e-02,  4.0208e-02,  9.5292e-03,\n",
      "        -3.3993e-02, -1.3784e-02,  4.1862e-02, -1.4244e-03, -3.2442e-02,\n",
      "        -3.7099e-03,  2.8875e-03,  2.6717e-02, -4.4080e-02,  3.4985e-02,\n",
      "         4.0353e-02,  2.5509e-03, -4.3605e-02,  2.5381e-02, -2.3312e-02,\n",
      "         3.1031e-02, -1.1979e-02,  1.8429e-02, -3.8399e-02,  1.1549e-02,\n",
      "         1.7698e-02, -3.3797e-02,  1.3796e-02, -3.6931e-02,  1.2257e-02,\n",
      "        -3.4840e-02, -1.3801e-02,  2.5995e-02,  1.6186e-02, -1.4539e-02,\n",
      "        -4.3308e-02, -1.6541e-02,  4.1238e-02, -8.9677e-03,  3.7361e-02,\n",
      "        -1.9966e-02, -7.7926e-03, -2.3459e-02, -2.1014e-02, -4.5359e-03,\n",
      "        -3.4481e-02,  1.7836e-02, -8.3524e-03,  4.3198e-02, -3.8543e-02,\n",
      "         2.9865e-02, -2.8399e-02, -2.7506e-02,  2.1277e-02, -2.3280e-03,\n",
      "         1.3669e-02, -2.5450e-02,  3.2072e-02, -1.0507e-02,  3.4263e-02,\n",
      "        -1.6659e-03,  7.1772e-03, -1.3909e-02, -1.1141e-02,  1.1945e-02,\n",
      "         1.8477e-02,  2.8687e-02,  1.7458e-02, -1.2866e-02,  3.3332e-02,\n",
      "        -6.5532e-03,  3.3106e-02,  2.7157e-02, -2.1113e-02, -3.0931e-02,\n",
      "        -3.1794e-02, -3.3882e-02,  3.5639e-02, -2.1552e-02, -4.2671e-02,\n",
      "         2.9036e-02,  4.1080e-02,  2.9550e-02, -3.6451e-02,  2.3034e-02,\n",
      "        -3.1654e-02, -1.1901e-02,  2.5274e-02, -9.1752e-03, -3.2908e-02,\n",
      "        -8.6679e-03,  3.1725e-02, -3.3964e-02, -5.8827e-03, -2.7774e-02,\n",
      "        -3.4311e-02, -2.6318e-02,  4.3508e-02,  2.8496e-02,  5.7827e-03,\n",
      "         2.8820e-02,  1.9026e-02, -3.2005e-02, -9.2645e-03,  1.1402e-02,\n",
      "         3.2795e-02,  3.8477e-04,  3.7628e-02,  3.5346e-02, -1.5637e-03,\n",
      "        -1.0768e-02,  4.1522e-02,  6.6700e-03, -3.9449e-02, -4.0165e-02,\n",
      "         3.7455e-02, -3.3460e-02,  2.0592e-02,  1.3683e-02,  1.1266e-03,\n",
      "         2.7405e-03, -4.2846e-02,  2.1298e-02, -3.1262e-03, -2.7306e-02,\n",
      "         2.8795e-02,  1.1685e-02,  2.1963e-02,  6.1692e-04, -3.4703e-02,\n",
      "         9.8610e-03,  4.4035e-02,  2.9943e-02, -4.0085e-02,  1.5578e-02,\n",
      "        -3.1401e-02,  3.7632e-02, -1.6857e-02, -1.9540e-02,  1.4586e-02,\n",
      "        -3.7210e-02, -1.9152e-02, -4.0817e-02, -4.0487e-02, -1.1998e-02,\n",
      "        -2.5090e-02, -1.3640e-02,  3.1944e-03,  4.1146e-02, -3.3651e-02,\n",
      "        -2.0305e-02, -4.0429e-03,  3.8601e-02,  3.7540e-03,  1.2358e-02,\n",
      "         8.0923e-03, -3.1890e-02,  9.0405e-03, -3.2865e-02,  2.3512e-02,\n",
      "        -1.0646e-02, -6.4079e-05,  3.0953e-02, -4.3657e-02,  1.2613e-02,\n",
      "        -3.4982e-02,  2.3734e-02, -2.5533e-02,  2.0114e-02,  1.3495e-02,\n",
      "        -4.0408e-02,  4.3891e-02,  3.2588e-02, -1.2744e-02, -6.1904e-03,\n",
      "        -4.0896e-02,  1.7735e-02, -1.8354e-02,  1.0876e-02, -1.9555e-02,\n",
      "         2.6990e-02, -3.7930e-02, -1.7447e-02, -3.7880e-02, -3.3116e-02,\n",
      "         3.9245e-02,  3.9222e-02, -4.0082e-02, -1.5525e-02,  8.3313e-03,\n",
      "        -1.0682e-02,  1.0559e-02, -4.1279e-03,  2.0919e-02,  3.9888e-02,\n",
      "        -3.7961e-02, -4.1791e-03, -3.1308e-02,  2.9455e-02, -2.3184e-03,\n",
      "         2.9002e-02,  3.1627e-02, -1.4581e-03,  7.1750e-04, -4.9936e-03,\n",
      "         3.7277e-02, -3.9093e-02,  2.8273e-02, -7.2325e-03,  2.6456e-02,\n",
      "         1.3428e-02, -1.1590e-02, -9.0277e-03,  2.5187e-02,  3.5403e-02,\n",
      "        -2.3445e-02,  1.2677e-02,  2.6261e-02,  2.8086e-02,  9.2215e-03,\n",
      "        -3.9261e-03, -1.6630e-02,  1.8094e-02,  4.2168e-02,  3.5390e-02,\n",
      "         1.0590e-02,  2.9856e-02,  1.8428e-02,  2.8050e-02, -3.0886e-02,\n",
      "         2.4316e-03,  1.0007e-02, -1.8302e-02, -1.3998e-02,  9.9709e-03,\n",
      "         5.4165e-04,  2.8919e-02,  4.0630e-02, -2.6261e-02, -1.0705e-02,\n",
      "        -3.1927e-04, -3.1478e-02,  1.5693e-02, -1.9394e-02,  4.0839e-02,\n",
      "        -3.4287e-02,  2.1058e-02, -4.0400e-02,  3.9859e-02, -1.4393e-02,\n",
      "         1.1794e-02, -3.7647e-02,  3.3082e-02, -3.1598e-02, -4.0801e-02,\n",
      "         2.9193e-02,  3.0060e-03,  3.5513e-02,  2.0897e-02, -1.0233e-02,\n",
      "        -2.2266e-02, -2.3217e-02, -8.3686e-03, -7.7014e-03, -1.2246e-03,\n",
      "         4.2030e-02, -2.5526e-02, -3.4496e-02,  2.8890e-02, -2.9117e-02,\n",
      "        -3.6492e-02,  2.0533e-02,  2.5642e-02, -1.6092e-02,  3.6677e-02,\n",
      "         1.7104e-02, -2.9482e-02,  9.8697e-04,  9.7174e-03, -3.3913e-02,\n",
      "         1.5809e-02,  2.9647e-03,  2.8790e-02,  3.9334e-02,  2.1073e-02,\n",
      "         2.2489e-03, -1.2810e-02,  3.8203e-02,  1.0557e-03,  1.9733e-02,\n",
      "         1.4967e-02,  1.7334e-02,  1.3486e-02, -4.9774e-03, -3.3356e-03,\n",
      "        -1.6483e-02,  3.4807e-02,  2.9799e-02, -2.2900e-02,  2.1355e-02,\n",
      "         1.4287e-02, -1.2257e-02,  1.9343e-02, -2.4151e-04,  4.8523e-03,\n",
      "        -2.8394e-02,  1.4600e-02,  2.1612e-02, -3.9485e-02, -2.1343e-02,\n",
      "         2.2399e-02,  1.5797e-02, -4.3395e-02, -1.9832e-02,  2.9439e-02,\n",
      "        -8.2538e-03,  4.3666e-02, -1.4292e-02,  1.0440e-03,  2.7911e-02,\n",
      "        -2.6805e-02,  3.8013e-03, -5.0017e-03,  4.0159e-02,  2.3090e-02,\n",
      "         3.3218e-02,  3.5906e-02, -5.7731e-03, -1.9321e-03,  4.0921e-02,\n",
      "         3.7954e-03, -2.7409e-03,  2.1960e-02,  3.3413e-02, -3.7034e-02,\n",
      "         4.3794e-02, -3.9035e-03, -2.3826e-03,  3.4734e-02, -1.6741e-03,\n",
      "        -6.3292e-03,  3.9883e-02,  3.0560e-02, -2.4936e-02, -2.7012e-02,\n",
      "        -4.3225e-02, -2.5587e-02,  3.5557e-02, -4.1017e-02, -1.0345e-02,\n",
      "        -2.6719e-02, -1.5445e-02, -4.3772e-02, -3.0332e-02,  1.4563e-02,\n",
      "         3.6111e-02,  1.1043e-02], requires_grad=True))\n",
      "('encoder.layers.0.ff.linear_1.weight', Parameter containing:\n",
      "tensor([[ 0.0126, -0.0369,  0.0407,  ..., -0.0400,  0.0002,  0.0015],\n",
      "        [-0.0333,  0.0449,  0.0176,  ..., -0.0080, -0.0285, -0.0219],\n",
      "        [-0.0206, -0.0003,  0.0372,  ..., -0.0008,  0.0142,  0.0277],\n",
      "        ...,\n",
      "        [-0.0036, -0.0385,  0.0429,  ..., -0.0360, -0.0364, -0.0122],\n",
      "        [ 0.0361, -0.0297,  0.0410,  ...,  0.0002,  0.0373, -0.0208],\n",
      "        [ 0.0273, -0.0335,  0.0278,  ..., -0.0215, -0.0184,  0.0392]],\n",
      "       requires_grad=True))\n",
      "('encoder.layers.0.ff.linear_1.bias', Parameter containing:\n",
      "tensor([ 0.0300,  0.0145,  0.0123,  ..., -0.0311, -0.0128, -0.0362],\n",
      "       requires_grad=True))\n",
      "('encoder.layers.0.ff.linear_2.weight', Parameter containing:\n",
      "tensor([[-0.0034, -0.0220, -0.0294,  ..., -0.0128, -0.0414, -0.0445],\n",
      "        [-0.0176,  0.0209,  0.0201,  ..., -0.0312,  0.0375, -0.0375],\n",
      "        [-0.0216, -0.0150,  0.0279,  ..., -0.0459, -0.0390, -0.0349],\n",
      "        ...,\n",
      "        [-0.0340,  0.0150,  0.0227,  ...,  0.0204, -0.0383, -0.0314],\n",
      "        [-0.0190,  0.0238, -0.0044,  ..., -0.0160, -0.0368, -0.0260],\n",
      "        [ 0.0142,  0.0140, -0.0231,  ...,  0.0239, -0.0331,  0.0397]],\n",
      "       requires_grad=True))\n",
      "('encoder.layers.0.ff.linear_2.bias', Parameter containing:\n",
      "tensor([-0.0191, -0.0202,  0.0098, -0.0181, -0.0091,  0.0018,  0.0116,  0.0048,\n",
      "         0.0055,  0.0112, -0.0203, -0.0175,  0.0147, -0.0040, -0.0033,  0.0004,\n",
      "        -0.0056,  0.0156, -0.0197,  0.0066, -0.0140,  0.0011,  0.0142, -0.0146,\n",
      "         0.0188, -0.0213,  0.0108,  0.0170, -0.0097, -0.0134, -0.0072,  0.0113,\n",
      "         0.0011,  0.0132, -0.0068,  0.0096, -0.0127, -0.0034, -0.0195, -0.0022,\n",
      "         0.0043,  0.0205,  0.0031, -0.0129, -0.0078,  0.0134,  0.0099, -0.0067,\n",
      "         0.0219,  0.0180,  0.0102,  0.0100, -0.0127,  0.0028,  0.0151,  0.0198,\n",
      "        -0.0087, -0.0192, -0.0034,  0.0089, -0.0078, -0.0044, -0.0132,  0.0065,\n",
      "         0.0005,  0.0014, -0.0204, -0.0209, -0.0114, -0.0159, -0.0128, -0.0135,\n",
      "         0.0041, -0.0166,  0.0195,  0.0126, -0.0057, -0.0077, -0.0046,  0.0139,\n",
      "        -0.0019, -0.0089, -0.0207, -0.0004, -0.0063, -0.0045,  0.0008, -0.0149,\n",
      "        -0.0003, -0.0022,  0.0057, -0.0205, -0.0174,  0.0157, -0.0022,  0.0093,\n",
      "         0.0030, -0.0207, -0.0084,  0.0014,  0.0164, -0.0020,  0.0060, -0.0011,\n",
      "        -0.0144,  0.0045,  0.0046, -0.0118, -0.0011,  0.0061,  0.0099, -0.0023,\n",
      "        -0.0121, -0.0030, -0.0007, -0.0188,  0.0141,  0.0096, -0.0086, -0.0206,\n",
      "        -0.0056, -0.0099,  0.0128,  0.0138, -0.0069,  0.0210, -0.0083,  0.0182,\n",
      "        -0.0162, -0.0022,  0.0093,  0.0220, -0.0170, -0.0040, -0.0093,  0.0029,\n",
      "         0.0195,  0.0208, -0.0105,  0.0044, -0.0087,  0.0027,  0.0052,  0.0203,\n",
      "         0.0220,  0.0176,  0.0129, -0.0044,  0.0050,  0.0179, -0.0048, -0.0024,\n",
      "        -0.0194,  0.0129,  0.0150, -0.0208, -0.0137, -0.0037, -0.0081, -0.0053,\n",
      "        -0.0138,  0.0187,  0.0139,  0.0076, -0.0073,  0.0198, -0.0184,  0.0200,\n",
      "        -0.0131, -0.0174,  0.0200, -0.0097,  0.0193,  0.0092,  0.0124,  0.0074,\n",
      "         0.0158, -0.0151,  0.0213, -0.0167, -0.0011, -0.0051,  0.0153, -0.0134,\n",
      "        -0.0015,  0.0192,  0.0009, -0.0097,  0.0185, -0.0136,  0.0024, -0.0179,\n",
      "         0.0126, -0.0019,  0.0053, -0.0085, -0.0048, -0.0090, -0.0194,  0.0047,\n",
      "        -0.0091, -0.0087,  0.0086,  0.0149, -0.0129,  0.0041, -0.0071, -0.0159,\n",
      "         0.0115,  0.0055,  0.0101, -0.0104, -0.0177, -0.0124, -0.0188,  0.0035,\n",
      "         0.0014,  0.0219,  0.0019,  0.0161, -0.0129,  0.0038,  0.0049,  0.0046,\n",
      "         0.0043,  0.0146,  0.0037,  0.0208,  0.0213, -0.0132,  0.0133, -0.0108,\n",
      "        -0.0060,  0.0205,  0.0191, -0.0012,  0.0152,  0.0114, -0.0178, -0.0196,\n",
      "        -0.0138, -0.0068,  0.0171,  0.0218, -0.0133,  0.0169, -0.0196,  0.0181,\n",
      "        -0.0157, -0.0050, -0.0171, -0.0191, -0.0094, -0.0195, -0.0025,  0.0028,\n",
      "         0.0180, -0.0162,  0.0152,  0.0007,  0.0033,  0.0209,  0.0009, -0.0120,\n",
      "         0.0169, -0.0162, -0.0017,  0.0145, -0.0178,  0.0081,  0.0209,  0.0102,\n",
      "         0.0115,  0.0089, -0.0041,  0.0043,  0.0091, -0.0096,  0.0171,  0.0027,\n",
      "         0.0107,  0.0002, -0.0160, -0.0100,  0.0120,  0.0103, -0.0083, -0.0160,\n",
      "        -0.0116, -0.0115, -0.0131, -0.0166,  0.0218,  0.0123, -0.0006, -0.0035,\n",
      "         0.0202,  0.0182, -0.0111, -0.0162, -0.0192,  0.0013, -0.0200, -0.0170,\n",
      "         0.0203, -0.0100, -0.0040, -0.0211, -0.0096, -0.0215, -0.0155, -0.0112,\n",
      "        -0.0152, -0.0086,  0.0041,  0.0139, -0.0061,  0.0046,  0.0024, -0.0032,\n",
      "        -0.0169, -0.0019, -0.0150,  0.0007,  0.0168, -0.0035, -0.0058,  0.0081,\n",
      "        -0.0153, -0.0188,  0.0102, -0.0053,  0.0198, -0.0168,  0.0158,  0.0166,\n",
      "         0.0040, -0.0122,  0.0090, -0.0177, -0.0089,  0.0177, -0.0180, -0.0199,\n",
      "        -0.0059,  0.0070,  0.0153, -0.0118, -0.0067,  0.0137, -0.0163,  0.0013,\n",
      "        -0.0044,  0.0046,  0.0204, -0.0152, -0.0048, -0.0036,  0.0118, -0.0187,\n",
      "         0.0004,  0.0221,  0.0168, -0.0138, -0.0195,  0.0161,  0.0168, -0.0049,\n",
      "         0.0025, -0.0087, -0.0203,  0.0124,  0.0189, -0.0060,  0.0031,  0.0017,\n",
      "        -0.0128,  0.0040, -0.0046, -0.0025,  0.0138, -0.0019,  0.0130,  0.0175,\n",
      "        -0.0136, -0.0206,  0.0006, -0.0077,  0.0157, -0.0125,  0.0164,  0.0122,\n",
      "        -0.0060,  0.0066, -0.0008,  0.0022,  0.0115,  0.0073, -0.0084, -0.0060,\n",
      "        -0.0036,  0.0213, -0.0075, -0.0152, -0.0171, -0.0113, -0.0217,  0.0021,\n",
      "         0.0163,  0.0121, -0.0150,  0.0172, -0.0100,  0.0080, -0.0143,  0.0135,\n",
      "         0.0172,  0.0145, -0.0161,  0.0059,  0.0160,  0.0036, -0.0187, -0.0111,\n",
      "         0.0004,  0.0032,  0.0171,  0.0176,  0.0091, -0.0121,  0.0206, -0.0178,\n",
      "         0.0153, -0.0013,  0.0015,  0.0048, -0.0149, -0.0143,  0.0060,  0.0115,\n",
      "         0.0135, -0.0206, -0.0074,  0.0212, -0.0136,  0.0163,  0.0065, -0.0152,\n",
      "         0.0118,  0.0061, -0.0018,  0.0114, -0.0069,  0.0026,  0.0058,  0.0075,\n",
      "        -0.0064,  0.0074, -0.0082,  0.0187,  0.0077,  0.0038, -0.0180,  0.0127,\n",
      "        -0.0017,  0.0134,  0.0093,  0.0215,  0.0125,  0.0032, -0.0167,  0.0013,\n",
      "        -0.0049, -0.0026,  0.0149, -0.0078,  0.0107,  0.0127, -0.0066,  0.0028,\n",
      "         0.0050,  0.0092, -0.0035,  0.0107,  0.0184,  0.0192, -0.0077, -0.0024,\n",
      "         0.0208, -0.0113,  0.0220,  0.0099,  0.0011,  0.0076,  0.0211,  0.0100,\n",
      "         0.0149,  0.0186, -0.0046,  0.0104, -0.0004,  0.0069,  0.0072, -0.0065,\n",
      "        -0.0013, -0.0110, -0.0083,  0.0036,  0.0124, -0.0080, -0.0174,  0.0113],\n",
      "       requires_grad=True))\n",
      "('encoder.layers.1.norm_1.alpha', Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True))\n",
      "('encoder.layers.1.norm_1.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))\n",
      "('encoder.layers.1.norm_2.alpha', Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True))\n",
      "('encoder.layers.1.norm_2.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))\n",
      "('encoder.layers.1.attn.q_linear.weight', Parameter containing:\n",
      "tensor([[-0.0419, -0.0617, -0.0066,  ...,  0.0182,  0.0039,  0.0252],\n",
      "        [ 0.0294, -0.0163,  0.0326,  ...,  0.0625, -0.0453,  0.0605],\n",
      "        [-0.0156, -0.0559, -0.0674,  ..., -0.0049,  0.0123,  0.0238],\n",
      "        ...,\n",
      "        [ 0.0053, -0.0434,  0.0199,  ...,  0.0468, -0.0430, -0.0198],\n",
      "        [ 0.0487, -0.0093, -0.0104,  ..., -0.0025, -0.0081, -0.0137],\n",
      "        [-0.0746, -0.0434, -0.0735,  ...,  0.0657,  0.0393,  0.0154]],\n",
      "       requires_grad=True))\n",
      "('encoder.layers.1.attn.q_linear.bias', Parameter containing:\n",
      "tensor([ 0.0037, -0.0255,  0.0169, -0.0331,  0.0107,  0.0101,  0.0304,  0.0379,\n",
      "        -0.0097,  0.0260,  0.0218,  0.0417, -0.0303,  0.0176, -0.0309, -0.0430,\n",
      "         0.0238, -0.0074, -0.0114,  0.0167, -0.0358, -0.0360, -0.0149,  0.0043,\n",
      "        -0.0048,  0.0150, -0.0285, -0.0426,  0.0094, -0.0173, -0.0380, -0.0102,\n",
      "         0.0309,  0.0027, -0.0394,  0.0356, -0.0072, -0.0125, -0.0017,  0.0092,\n",
      "         0.0202,  0.0127,  0.0399,  0.0216, -0.0212,  0.0311,  0.0224, -0.0084,\n",
      "         0.0053,  0.0103,  0.0417, -0.0023, -0.0191, -0.0258, -0.0392,  0.0277,\n",
      "         0.0173,  0.0373, -0.0245, -0.0335,  0.0165,  0.0316,  0.0409, -0.0081,\n",
      "        -0.0322,  0.0059, -0.0097, -0.0315,  0.0275,  0.0417,  0.0401, -0.0423,\n",
      "        -0.0021,  0.0090,  0.0405,  0.0289,  0.0138, -0.0350, -0.0347, -0.0119,\n",
      "        -0.0327,  0.0427, -0.0294,  0.0197,  0.0057,  0.0230, -0.0014,  0.0094,\n",
      "        -0.0130,  0.0073, -0.0301,  0.0058,  0.0283, -0.0328, -0.0093, -0.0205,\n",
      "         0.0124, -0.0295, -0.0362,  0.0110,  0.0352, -0.0014, -0.0147,  0.0026,\n",
      "        -0.0021,  0.0373, -0.0169,  0.0221, -0.0183, -0.0347,  0.0166,  0.0411,\n",
      "         0.0097,  0.0383,  0.0105, -0.0187,  0.0292, -0.0311, -0.0054, -0.0376,\n",
      "        -0.0035, -0.0335, -0.0395, -0.0290,  0.0184,  0.0061, -0.0029, -0.0261,\n",
      "         0.0081, -0.0248,  0.0216,  0.0435,  0.0049, -0.0248,  0.0178, -0.0417,\n",
      "         0.0046, -0.0085, -0.0037,  0.0167,  0.0268,  0.0214, -0.0002,  0.0087,\n",
      "         0.0216, -0.0289, -0.0435,  0.0248,  0.0219,  0.0374, -0.0103,  0.0178,\n",
      "        -0.0196, -0.0183, -0.0039, -0.0123,  0.0441,  0.0344, -0.0170, -0.0327,\n",
      "         0.0031,  0.0403,  0.0421,  0.0155, -0.0193, -0.0438,  0.0279, -0.0390,\n",
      "        -0.0025,  0.0439, -0.0010,  0.0180, -0.0370, -0.0361, -0.0065, -0.0079,\n",
      "        -0.0313, -0.0135, -0.0324, -0.0410, -0.0143, -0.0335,  0.0306, -0.0280,\n",
      "        -0.0004, -0.0124, -0.0081,  0.0030, -0.0191, -0.0208, -0.0217, -0.0232,\n",
      "         0.0089, -0.0083, -0.0104, -0.0048,  0.0274, -0.0127, -0.0288,  0.0384,\n",
      "         0.0272, -0.0144, -0.0121, -0.0184,  0.0394,  0.0353,  0.0361,  0.0419,\n",
      "        -0.0144,  0.0428, -0.0371,  0.0312, -0.0044, -0.0170,  0.0262,  0.0397,\n",
      "         0.0410, -0.0280,  0.0066,  0.0355, -0.0214,  0.0383,  0.0049,  0.0150,\n",
      "        -0.0049, -0.0141, -0.0122,  0.0345, -0.0268, -0.0331,  0.0371,  0.0335,\n",
      "         0.0208, -0.0328, -0.0370,  0.0182,  0.0078, -0.0385, -0.0178, -0.0399,\n",
      "        -0.0116,  0.0287, -0.0139,  0.0084,  0.0230, -0.0364,  0.0236,  0.0218,\n",
      "        -0.0149, -0.0036, -0.0107, -0.0052, -0.0342, -0.0164, -0.0116,  0.0315,\n",
      "        -0.0323,  0.0271, -0.0114,  0.0366,  0.0167, -0.0101,  0.0063,  0.0352,\n",
      "         0.0366, -0.0245, -0.0193,  0.0115,  0.0005, -0.0404,  0.0032,  0.0237,\n",
      "        -0.0327,  0.0330, -0.0214, -0.0249,  0.0040,  0.0294, -0.0307,  0.0220,\n",
      "         0.0227, -0.0424,  0.0101,  0.0261, -0.0301,  0.0232, -0.0074, -0.0027,\n",
      "        -0.0110, -0.0421, -0.0341, -0.0375,  0.0367,  0.0201,  0.0216, -0.0221,\n",
      "         0.0121,  0.0123,  0.0011,  0.0357,  0.0343, -0.0065,  0.0032, -0.0132,\n",
      "         0.0298,  0.0157, -0.0166,  0.0048,  0.0338,  0.0369, -0.0315, -0.0320,\n",
      "        -0.0418, -0.0441,  0.0062,  0.0398, -0.0191,  0.0126,  0.0055, -0.0017,\n",
      "         0.0270,  0.0223, -0.0040,  0.0306, -0.0057,  0.0024, -0.0400, -0.0041,\n",
      "         0.0172, -0.0377, -0.0276, -0.0237,  0.0428, -0.0330,  0.0380,  0.0430,\n",
      "        -0.0216, -0.0205,  0.0130, -0.0268, -0.0084,  0.0393, -0.0429,  0.0063,\n",
      "         0.0284,  0.0134,  0.0224,  0.0202, -0.0012,  0.0131,  0.0151,  0.0341,\n",
      "         0.0066, -0.0254,  0.0018,  0.0264, -0.0141,  0.0182,  0.0022,  0.0299,\n",
      "        -0.0309,  0.0433, -0.0062, -0.0119,  0.0133,  0.0331,  0.0083, -0.0292,\n",
      "         0.0238,  0.0323, -0.0008, -0.0237, -0.0045,  0.0246, -0.0194, -0.0231,\n",
      "        -0.0327,  0.0414, -0.0289, -0.0342,  0.0324,  0.0251, -0.0156,  0.0079,\n",
      "        -0.0037, -0.0124, -0.0254,  0.0001,  0.0375, -0.0376,  0.0144, -0.0183,\n",
      "        -0.0313,  0.0146, -0.0395, -0.0202,  0.0293,  0.0299, -0.0205,  0.0363,\n",
      "        -0.0068,  0.0183, -0.0390, -0.0326,  0.0383,  0.0324,  0.0074, -0.0146,\n",
      "        -0.0050, -0.0101, -0.0414,  0.0151, -0.0316,  0.0319,  0.0188, -0.0413,\n",
      "        -0.0296,  0.0041, -0.0290,  0.0402, -0.0003,  0.0425, -0.0035,  0.0211,\n",
      "         0.0393, -0.0331,  0.0223,  0.0265, -0.0279, -0.0205, -0.0233, -0.0169,\n",
      "        -0.0118, -0.0391, -0.0327, -0.0169, -0.0170, -0.0298,  0.0085, -0.0274,\n",
      "         0.0216,  0.0299, -0.0331,  0.0376, -0.0394,  0.0259,  0.0022, -0.0055,\n",
      "         0.0033,  0.0048,  0.0219, -0.0085,  0.0388, -0.0075, -0.0249, -0.0128,\n",
      "         0.0300, -0.0179, -0.0366, -0.0409, -0.0308,  0.0340, -0.0272, -0.0118,\n",
      "         0.0388,  0.0419,  0.0168, -0.0420, -0.0360,  0.0100, -0.0208, -0.0411,\n",
      "         0.0340, -0.0215, -0.0098,  0.0176, -0.0420,  0.0214,  0.0045, -0.0278,\n",
      "        -0.0265,  0.0322,  0.0229, -0.0328,  0.0039, -0.0211,  0.0072,  0.0363,\n",
      "         0.0186,  0.0341, -0.0074, -0.0435, -0.0322,  0.0327, -0.0181,  0.0188,\n",
      "         0.0196,  0.0003,  0.0204, -0.0376,  0.0309, -0.0105,  0.0175, -0.0253,\n",
      "        -0.0263, -0.0417,  0.0046, -0.0128, -0.0029,  0.0165,  0.0078, -0.0064],\n",
      "       requires_grad=True))\n",
      "('encoder.layers.1.attn.v_linear.weight', Parameter containing:\n",
      "tensor([[ 0.0551,  0.0232, -0.0653,  ...,  0.0727, -0.0446,  0.0291],\n",
      "        [ 0.0708,  0.0529,  0.0243,  ..., -0.0401,  0.0147, -0.0118],\n",
      "        [-0.0362, -0.0406,  0.0106,  ..., -0.0753,  0.0115,  0.0186],\n",
      "        ...,\n",
      "        [ 0.0330, -0.0148, -0.0310,  ...,  0.0699,  0.0291,  0.0152],\n",
      "        [ 0.0537,  0.0380,  0.0046,  ..., -0.0555,  0.0250, -0.0024],\n",
      "        [ 0.0429, -0.0199, -0.0256,  ..., -0.0360,  0.0190,  0.0188]],\n",
      "       requires_grad=True))\n",
      "('encoder.layers.1.attn.v_linear.bias', Parameter containing:\n",
      "tensor([ 4.2113e-02,  1.5851e-02, -1.7450e-02, -4.2895e-02, -6.4087e-03,\n",
      "        -5.0386e-03,  3.6729e-02, -3.2138e-02,  2.8940e-02,  1.4764e-02,\n",
      "         4.2312e-02,  3.6593e-02, -4.0668e-02,  3.9951e-02, -3.3939e-02,\n",
      "         8.0692e-03, -3.5519e-02,  1.5025e-02, -1.0814e-02,  3.9151e-02,\n",
      "         2.9167e-02,  2.4215e-03,  2.7750e-02, -4.8392e-03,  6.7985e-03,\n",
      "        -1.0831e-02, -3.8153e-02, -2.9198e-02, -2.9042e-02, -2.2730e-02,\n",
      "         2.2331e-03,  3.4279e-02, -2.1103e-02, -3.9359e-03, -2.3910e-02,\n",
      "         3.0465e-02, -5.5797e-03, -2.6073e-02, -5.4225e-03, -4.0814e-02,\n",
      "         2.1841e-02, -2.5196e-02,  6.4510e-03,  2.9795e-02, -3.5295e-02,\n",
      "         1.2397e-03, -3.0160e-02, -1.4703e-02, -1.5040e-02, -1.5722e-02,\n",
      "        -1.8178e-02,  1.6263e-02,  2.5149e-02, -3.4318e-02, -2.8332e-02,\n",
      "        -6.3897e-03, -2.7346e-02, -4.3754e-03,  2.9059e-02, -2.8649e-02,\n",
      "        -3.8368e-02,  2.4827e-02,  2.2584e-02, -2.3601e-02, -5.7184e-03,\n",
      "         1.4377e-02,  5.0146e-03,  1.3779e-02,  2.3479e-02,  7.6500e-03,\n",
      "         1.7557e-02,  3.7502e-02,  4.3230e-02,  4.9483e-03, -3.1885e-02,\n",
      "        -1.1206e-02, -3.2392e-02, -2.5045e-02, -3.1097e-03,  1.5950e-02,\n",
      "        -6.6972e-03, -2.0565e-02,  6.4086e-03,  2.3452e-02, -4.2593e-02,\n",
      "        -3.4376e-02,  2.4187e-02,  4.1653e-02,  2.3370e-02,  1.5631e-02,\n",
      "         2.3012e-02,  2.2945e-02,  3.1861e-02, -1.6558e-02,  4.3123e-02,\n",
      "         9.2278e-03, -2.7086e-03,  7.8744e-03,  8.0697e-03,  3.2652e-02,\n",
      "        -1.6191e-02,  3.4992e-02,  2.9780e-02,  3.4435e-02,  3.4536e-02,\n",
      "         1.2392e-02, -1.0935e-02, -2.3384e-02,  1.8899e-02,  2.3674e-02,\n",
      "        -4.2509e-02, -1.9072e-02, -1.4387e-02,  2.6450e-02, -2.0853e-02,\n",
      "         3.4762e-02,  3.5390e-02,  6.2467e-04,  2.8475e-02,  1.5546e-02,\n",
      "         2.3763e-03,  4.4156e-02,  3.5264e-02,  2.6563e-02,  4.0614e-02,\n",
      "         2.3431e-02,  2.8680e-02,  1.0449e-02, -3.4043e-02,  2.9406e-02,\n",
      "         2.7141e-02,  4.1027e-02, -3.9708e-02,  3.2239e-02,  1.1796e-02,\n",
      "        -3.3470e-02, -2.1774e-02,  3.9180e-02,  2.8233e-02, -1.6859e-02,\n",
      "        -3.9849e-03, -3.2929e-02, -1.8275e-02, -2.9675e-02, -2.9361e-02,\n",
      "         4.3643e-02, -2.0559e-02,  3.3972e-02,  2.3274e-02, -1.8623e-02,\n",
      "         1.4036e-02,  5.9390e-03, -3.9519e-02,  4.3914e-02, -1.4694e-02,\n",
      "         1.2145e-02,  3.0321e-02, -3.2974e-02, -2.2487e-02,  1.6057e-02,\n",
      "        -9.3509e-03, -1.1845e-02,  3.5449e-02, -3.3716e-02, -9.2715e-03,\n",
      "         3.1231e-02, -1.9629e-02, -1.8580e-02, -1.3048e-02,  2.2357e-02,\n",
      "        -3.0027e-02, -3.8051e-02,  1.7769e-02, -9.2919e-03, -2.8475e-03,\n",
      "        -1.3224e-02, -3.0929e-03,  2.5307e-02,  1.0416e-02,  7.1552e-03,\n",
      "         1.9898e-02,  2.6357e-02,  4.2880e-02,  3.2909e-02,  2.1154e-02,\n",
      "         3.4663e-02,  3.0058e-02, -4.1757e-02, -1.2631e-02,  1.7101e-02,\n",
      "        -3.4388e-02, -4.2318e-02,  2.4507e-02, -6.2388e-03, -2.6327e-02,\n",
      "        -3.9174e-02, -4.3351e-02,  8.0261e-03,  2.7526e-02,  3.2893e-02,\n",
      "         1.6539e-02, -4.2587e-02,  3.1007e-02, -9.6455e-03, -2.8560e-02,\n",
      "         4.2560e-02,  2.6695e-02,  1.9342e-02, -3.7470e-02,  1.8871e-02,\n",
      "         7.0978e-03,  3.9967e-02,  1.5689e-02,  2.8680e-02, -7.0472e-03,\n",
      "         8.1956e-03, -1.6854e-02, -2.6265e-02,  1.1372e-04, -8.6597e-03,\n",
      "         1.0542e-02, -1.5425e-02,  3.6033e-02,  7.3687e-03, -4.2952e-02,\n",
      "        -1.7665e-02, -2.8266e-02, -2.4220e-02, -2.0369e-02, -2.0754e-02,\n",
      "        -3.1167e-02, -3.2624e-02, -1.1506e-02, -3.4007e-02,  4.0915e-03,\n",
      "         4.0841e-02,  3.7551e-02,  1.1735e-02, -5.3644e-06, -3.9358e-02,\n",
      "         3.8169e-02, -1.6149e-02, -3.4050e-02, -7.3740e-03,  4.0952e-02,\n",
      "        -8.0734e-03, -1.5045e-03, -2.7533e-02, -1.1424e-02,  1.9700e-02,\n",
      "        -3.0426e-03,  1.1234e-02,  3.1455e-03,  8.9024e-04,  1.9487e-02,\n",
      "         3.3778e-02, -3.8216e-02, -3.1973e-02,  3.8842e-02, -3.6172e-02,\n",
      "         2.0288e-03,  2.8444e-02,  1.9681e-02, -3.5453e-03,  3.1852e-03,\n",
      "         1.4935e-02,  2.3969e-02,  2.1789e-02,  3.4326e-02, -3.5032e-02,\n",
      "         4.5000e-03, -3.2066e-02, -3.9639e-02, -3.1132e-02, -3.4109e-02,\n",
      "         1.2766e-02, -3.5620e-02, -1.1669e-02,  2.5384e-03, -2.9883e-02,\n",
      "         1.6426e-02, -1.2381e-02, -2.6539e-03, -2.6286e-02,  3.9408e-02,\n",
      "         4.1561e-02, -1.9339e-02, -2.8244e-02,  2.1043e-02, -3.1252e-02,\n",
      "         6.6334e-03, -2.5583e-02, -2.1440e-02, -7.2778e-03, -2.1829e-02,\n",
      "        -2.1525e-03,  3.8502e-02,  1.5328e-02, -1.9165e-02, -2.3123e-03,\n",
      "         4.2497e-02,  2.3590e-03,  3.0974e-02, -6.1708e-03, -7.3074e-03,\n",
      "         3.7644e-02,  2.7490e-02, -3.8924e-02, -3.7440e-02,  4.4168e-02,\n",
      "         3.1464e-02, -6.9982e-03,  1.6135e-02,  5.7168e-03, -2.0917e-02,\n",
      "         1.8525e-02,  3.0563e-02,  4.2785e-02,  3.5348e-02,  1.9730e-02,\n",
      "        -4.2467e-02,  3.7823e-02,  3.3418e-03,  7.1137e-03, -2.7478e-02,\n",
      "         1.6974e-02,  1.7053e-02,  3.2009e-02, -2.8223e-02, -1.7592e-02,\n",
      "        -2.0927e-02,  1.1907e-02, -2.9137e-02,  1.0654e-03, -1.3000e-02,\n",
      "         2.7631e-02,  1.7736e-02,  3.5504e-02,  7.3069e-03, -1.0474e-02,\n",
      "         1.0544e-02,  2.4495e-02,  2.0258e-02, -5.6692e-03,  1.0601e-03,\n",
      "        -3.7201e-02,  2.5732e-02, -5.1630e-04,  9.3522e-03, -3.4851e-02,\n",
      "        -1.7323e-02, -2.8997e-02, -3.9547e-02, -3.6293e-02, -1.3265e-02,\n",
      "        -1.2675e-02, -2.1615e-02,  1.4997e-02,  3.6799e-02, -2.1437e-02,\n",
      "         2.6789e-02,  2.6972e-02, -1.9737e-03, -1.4478e-02, -5.0306e-03,\n",
      "        -2.9115e-02, -1.7533e-02,  1.3315e-02,  8.5701e-03,  1.1155e-02,\n",
      "         1.8052e-02, -6.7017e-03,  2.7737e-03,  1.2492e-02, -1.4770e-02,\n",
      "        -9.7481e-03, -4.0069e-02,  2.8077e-02,  8.0499e-03,  3.1280e-02,\n",
      "        -1.1633e-02,  1.6202e-02,  2.2335e-02,  1.2777e-02,  4.0319e-02,\n",
      "         1.8177e-02,  2.4750e-02, -2.2074e-02, -3.6800e-02, -8.1532e-03,\n",
      "        -4.0069e-02,  1.2473e-02, -3.1786e-02, -1.7763e-02,  1.0229e-02,\n",
      "        -4.6579e-03, -3.9774e-02,  4.2938e-02,  5.1320e-03, -1.8982e-03,\n",
      "        -7.7134e-03, -1.8520e-02, -4.1662e-02, -1.0299e-02,  3.6774e-02,\n",
      "         9.1219e-03,  3.3715e-02, -3.8402e-02,  3.5109e-02, -1.4965e-02,\n",
      "        -3.5052e-02,  1.0958e-02,  3.9066e-02, -1.6442e-02, -6.0835e-03,\n",
      "         1.1498e-02, -6.7880e-03, -3.9587e-02,  2.0255e-03,  1.0725e-02,\n",
      "        -2.0047e-02, -3.1741e-02, -3.3663e-02, -1.8196e-02,  2.0481e-02,\n",
      "         2.6546e-02, -7.0763e-03,  1.5939e-02,  2.7903e-02,  1.6286e-02,\n",
      "        -4.2395e-02, -3.9589e-02,  2.6600e-02, -1.2837e-02, -2.7907e-02,\n",
      "        -2.2141e-02,  4.0994e-02,  3.6008e-02,  5.7013e-03, -4.1493e-02,\n",
      "        -1.9427e-02, -1.2082e-02, -3.3603e-02,  2.7140e-03,  3.0635e-02,\n",
      "         1.7389e-02, -1.4015e-02,  1.8548e-02, -3.0820e-02, -2.8175e-02,\n",
      "        -1.2806e-03,  3.4184e-02, -4.0249e-02,  1.9629e-02, -1.0646e-02,\n",
      "         6.7550e-03, -1.8758e-02,  4.1422e-02, -1.0605e-02,  1.8594e-02,\n",
      "         6.7012e-03,  2.5013e-02,  2.9761e-02, -4.3397e-02,  1.9559e-02,\n",
      "        -1.8084e-02, -2.3689e-02,  4.8354e-03, -2.1707e-02, -2.0083e-02,\n",
      "        -3.3928e-02,  3.9382e-02,  3.4075e-02, -1.7718e-02, -2.9083e-02,\n",
      "        -2.2975e-02,  2.7669e-02, -2.3874e-02,  8.4132e-03, -8.6920e-04,\n",
      "        -4.3509e-02, -1.7755e-02, -7.7514e-03, -2.2815e-02,  2.9627e-02,\n",
      "        -7.2082e-03,  4.3288e-03,  1.1265e-02, -2.2161e-02,  7.2518e-03,\n",
      "         2.7609e-02, -2.0242e-03, -3.0457e-02,  1.6602e-02, -5.7289e-03,\n",
      "         2.3394e-02, -1.2780e-03, -1.0862e-02,  1.4185e-02,  2.6841e-02,\n",
      "         4.0839e-02, -3.6305e-02,  3.3203e-03,  2.6790e-02, -1.6849e-02,\n",
      "        -7.3975e-03, -1.9108e-02,  4.0616e-02,  3.0909e-02, -4.7275e-03,\n",
      "        -3.0954e-02,  8.0627e-03], requires_grad=True))\n",
      "('encoder.layers.1.attn.k_linear.weight', Parameter containing:\n",
      "tensor([[ 0.0713,  0.0344,  0.0678,  ..., -0.0548,  0.0286,  0.0039],\n",
      "        [ 0.0369,  0.0036,  0.0549,  ...,  0.0690, -0.0741,  0.0712],\n",
      "        [ 0.0559,  0.0133, -0.0330,  ...,  0.0512,  0.0388,  0.0661],\n",
      "        ...,\n",
      "        [ 0.0604, -0.0090, -0.0616,  ...,  0.0471, -0.0138, -0.0648],\n",
      "        [-0.0245, -0.0361, -0.0055,  ...,  0.0054,  0.0585, -0.0243],\n",
      "        [ 0.0447,  0.0068,  0.0077,  ...,  0.0525, -0.0314,  0.0329]],\n",
      "       requires_grad=True))\n",
      "('encoder.layers.1.attn.k_linear.bias', Parameter containing:\n",
      "tensor([ 2.8466e-02, -3.1576e-02, -2.1382e-02,  2.8040e-02, -1.9033e-02,\n",
      "        -2.3035e-02,  1.2237e-02,  4.3663e-02, -3.7604e-02, -4.3929e-02,\n",
      "         1.8038e-02,  4.1589e-02, -2.5162e-02, -3.8133e-02,  3.5641e-02,\n",
      "         4.1162e-03,  2.1725e-02, -2.6714e-02,  4.5525e-03, -1.3898e-02,\n",
      "         2.6634e-03, -2.1919e-02, -2.2364e-02, -3.7802e-02, -2.5131e-02,\n",
      "         1.1673e-02, -4.0482e-02, -4.8865e-03, -1.4762e-02, -8.6488e-03,\n",
      "        -3.3670e-02, -2.9190e-02,  3.3692e-02, -2.9774e-02, -2.1007e-03,\n",
      "         1.0623e-02,  4.2860e-02,  6.0571e-03, -3.8645e-02, -5.9925e-03,\n",
      "         4.2115e-04, -6.2222e-03,  1.0368e-02,  2.7856e-03,  3.3152e-02,\n",
      "        -7.2547e-03, -3.1418e-02, -4.5041e-03, -4.0171e-02, -2.3775e-02,\n",
      "        -1.1850e-05,  2.9324e-02, -9.2894e-03, -1.0094e-02,  8.1878e-03,\n",
      "        -7.3896e-03, -4.1529e-02, -4.0507e-02, -2.5653e-02, -2.4183e-02,\n",
      "        -1.8531e-02,  1.5281e-02, -1.5755e-02,  1.6397e-02, -3.7667e-02,\n",
      "        -1.7488e-02,  2.8644e-02,  5.3139e-03,  8.6694e-03,  3.8921e-02,\n",
      "        -2.6988e-02, -3.7142e-02, -7.0886e-03, -3.9489e-02, -1.6972e-02,\n",
      "         2.6430e-02, -7.1957e-03,  2.5814e-02, -2.5517e-02, -3.1044e-02,\n",
      "         6.6013e-03,  1.3303e-02, -3.3507e-03, -3.1976e-02,  3.7873e-02,\n",
      "        -4.3684e-02,  2.2735e-02, -4.3717e-02, -1.6612e-02,  1.4361e-02,\n",
      "         2.8796e-02,  1.5781e-02, -4.2693e-02, -2.6894e-02,  3.1873e-02,\n",
      "         2.0140e-02, -2.0802e-03, -2.5637e-03,  5.9422e-03,  3.4819e-02,\n",
      "         1.3108e-02,  3.3180e-03, -4.5628e-03,  3.6137e-02, -2.8060e-02,\n",
      "        -1.9545e-02,  7.0133e-03,  2.2358e-03,  2.4410e-02, -3.3672e-03,\n",
      "        -3.0463e-02,  1.8442e-02, -2.1668e-02, -1.0417e-02, -4.0111e-02,\n",
      "         1.3184e-03, -1.0552e-02,  3.1422e-02, -1.9723e-02, -1.7379e-02,\n",
      "        -1.4189e-02,  1.4435e-02, -3.9165e-02,  4.0357e-03,  2.1617e-02,\n",
      "         1.8650e-02,  1.2756e-02, -1.4909e-02, -1.9367e-02,  4.1250e-03,\n",
      "         7.2022e-03,  7.0841e-03, -9.6479e-03,  1.7322e-02,  1.9049e-03,\n",
      "         3.3038e-02, -3.6010e-02, -4.1597e-02,  4.2095e-02,  7.0053e-03,\n",
      "        -1.2476e-02,  1.3386e-02, -4.2721e-02,  1.7849e-02, -3.3959e-02,\n",
      "         2.0250e-02, -8.0928e-03,  7.9811e-03, -3.4161e-02, -2.8154e-02,\n",
      "        -2.4738e-02, -1.3604e-02, -3.0891e-02, -5.1802e-03,  1.6409e-02,\n",
      "         6.8439e-03, -2.7238e-02, -8.7744e-03, -1.4245e-02,  1.6753e-02,\n",
      "         1.5804e-02,  1.5015e-02,  3.1510e-02,  2.1790e-02, -1.7269e-02,\n",
      "         2.3606e-02, -1.2520e-02, -2.4243e-02,  3.9948e-02,  2.4889e-02,\n",
      "        -4.0360e-02,  1.7399e-03,  3.1816e-02,  4.1331e-02, -6.7625e-04,\n",
      "         3.6791e-02,  1.7415e-02, -5.1280e-03,  3.2169e-02, -1.9530e-02,\n",
      "         4.0514e-04,  3.7053e-02,  1.1483e-02,  3.8117e-03,  2.7934e-02,\n",
      "        -3.1959e-02, -4.3440e-02, -6.0972e-03, -2.3632e-02, -2.7036e-02,\n",
      "         3.6770e-02,  1.6995e-03, -2.0404e-02, -1.4274e-02,  4.1004e-02,\n",
      "        -1.4619e-02, -2.6548e-02, -3.4694e-02,  3.2412e-03, -3.8872e-02,\n",
      "        -2.3288e-03, -3.1133e-02, -4.9501e-03, -1.7422e-02, -6.3620e-03,\n",
      "         3.0451e-02, -2.2695e-02, -3.7003e-02, -1.9297e-03,  3.6891e-02,\n",
      "        -1.9107e-02,  9.7054e-04,  1.4439e-02,  3.8221e-02, -7.5843e-03,\n",
      "         2.0589e-02, -4.3379e-02, -3.7846e-02, -2.8062e-02, -2.3367e-02,\n",
      "         1.5211e-03, -2.6829e-02, -4.3265e-02,  1.7478e-02,  3.3655e-02,\n",
      "         2.7938e-02,  2.5354e-03,  2.8159e-02,  8.5770e-03, -3.7975e-02,\n",
      "         1.3817e-02,  4.3845e-02, -1.5784e-02,  1.7846e-02, -2.4755e-02,\n",
      "         1.3470e-02, -4.8110e-03,  6.0575e-03, -5.3527e-03,  8.3360e-03,\n",
      "        -1.5052e-02,  3.4404e-02,  1.3029e-02,  1.6181e-02,  2.9638e-02,\n",
      "         1.9958e-02,  2.3601e-02,  4.2157e-02,  2.9096e-02, -4.7397e-03,\n",
      "         1.3966e-02, -3.6908e-02,  2.1043e-02,  1.5177e-02, -9.8751e-03,\n",
      "         1.0005e-03, -1.3313e-02,  1.6878e-02, -4.0443e-02, -3.5311e-03,\n",
      "         2.8758e-02,  1.0232e-02, -7.5291e-03,  2.8563e-03, -4.7029e-03,\n",
      "         9.5207e-03, -1.8018e-02, -9.7055e-03, -4.1400e-02,  2.8311e-02,\n",
      "        -5.5068e-03,  2.9616e-02, -1.1018e-02,  2.1123e-02, -1.5897e-02,\n",
      "         2.2421e-02,  3.5948e-02,  2.9202e-02,  4.4113e-02,  1.5457e-02,\n",
      "        -3.0534e-02,  2.2483e-02, -1.5201e-02, -3.1705e-02,  2.4663e-02,\n",
      "         2.6117e-02, -3.1813e-02, -3.1776e-02,  2.8752e-02,  2.6732e-02,\n",
      "         1.7115e-03,  2.9238e-02, -2.2809e-02, -2.5481e-02,  2.1772e-03,\n",
      "        -1.7083e-02,  3.6425e-02, -2.3514e-02,  3.7942e-02,  2.9621e-02,\n",
      "        -1.3041e-02, -2.7971e-02,  3.1175e-02, -3.7329e-02, -3.2022e-02,\n",
      "         3.1949e-02, -1.8306e-02, -2.6754e-02, -1.0185e-02, -3.5084e-02,\n",
      "        -5.3606e-03,  2.1412e-02, -2.4348e-02,  3.2649e-02,  4.0224e-02,\n",
      "        -7.2036e-04, -3.7017e-02, -3.0335e-02, -4.2164e-02,  2.6888e-02,\n",
      "        -6.2750e-03, -2.1122e-02, -3.9755e-02,  3.4689e-02, -9.1327e-03,\n",
      "         4.2169e-02,  1.4085e-02, -5.6629e-03,  3.7312e-02, -2.8975e-02,\n",
      "        -2.2938e-02,  9.3918e-04,  2.8349e-02, -3.9844e-02,  2.0003e-02,\n",
      "        -1.0823e-02, -3.9257e-02,  2.4904e-02,  3.9756e-03,  3.5340e-02,\n",
      "        -3.7791e-02,  4.5258e-03, -1.6417e-02, -2.4745e-02,  2.7035e-02,\n",
      "         3.3336e-02, -1.4166e-02,  7.2056e-03, -2.6066e-02,  5.4514e-03,\n",
      "         2.2904e-02, -1.3844e-02, -4.2029e-03,  1.2859e-03, -1.9006e-02,\n",
      "        -1.9352e-02, -2.8439e-02,  4.3345e-02,  3.6851e-02,  2.2704e-02,\n",
      "         9.7455e-03, -3.7895e-02, -3.7768e-02, -1.8372e-03,  2.9151e-02,\n",
      "         4.1177e-02,  3.5148e-03, -9.4573e-03,  3.5877e-03, -2.0087e-02,\n",
      "         2.0176e-04, -3.7562e-02, -3.1867e-02, -2.1994e-02,  3.6004e-02,\n",
      "        -2.3605e-02,  1.2749e-02,  3.0475e-03, -3.3361e-02, -1.7551e-02,\n",
      "         1.7869e-02, -2.3366e-02, -9.8540e-03, -1.6827e-02, -1.3040e-02,\n",
      "        -3.8654e-02, -3.9348e-02,  2.3817e-02,  4.0248e-02, -3.7212e-02,\n",
      "         9.2285e-03,  1.8703e-02, -3.5633e-02,  3.6174e-02,  1.4531e-02,\n",
      "        -1.8495e-02,  2.7171e-02,  3.6090e-03, -8.9996e-03, -2.4432e-02,\n",
      "         1.1079e-02, -3.1874e-02, -3.4302e-02,  3.5858e-02,  2.0719e-02,\n",
      "        -3.1681e-03,  2.1033e-02, -6.0762e-03, -2.0588e-02,  2.8543e-03,\n",
      "         1.1513e-02, -7.6844e-03, -1.2826e-02,  3.3306e-02, -3.6754e-02,\n",
      "         3.6549e-02, -8.0794e-03, -3.3808e-02, -1.6690e-02, -2.1592e-02,\n",
      "         7.8664e-04,  2.6582e-02, -1.4578e-02, -3.1909e-02, -2.6587e-02,\n",
      "         2.3355e-02,  2.7460e-03,  1.4893e-02, -2.9185e-02,  6.4524e-03,\n",
      "        -3.3763e-02,  3.3056e-02,  3.8978e-02,  1.5069e-02,  2.0968e-02,\n",
      "         3.7773e-02,  2.1346e-02,  1.5759e-02,  1.4843e-02, -9.2096e-03,\n",
      "        -3.3041e-02, -1.9303e-02, -1.1495e-02,  1.9062e-02,  3.1878e-02,\n",
      "         9.8645e-03, -3.8840e-02, -3.0855e-02, -1.7385e-02,  7.1576e-03,\n",
      "         1.2452e-02, -4.2648e-02,  3.4358e-02,  4.3458e-03,  3.6405e-02,\n",
      "        -2.9515e-02,  3.5475e-02, -8.0282e-03, -3.2711e-02, -4.5234e-03,\n",
      "        -2.3730e-02, -9.8844e-03, -3.0309e-02, -3.5248e-02,  8.3474e-03,\n",
      "         4.1270e-02,  3.7590e-02,  2.6379e-02, -4.3542e-02, -3.3868e-02,\n",
      "        -1.9520e-02, -2.3184e-02,  3.1624e-02, -1.1634e-02, -2.9302e-02,\n",
      "        -3.6573e-02, -3.6630e-02, -2.9054e-02,  1.1941e-02, -2.6494e-02,\n",
      "        -4.3305e-02, -1.2661e-02, -3.5914e-02, -3.2297e-02, -2.2791e-02,\n",
      "         2.5692e-02,  4.0889e-02,  3.6008e-02, -2.5321e-02, -7.8863e-03,\n",
      "         1.2929e-02, -1.8719e-02,  4.0269e-02, -2.1128e-02,  3.4347e-02,\n",
      "        -4.2727e-02, -1.9288e-02,  6.7732e-03, -4.2316e-03,  7.3233e-03,\n",
      "        -1.4291e-02, -3.8192e-02,  2.7631e-02,  2.8169e-02,  1.8133e-02,\n",
      "         2.2899e-02, -4.5350e-04, -2.1982e-02, -1.6685e-02, -1.8313e-02,\n",
      "         3.6036e-02, -3.5458e-02], requires_grad=True))\n",
      "('encoder.layers.1.attn.out.weight', Parameter containing:\n",
      "tensor([[-0.0657, -0.0110,  0.0158,  ..., -0.0758,  0.0710,  0.0404],\n",
      "        [ 0.0746, -0.0212,  0.0437,  ..., -0.0322, -0.0717,  0.0542],\n",
      "        [ 0.0410, -0.0301,  0.0509,  ..., -0.0257,  0.0146,  0.0122],\n",
      "        ...,\n",
      "        [-0.0040,  0.0679, -0.0725,  ...,  0.0594,  0.0745, -0.0175],\n",
      "        [ 0.0074,  0.0164, -0.0087,  ..., -0.0071,  0.0293,  0.0530],\n",
      "        [-0.0592, -0.0589, -0.0620,  ...,  0.0569,  0.0424, -0.0217]],\n",
      "       requires_grad=True))\n",
      "('encoder.layers.1.attn.out.bias', Parameter containing:\n",
      "tensor([ 3.6173e-02,  1.6211e-03,  2.4914e-02,  3.2639e-02,  2.9335e-02,\n",
      "         1.1391e-02, -1.1946e-02,  1.0488e-02, -3.1338e-02,  2.9513e-02,\n",
      "         5.4303e-03, -4.1162e-02, -3.9499e-02,  1.8841e-02, -2.7576e-02,\n",
      "         7.8569e-03, -3.4326e-02,  2.3920e-03,  1.9933e-02, -5.1718e-03,\n",
      "         2.4679e-02,  3.1648e-02,  3.3778e-02,  2.8280e-02, -3.0736e-02,\n",
      "         1.4679e-02, -2.1490e-02, -1.5910e-02, -3.4204e-02, -3.6788e-02,\n",
      "         3.3354e-02, -2.9069e-03,  2.8820e-02,  3.8679e-02, -2.8400e-02,\n",
      "         1.1979e-02,  2.6168e-02,  3.9946e-02,  4.0310e-02,  1.8609e-02,\n",
      "        -4.1103e-02,  2.7008e-02,  4.2743e-02, -3.1683e-03, -3.0279e-02,\n",
      "         2.5710e-02,  3.7383e-02, -2.1197e-02,  2.2708e-02, -4.2422e-02,\n",
      "         3.7280e-02, -2.0359e-02, -3.0157e-03,  3.4316e-02,  6.7439e-03,\n",
      "         2.4612e-04, -3.5880e-02, -4.1755e-02, -3.8700e-02, -4.1624e-02,\n",
      "         1.8720e-02,  4.2409e-02,  2.5815e-02,  1.1577e-02, -4.1043e-02,\n",
      "        -2.3002e-03, -3.4853e-02,  3.8617e-02, -2.4050e-02, -2.0056e-02,\n",
      "         2.8247e-02,  3.3182e-02,  2.1098e-02,  2.9222e-02, -1.8749e-02,\n",
      "        -2.4707e-02,  5.5640e-03,  3.5089e-02, -1.1001e-02, -3.8201e-02,\n",
      "         3.1635e-02,  2.3236e-02,  2.4273e-02, -3.1115e-03, -1.5420e-02,\n",
      "         4.3964e-02, -1.4401e-02, -6.0152e-03,  3.2525e-02, -2.7180e-02,\n",
      "        -2.2690e-02,  4.3622e-02,  2.5715e-02,  3.6379e-02, -4.1415e-02,\n",
      "        -3.8484e-02, -4.3007e-02, -3.9885e-02, -1.2271e-02, -3.7029e-02,\n",
      "         2.4156e-02,  1.5509e-02,  7.1704e-03, -4.0322e-02,  3.4739e-02,\n",
      "        -3.2848e-02,  1.3377e-02, -3.5581e-03,  1.7323e-02,  1.5545e-03,\n",
      "         4.1564e-02,  3.3363e-02,  4.0752e-02, -1.7113e-02,  1.2511e-02,\n",
      "         1.6957e-02, -5.8650e-03,  2.2142e-02,  3.5512e-02, -4.3793e-02,\n",
      "         9.2476e-03, -4.3238e-02,  4.2163e-02,  4.2876e-02, -1.6486e-02,\n",
      "         5.7445e-03, -1.6908e-02,  1.7725e-02, -2.1374e-02,  3.3253e-02,\n",
      "        -4.2019e-02,  4.2907e-02, -2.9750e-02, -1.1331e-02, -4.3203e-02,\n",
      "         2.7376e-02, -5.8672e-03, -2.2327e-02, -2.1135e-03, -7.7616e-04,\n",
      "        -3.3093e-02, -1.2653e-03, -2.1780e-02,  2.4166e-02,  3.9501e-02,\n",
      "         2.5473e-02,  2.6114e-02, -7.1739e-03,  3.4049e-02, -2.8074e-02,\n",
      "         3.0200e-02, -4.3572e-02, -3.6449e-02, -4.1135e-02, -4.0824e-02,\n",
      "         9.1651e-03, -1.6268e-03,  3.2338e-02,  4.0208e-02,  9.5292e-03,\n",
      "        -3.3993e-02, -1.3784e-02,  4.1862e-02, -1.4244e-03, -3.2442e-02,\n",
      "        -3.7099e-03,  2.8875e-03,  2.6717e-02, -4.4080e-02,  3.4985e-02,\n",
      "         4.0353e-02,  2.5509e-03, -4.3605e-02,  2.5381e-02, -2.3312e-02,\n",
      "         3.1031e-02, -1.1979e-02,  1.8429e-02, -3.8399e-02,  1.1549e-02,\n",
      "         1.7698e-02, -3.3797e-02,  1.3796e-02, -3.6931e-02,  1.2257e-02,\n",
      "        -3.4840e-02, -1.3801e-02,  2.5995e-02,  1.6186e-02, -1.4539e-02,\n",
      "        -4.3308e-02, -1.6541e-02,  4.1238e-02, -8.9677e-03,  3.7361e-02,\n",
      "        -1.9966e-02, -7.7926e-03, -2.3459e-02, -2.1014e-02, -4.5359e-03,\n",
      "        -3.4481e-02,  1.7836e-02, -8.3524e-03,  4.3198e-02, -3.8543e-02,\n",
      "         2.9865e-02, -2.8399e-02, -2.7506e-02,  2.1277e-02, -2.3280e-03,\n",
      "         1.3669e-02, -2.5450e-02,  3.2072e-02, -1.0507e-02,  3.4263e-02,\n",
      "        -1.6659e-03,  7.1772e-03, -1.3909e-02, -1.1141e-02,  1.1945e-02,\n",
      "         1.8477e-02,  2.8687e-02,  1.7458e-02, -1.2866e-02,  3.3332e-02,\n",
      "        -6.5532e-03,  3.3106e-02,  2.7157e-02, -2.1113e-02, -3.0931e-02,\n",
      "        -3.1794e-02, -3.3882e-02,  3.5639e-02, -2.1552e-02, -4.2671e-02,\n",
      "         2.9036e-02,  4.1080e-02,  2.9550e-02, -3.6451e-02,  2.3034e-02,\n",
      "        -3.1654e-02, -1.1901e-02,  2.5274e-02, -9.1752e-03, -3.2908e-02,\n",
      "        -8.6679e-03,  3.1725e-02, -3.3964e-02, -5.8827e-03, -2.7774e-02,\n",
      "        -3.4311e-02, -2.6318e-02,  4.3508e-02,  2.8496e-02,  5.7827e-03,\n",
      "         2.8820e-02,  1.9026e-02, -3.2005e-02, -9.2645e-03,  1.1402e-02,\n",
      "         3.2795e-02,  3.8477e-04,  3.7628e-02,  3.5346e-02, -1.5637e-03,\n",
      "        -1.0768e-02,  4.1522e-02,  6.6700e-03, -3.9449e-02, -4.0165e-02,\n",
      "         3.7455e-02, -3.3460e-02,  2.0592e-02,  1.3683e-02,  1.1266e-03,\n",
      "         2.7405e-03, -4.2846e-02,  2.1298e-02, -3.1262e-03, -2.7306e-02,\n",
      "         2.8795e-02,  1.1685e-02,  2.1963e-02,  6.1692e-04, -3.4703e-02,\n",
      "         9.8610e-03,  4.4035e-02,  2.9943e-02, -4.0085e-02,  1.5578e-02,\n",
      "        -3.1401e-02,  3.7632e-02, -1.6857e-02, -1.9540e-02,  1.4586e-02,\n",
      "        -3.7210e-02, -1.9152e-02, -4.0817e-02, -4.0487e-02, -1.1998e-02,\n",
      "        -2.5090e-02, -1.3640e-02,  3.1944e-03,  4.1146e-02, -3.3651e-02,\n",
      "        -2.0305e-02, -4.0429e-03,  3.8601e-02,  3.7540e-03,  1.2358e-02,\n",
      "         8.0923e-03, -3.1890e-02,  9.0405e-03, -3.2865e-02,  2.3512e-02,\n",
      "        -1.0646e-02, -6.4079e-05,  3.0953e-02, -4.3657e-02,  1.2613e-02,\n",
      "        -3.4982e-02,  2.3734e-02, -2.5533e-02,  2.0114e-02,  1.3495e-02,\n",
      "        -4.0408e-02,  4.3891e-02,  3.2588e-02, -1.2744e-02, -6.1904e-03,\n",
      "        -4.0896e-02,  1.7735e-02, -1.8354e-02,  1.0876e-02, -1.9555e-02,\n",
      "         2.6990e-02, -3.7930e-02, -1.7447e-02, -3.7880e-02, -3.3116e-02,\n",
      "         3.9245e-02,  3.9222e-02, -4.0082e-02, -1.5525e-02,  8.3313e-03,\n",
      "        -1.0682e-02,  1.0559e-02, -4.1279e-03,  2.0919e-02,  3.9888e-02,\n",
      "        -3.7961e-02, -4.1791e-03, -3.1308e-02,  2.9455e-02, -2.3184e-03,\n",
      "         2.9002e-02,  3.1627e-02, -1.4581e-03,  7.1750e-04, -4.9936e-03,\n",
      "         3.7277e-02, -3.9093e-02,  2.8273e-02, -7.2325e-03,  2.6456e-02,\n",
      "         1.3428e-02, -1.1590e-02, -9.0277e-03,  2.5187e-02,  3.5403e-02,\n",
      "        -2.3445e-02,  1.2677e-02,  2.6261e-02,  2.8086e-02,  9.2215e-03,\n",
      "        -3.9261e-03, -1.6630e-02,  1.8094e-02,  4.2168e-02,  3.5390e-02,\n",
      "         1.0590e-02,  2.9856e-02,  1.8428e-02,  2.8050e-02, -3.0886e-02,\n",
      "         2.4316e-03,  1.0007e-02, -1.8302e-02, -1.3998e-02,  9.9709e-03,\n",
      "         5.4165e-04,  2.8919e-02,  4.0630e-02, -2.6261e-02, -1.0705e-02,\n",
      "        -3.1927e-04, -3.1478e-02,  1.5693e-02, -1.9394e-02,  4.0839e-02,\n",
      "        -3.4287e-02,  2.1058e-02, -4.0400e-02,  3.9859e-02, -1.4393e-02,\n",
      "         1.1794e-02, -3.7647e-02,  3.3082e-02, -3.1598e-02, -4.0801e-02,\n",
      "         2.9193e-02,  3.0060e-03,  3.5513e-02,  2.0897e-02, -1.0233e-02,\n",
      "        -2.2266e-02, -2.3217e-02, -8.3686e-03, -7.7014e-03, -1.2246e-03,\n",
      "         4.2030e-02, -2.5526e-02, -3.4496e-02,  2.8890e-02, -2.9117e-02,\n",
      "        -3.6492e-02,  2.0533e-02,  2.5642e-02, -1.6092e-02,  3.6677e-02,\n",
      "         1.7104e-02, -2.9482e-02,  9.8697e-04,  9.7174e-03, -3.3913e-02,\n",
      "         1.5809e-02,  2.9647e-03,  2.8790e-02,  3.9334e-02,  2.1073e-02,\n",
      "         2.2489e-03, -1.2810e-02,  3.8203e-02,  1.0557e-03,  1.9733e-02,\n",
      "         1.4967e-02,  1.7334e-02,  1.3486e-02, -4.9774e-03, -3.3356e-03,\n",
      "        -1.6483e-02,  3.4807e-02,  2.9799e-02, -2.2900e-02,  2.1355e-02,\n",
      "         1.4287e-02, -1.2257e-02,  1.9343e-02, -2.4151e-04,  4.8523e-03,\n",
      "        -2.8394e-02,  1.4600e-02,  2.1612e-02, -3.9485e-02, -2.1343e-02,\n",
      "         2.2399e-02,  1.5797e-02, -4.3395e-02, -1.9832e-02,  2.9439e-02,\n",
      "        -8.2538e-03,  4.3666e-02, -1.4292e-02,  1.0440e-03,  2.7911e-02,\n",
      "        -2.6805e-02,  3.8013e-03, -5.0017e-03,  4.0159e-02,  2.3090e-02,\n",
      "         3.3218e-02,  3.5906e-02, -5.7731e-03, -1.9321e-03,  4.0921e-02,\n",
      "         3.7954e-03, -2.7409e-03,  2.1960e-02,  3.3413e-02, -3.7034e-02,\n",
      "         4.3794e-02, -3.9035e-03, -2.3826e-03,  3.4734e-02, -1.6741e-03,\n",
      "        -6.3292e-03,  3.9883e-02,  3.0560e-02, -2.4936e-02, -2.7012e-02,\n",
      "        -4.3225e-02, -2.5587e-02,  3.5557e-02, -4.1017e-02, -1.0345e-02,\n",
      "        -2.6719e-02, -1.5445e-02, -4.3772e-02, -3.0332e-02,  1.4563e-02,\n",
      "         3.6111e-02,  1.1043e-02], requires_grad=True))\n",
      "('encoder.layers.1.ff.linear_1.weight', Parameter containing:\n",
      "tensor([[-0.0161, -0.0393,  0.0311,  ..., -0.0256, -0.0004, -0.0101],\n",
      "        [-0.0395,  0.0356, -0.0087,  ..., -0.0447, -0.0405,  0.0095],\n",
      "        [ 0.0095,  0.0297,  0.0087,  ...,  0.0238,  0.0186, -0.0153],\n",
      "        ...,\n",
      "        [-0.0054, -0.0183,  0.0396,  ..., -0.0039, -0.0444, -0.0111],\n",
      "        [ 0.0212, -0.0470, -0.0299,  ...,  0.0338, -0.0030,  0.0021],\n",
      "        [ 0.0344, -0.0015,  0.0014,  ..., -0.0281,  0.0262,  0.0400]],\n",
      "       requires_grad=True))\n",
      "('encoder.layers.1.ff.linear_1.bias', Parameter containing:\n",
      "tensor([ 0.0300,  0.0145,  0.0123,  ..., -0.0311, -0.0128, -0.0362],\n",
      "       requires_grad=True))\n",
      "('encoder.layers.1.ff.linear_2.weight', Parameter containing:\n",
      "tensor([[ 0.0014, -0.0260, -0.0349,  ...,  0.0475,  0.0262, -0.0306],\n",
      "        [ 0.0401, -0.0329,  0.0042,  ..., -0.0366,  0.0332, -0.0223],\n",
      "        [ 0.0198,  0.0474, -0.0169,  ..., -0.0106,  0.0121, -0.0456],\n",
      "        ...,\n",
      "        [-0.0044, -0.0160,  0.0111,  ..., -0.0083,  0.0203,  0.0071],\n",
      "        [-0.0196,  0.0308, -0.0099,  ..., -0.0254, -0.0248,  0.0305],\n",
      "        [ 0.0167, -0.0390,  0.0164,  ...,  0.0427, -0.0102, -0.0296]],\n",
      "       requires_grad=True))\n",
      "('encoder.layers.1.ff.linear_2.bias', Parameter containing:\n",
      "tensor([-0.0191, -0.0202,  0.0098, -0.0181, -0.0091,  0.0018,  0.0116,  0.0048,\n",
      "         0.0055,  0.0112, -0.0203, -0.0175,  0.0147, -0.0040, -0.0033,  0.0004,\n",
      "        -0.0056,  0.0156, -0.0197,  0.0066, -0.0140,  0.0011,  0.0142, -0.0146,\n",
      "         0.0188, -0.0213,  0.0108,  0.0170, -0.0097, -0.0134, -0.0072,  0.0113,\n",
      "         0.0011,  0.0132, -0.0068,  0.0096, -0.0127, -0.0034, -0.0195, -0.0022,\n",
      "         0.0043,  0.0205,  0.0031, -0.0129, -0.0078,  0.0134,  0.0099, -0.0067,\n",
      "         0.0219,  0.0180,  0.0102,  0.0100, -0.0127,  0.0028,  0.0151,  0.0198,\n",
      "        -0.0087, -0.0192, -0.0034,  0.0089, -0.0078, -0.0044, -0.0132,  0.0065,\n",
      "         0.0005,  0.0014, -0.0204, -0.0209, -0.0114, -0.0159, -0.0128, -0.0135,\n",
      "         0.0041, -0.0166,  0.0195,  0.0126, -0.0057, -0.0077, -0.0046,  0.0139,\n",
      "        -0.0019, -0.0089, -0.0207, -0.0004, -0.0063, -0.0045,  0.0008, -0.0149,\n",
      "        -0.0003, -0.0022,  0.0057, -0.0205, -0.0174,  0.0157, -0.0022,  0.0093,\n",
      "         0.0030, -0.0207, -0.0084,  0.0014,  0.0164, -0.0020,  0.0060, -0.0011,\n",
      "        -0.0144,  0.0045,  0.0046, -0.0118, -0.0011,  0.0061,  0.0099, -0.0023,\n",
      "        -0.0121, -0.0030, -0.0007, -0.0188,  0.0141,  0.0096, -0.0086, -0.0206,\n",
      "        -0.0056, -0.0099,  0.0128,  0.0138, -0.0069,  0.0210, -0.0083,  0.0182,\n",
      "        -0.0162, -0.0022,  0.0093,  0.0220, -0.0170, -0.0040, -0.0093,  0.0029,\n",
      "         0.0195,  0.0208, -0.0105,  0.0044, -0.0087,  0.0027,  0.0052,  0.0203,\n",
      "         0.0220,  0.0176,  0.0129, -0.0044,  0.0050,  0.0179, -0.0048, -0.0024,\n",
      "        -0.0194,  0.0129,  0.0150, -0.0208, -0.0137, -0.0037, -0.0081, -0.0053,\n",
      "        -0.0138,  0.0187,  0.0139,  0.0076, -0.0073,  0.0198, -0.0184,  0.0200,\n",
      "        -0.0131, -0.0174,  0.0200, -0.0097,  0.0193,  0.0092,  0.0124,  0.0074,\n",
      "         0.0158, -0.0151,  0.0213, -0.0167, -0.0011, -0.0051,  0.0153, -0.0134,\n",
      "        -0.0015,  0.0192,  0.0009, -0.0097,  0.0185, -0.0136,  0.0024, -0.0179,\n",
      "         0.0126, -0.0019,  0.0053, -0.0085, -0.0048, -0.0090, -0.0194,  0.0047,\n",
      "        -0.0091, -0.0087,  0.0086,  0.0149, -0.0129,  0.0041, -0.0071, -0.0159,\n",
      "         0.0115,  0.0055,  0.0101, -0.0104, -0.0177, -0.0124, -0.0188,  0.0035,\n",
      "         0.0014,  0.0219,  0.0019,  0.0161, -0.0129,  0.0038,  0.0049,  0.0046,\n",
      "         0.0043,  0.0146,  0.0037,  0.0208,  0.0213, -0.0132,  0.0133, -0.0108,\n",
      "        -0.0060,  0.0205,  0.0191, -0.0012,  0.0152,  0.0114, -0.0178, -0.0196,\n",
      "        -0.0138, -0.0068,  0.0171,  0.0218, -0.0133,  0.0169, -0.0196,  0.0181,\n",
      "        -0.0157, -0.0050, -0.0171, -0.0191, -0.0094, -0.0195, -0.0025,  0.0028,\n",
      "         0.0180, -0.0162,  0.0152,  0.0007,  0.0033,  0.0209,  0.0009, -0.0120,\n",
      "         0.0169, -0.0162, -0.0017,  0.0145, -0.0178,  0.0081,  0.0209,  0.0102,\n",
      "         0.0115,  0.0089, -0.0041,  0.0043,  0.0091, -0.0096,  0.0171,  0.0027,\n",
      "         0.0107,  0.0002, -0.0160, -0.0100,  0.0120,  0.0103, -0.0083, -0.0160,\n",
      "        -0.0116, -0.0115, -0.0131, -0.0166,  0.0218,  0.0123, -0.0006, -0.0035,\n",
      "         0.0202,  0.0182, -0.0111, -0.0162, -0.0192,  0.0013, -0.0200, -0.0170,\n",
      "         0.0203, -0.0100, -0.0040, -0.0211, -0.0096, -0.0215, -0.0155, -0.0112,\n",
      "        -0.0152, -0.0086,  0.0041,  0.0139, -0.0061,  0.0046,  0.0024, -0.0032,\n",
      "        -0.0169, -0.0019, -0.0150,  0.0007,  0.0168, -0.0035, -0.0058,  0.0081,\n",
      "        -0.0153, -0.0188,  0.0102, -0.0053,  0.0198, -0.0168,  0.0158,  0.0166,\n",
      "         0.0040, -0.0122,  0.0090, -0.0177, -0.0089,  0.0177, -0.0180, -0.0199,\n",
      "        -0.0059,  0.0070,  0.0153, -0.0118, -0.0067,  0.0137, -0.0163,  0.0013,\n",
      "        -0.0044,  0.0046,  0.0204, -0.0152, -0.0048, -0.0036,  0.0118, -0.0187,\n",
      "         0.0004,  0.0221,  0.0168, -0.0138, -0.0195,  0.0161,  0.0168, -0.0049,\n",
      "         0.0025, -0.0087, -0.0203,  0.0124,  0.0189, -0.0060,  0.0031,  0.0017,\n",
      "        -0.0128,  0.0040, -0.0046, -0.0025,  0.0138, -0.0019,  0.0130,  0.0175,\n",
      "        -0.0136, -0.0206,  0.0006, -0.0077,  0.0157, -0.0125,  0.0164,  0.0122,\n",
      "        -0.0060,  0.0066, -0.0008,  0.0022,  0.0115,  0.0073, -0.0084, -0.0060,\n",
      "        -0.0036,  0.0213, -0.0075, -0.0152, -0.0171, -0.0113, -0.0217,  0.0021,\n",
      "         0.0163,  0.0121, -0.0150,  0.0172, -0.0100,  0.0080, -0.0143,  0.0135,\n",
      "         0.0172,  0.0145, -0.0161,  0.0059,  0.0160,  0.0036, -0.0187, -0.0111,\n",
      "         0.0004,  0.0032,  0.0171,  0.0176,  0.0091, -0.0121,  0.0206, -0.0178,\n",
      "         0.0153, -0.0013,  0.0015,  0.0048, -0.0149, -0.0143,  0.0060,  0.0115,\n",
      "         0.0135, -0.0206, -0.0074,  0.0212, -0.0136,  0.0163,  0.0065, -0.0152,\n",
      "         0.0118,  0.0061, -0.0018,  0.0114, -0.0069,  0.0026,  0.0058,  0.0075,\n",
      "        -0.0064,  0.0074, -0.0082,  0.0187,  0.0077,  0.0038, -0.0180,  0.0127,\n",
      "        -0.0017,  0.0134,  0.0093,  0.0215,  0.0125,  0.0032, -0.0167,  0.0013,\n",
      "        -0.0049, -0.0026,  0.0149, -0.0078,  0.0107,  0.0127, -0.0066,  0.0028,\n",
      "         0.0050,  0.0092, -0.0035,  0.0107,  0.0184,  0.0192, -0.0077, -0.0024,\n",
      "         0.0208, -0.0113,  0.0220,  0.0099,  0.0011,  0.0076,  0.0211,  0.0100,\n",
      "         0.0149,  0.0186, -0.0046,  0.0104, -0.0004,  0.0069,  0.0072, -0.0065,\n",
      "        -0.0013, -0.0110, -0.0083,  0.0036,  0.0124, -0.0080, -0.0174,  0.0113],\n",
      "       requires_grad=True))\n",
      "('encoder.norm.alpha', Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True))\n",
      "('encoder.norm.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))\n",
      "('decoder.embed.embed.weight', Parameter containing:\n",
      "tensor([[-0.0142, -0.0275, -0.0165,  ..., -0.0219, -0.0218, -0.0187],\n",
      "        [-0.0336,  0.0290, -0.0229,  ...,  0.0408,  0.0162,  0.0391],\n",
      "        [ 0.0101,  0.0349, -0.0161,  ..., -0.0405,  0.0315, -0.0336],\n",
      "        ...,\n",
      "        [-0.0235,  0.0401, -0.0208,  ...,  0.0261,  0.0117, -0.0254],\n",
      "        [-0.0174, -0.0295,  0.0395,  ..., -0.0053,  0.0179,  0.0386],\n",
      "        [-0.0360,  0.0310, -0.0253,  ...,  0.0206,  0.0359, -0.0307]],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.0.norm_1.alpha', Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True))\n",
      "('decoder.layers.0.norm_1.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))\n",
      "('decoder.layers.0.norm_2.alpha', Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True))\n",
      "('decoder.layers.0.norm_2.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))\n",
      "('decoder.layers.0.norm_3.alpha', Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True))\n",
      "('decoder.layers.0.norm_3.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))\n",
      "('decoder.layers.0.attn_1.q_linear.weight', Parameter containing:\n",
      "tensor([[ 0.0354, -0.0236, -0.0654,  ...,  0.0571, -0.0574,  0.0434],\n",
      "        [-0.0294, -0.0198,  0.0157,  ..., -0.0025, -0.0223, -0.0704],\n",
      "        [ 0.0416,  0.0258, -0.0713,  ..., -0.0593,  0.0570,  0.0269],\n",
      "        ...,\n",
      "        [ 0.0270, -0.0649,  0.0642,  ..., -0.0373, -0.0190,  0.0407],\n",
      "        [-0.0518,  0.0150, -0.0644,  ..., -0.0449, -0.0109, -0.0409],\n",
      "        [-0.0473, -0.0533,  0.0208,  ...,  0.0462,  0.0259,  0.0631]],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.0.attn_1.q_linear.bias', Parameter containing:\n",
      "tensor([-0.0276,  0.0436,  0.0295,  0.0235, -0.0370,  0.0072,  0.0084, -0.0324,\n",
      "        -0.0252,  0.0325,  0.0047,  0.0264, -0.0405,  0.0010,  0.0099, -0.0214,\n",
      "        -0.0054, -0.0366,  0.0221, -0.0203,  0.0141,  0.0044,  0.0327,  0.0372,\n",
      "         0.0085,  0.0027,  0.0303,  0.0350,  0.0332,  0.0142, -0.0140, -0.0074,\n",
      "         0.0272, -0.0271, -0.0069, -0.0129, -0.0182,  0.0097,  0.0407, -0.0198,\n",
      "        -0.0211, -0.0036, -0.0144,  0.0120, -0.0138,  0.0012, -0.0058,  0.0219,\n",
      "         0.0019, -0.0426,  0.0185, -0.0203, -0.0411, -0.0155, -0.0117, -0.0226,\n",
      "         0.0418, -0.0416, -0.0154,  0.0296, -0.0137, -0.0051,  0.0070, -0.0067,\n",
      "         0.0420,  0.0420,  0.0169,  0.0436, -0.0073,  0.0019, -0.0138, -0.0121,\n",
      "        -0.0211,  0.0369, -0.0352,  0.0078,  0.0407, -0.0406, -0.0316, -0.0367,\n",
      "        -0.0066, -0.0313,  0.0232, -0.0430,  0.0110, -0.0093, -0.0326,  0.0022,\n",
      "        -0.0272, -0.0142,  0.0053,  0.0231,  0.0134,  0.0051, -0.0046,  0.0437,\n",
      "         0.0181,  0.0240,  0.0205,  0.0018,  0.0341, -0.0368, -0.0023,  0.0311,\n",
      "        -0.0309,  0.0185, -0.0408,  0.0178,  0.0146, -0.0324, -0.0143, -0.0334,\n",
      "         0.0364, -0.0359,  0.0146,  0.0099,  0.0288, -0.0123,  0.0059, -0.0401,\n",
      "        -0.0094,  0.0221, -0.0073, -0.0174,  0.0320, -0.0023,  0.0018, -0.0095,\n",
      "         0.0204, -0.0368,  0.0375, -0.0293,  0.0111, -0.0368,  0.0384, -0.0241,\n",
      "         0.0145,  0.0260, -0.0184,  0.0140,  0.0284,  0.0307, -0.0012,  0.0119,\n",
      "         0.0439, -0.0396,  0.0147,  0.0328, -0.0350, -0.0043, -0.0392,  0.0095,\n",
      "        -0.0350,  0.0227,  0.0110,  0.0405,  0.0159, -0.0055,  0.0039, -0.0020,\n",
      "         0.0137, -0.0108,  0.0425,  0.0213, -0.0098, -0.0394,  0.0107,  0.0127,\n",
      "         0.0329,  0.0143,  0.0059, -0.0315, -0.0416, -0.0360,  0.0237,  0.0098,\n",
      "         0.0014,  0.0049, -0.0378, -0.0280, -0.0012,  0.0169,  0.0114, -0.0368,\n",
      "        -0.0157,  0.0335,  0.0389,  0.0104,  0.0338, -0.0326,  0.0181,  0.0161,\n",
      "         0.0293, -0.0338,  0.0171, -0.0093, -0.0340,  0.0192, -0.0126,  0.0100,\n",
      "         0.0164,  0.0048, -0.0091, -0.0100,  0.0366,  0.0003,  0.0413, -0.0160,\n",
      "         0.0426, -0.0144, -0.0071,  0.0169, -0.0232, -0.0226, -0.0029, -0.0286,\n",
      "         0.0042,  0.0349,  0.0048, -0.0071,  0.0322, -0.0025,  0.0212, -0.0371,\n",
      "        -0.0038,  0.0126, -0.0155,  0.0393, -0.0123,  0.0306,  0.0407, -0.0027,\n",
      "        -0.0126, -0.0122,  0.0356,  0.0353,  0.0055,  0.0153,  0.0010, -0.0398,\n",
      "        -0.0250, -0.0033, -0.0405, -0.0038,  0.0342, -0.0080, -0.0425,  0.0005,\n",
      "         0.0383, -0.0077,  0.0361, -0.0167,  0.0303,  0.0404, -0.0111,  0.0042,\n",
      "        -0.0305, -0.0405,  0.0404,  0.0075, -0.0282, -0.0331, -0.0096, -0.0280,\n",
      "        -0.0175, -0.0057, -0.0430,  0.0292,  0.0437, -0.0220, -0.0414,  0.0188,\n",
      "        -0.0044, -0.0281,  0.0315, -0.0413,  0.0209,  0.0094,  0.0139,  0.0127,\n",
      "        -0.0014,  0.0328,  0.0376,  0.0103,  0.0263, -0.0012,  0.0277,  0.0347,\n",
      "         0.0254, -0.0159,  0.0133,  0.0321,  0.0255, -0.0349, -0.0026,  0.0028,\n",
      "         0.0154, -0.0411,  0.0371,  0.0245, -0.0257, -0.0149, -0.0365, -0.0389,\n",
      "        -0.0030, -0.0187,  0.0440,  0.0041, -0.0275, -0.0440, -0.0046, -0.0424,\n",
      "         0.0171,  0.0432,  0.0010,  0.0356,  0.0114, -0.0115,  0.0150, -0.0101,\n",
      "        -0.0393,  0.0010, -0.0008,  0.0362,  0.0002,  0.0202,  0.0098,  0.0080,\n",
      "         0.0196,  0.0422,  0.0223, -0.0156,  0.0334, -0.0141,  0.0206, -0.0068,\n",
      "         0.0122,  0.0417,  0.0308, -0.0251,  0.0423, -0.0099, -0.0351, -0.0101,\n",
      "         0.0427,  0.0213, -0.0087,  0.0345, -0.0061,  0.0251, -0.0377, -0.0346,\n",
      "         0.0225,  0.0420,  0.0322,  0.0147, -0.0139,  0.0120,  0.0371,  0.0171,\n",
      "        -0.0296,  0.0278,  0.0202,  0.0167, -0.0435,  0.0155, -0.0221,  0.0262,\n",
      "        -0.0361,  0.0414, -0.0138,  0.0324,  0.0004,  0.0419, -0.0284,  0.0169,\n",
      "         0.0394,  0.0161,  0.0099, -0.0438, -0.0027,  0.0355, -0.0268, -0.0213,\n",
      "         0.0257, -0.0017, -0.0268,  0.0017,  0.0381, -0.0186, -0.0326,  0.0149,\n",
      "        -0.0305,  0.0007, -0.0238,  0.0292, -0.0439, -0.0035, -0.0127, -0.0172,\n",
      "        -0.0055,  0.0381, -0.0251, -0.0078,  0.0394,  0.0234,  0.0350,  0.0234,\n",
      "        -0.0313,  0.0173,  0.0365, -0.0401,  0.0079,  0.0400, -0.0289,  0.0004,\n",
      "        -0.0440, -0.0439, -0.0363, -0.0355, -0.0282,  0.0086, -0.0297, -0.0259,\n",
      "         0.0084, -0.0219, -0.0086,  0.0014, -0.0124,  0.0073, -0.0135, -0.0192,\n",
      "        -0.0235,  0.0294,  0.0148,  0.0299,  0.0254,  0.0238, -0.0359,  0.0271,\n",
      "         0.0071,  0.0025,  0.0270,  0.0151, -0.0272, -0.0275,  0.0148, -0.0143,\n",
      "        -0.0327, -0.0064,  0.0029,  0.0088,  0.0409,  0.0239,  0.0299, -0.0146,\n",
      "        -0.0402, -0.0216,  0.0097, -0.0111,  0.0288, -0.0162, -0.0241,  0.0053,\n",
      "        -0.0172,  0.0256, -0.0249, -0.0373,  0.0352,  0.0013,  0.0420,  0.0406,\n",
      "        -0.0061, -0.0390, -0.0010, -0.0208, -0.0154, -0.0343, -0.0179, -0.0262,\n",
      "        -0.0025, -0.0055, -0.0401,  0.0167,  0.0110,  0.0375, -0.0055,  0.0267,\n",
      "         0.0321, -0.0082,  0.0220,  0.0284,  0.0196, -0.0344, -0.0198, -0.0133,\n",
      "         0.0084, -0.0247,  0.0225, -0.0338,  0.0082, -0.0287, -0.0271, -0.0025,\n",
      "         0.0163, -0.0053, -0.0163,  0.0104,  0.0345, -0.0272, -0.0388,  0.0023],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.0.attn_1.v_linear.weight', Parameter containing:\n",
      "tensor([[ 0.0612, -0.0298, -0.0684,  ..., -0.0737,  0.0527, -0.0340],\n",
      "        [-0.0601,  0.0312, -0.0067,  ..., -0.0106, -0.0369, -0.0679],\n",
      "        [ 0.0730,  0.0741, -0.0599,  ..., -0.0348, -0.0733,  0.0549],\n",
      "        ...,\n",
      "        [-0.0600,  0.0294, -0.0465,  ..., -0.0650, -0.0533, -0.0095],\n",
      "        [ 0.0048, -0.0165,  0.0617,  ...,  0.0103,  0.0350,  0.0619],\n",
      "        [ 0.0455, -0.0583, -0.0073,  ...,  0.0538, -0.0540, -0.0061]],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.0.attn_1.v_linear.bias', Parameter containing:\n",
      "tensor([ 7.6864e-03,  2.4679e-03,  1.7577e-02, -2.1017e-02,  4.3056e-02,\n",
      "         2.5087e-02,  2.5152e-02, -1.3269e-02, -2.3054e-02,  4.2896e-02,\n",
      "        -1.8359e-02, -3.8088e-03, -2.5711e-03,  3.4326e-03,  1.6270e-02,\n",
      "        -5.0351e-03,  7.0423e-03, -7.8722e-03, -2.4609e-02, -6.8483e-03,\n",
      "        -1.4784e-02, -2.3725e-02, -2.0732e-02, -1.1557e-02,  3.9233e-02,\n",
      "        -4.7620e-03,  1.9538e-02, -4.1420e-02, -3.5757e-02,  2.6071e-02,\n",
      "        -2.9061e-02, -2.2952e-02, -1.2534e-02,  1.8338e-03,  5.4436e-03,\n",
      "         4.3617e-02, -5.0393e-03, -4.2454e-02, -3.4783e-02,  2.5328e-02,\n",
      "        -3.3186e-02, -6.4352e-03,  3.5670e-02, -3.2353e-03, -2.2845e-02,\n",
      "        -2.1957e-02, -3.0922e-02, -8.1887e-04, -1.1087e-02, -2.5720e-02,\n",
      "         4.2626e-02,  1.6772e-02,  1.2278e-02, -3.1026e-02, -6.3714e-03,\n",
      "        -4.2191e-02, -2.6561e-02, -3.2580e-02, -2.9507e-02,  3.3328e-02,\n",
      "        -2.2422e-02,  3.0108e-03,  3.7260e-03, -2.1786e-02, -6.3530e-03,\n",
      "        -5.1625e-03, -2.9086e-02, -2.2336e-02,  2.1983e-02, -5.1379e-03,\n",
      "         9.9927e-03,  4.3463e-02,  2.6271e-02, -4.6421e-03,  3.3183e-02,\n",
      "        -1.8169e-02,  3.8871e-02,  5.5561e-03,  1.2095e-02,  2.8116e-02,\n",
      "         1.9631e-02,  9.5049e-03,  3.8501e-02,  3.2243e-03, -2.8107e-02,\n",
      "         1.4170e-02,  1.2076e-03, -6.9066e-03, -4.4671e-03,  1.7440e-02,\n",
      "        -1.4973e-02,  2.0242e-02, -3.3910e-02, -1.1958e-02,  2.4406e-02,\n",
      "        -2.6287e-02, -2.7726e-03, -4.1987e-02,  1.2548e-02,  2.5995e-04,\n",
      "        -3.3298e-03, -2.4417e-02, -9.9068e-03, -2.7871e-02,  4.3174e-02,\n",
      "         7.4704e-03, -2.3004e-02, -7.3931e-03, -3.1627e-02,  4.0111e-03,\n",
      "        -1.0932e-02, -1.5317e-02, -2.3558e-02,  2.0372e-02, -4.3263e-02,\n",
      "        -1.3874e-02, -7.6232e-03,  3.6927e-02,  2.2058e-02,  1.7281e-02,\n",
      "         6.1286e-03, -2.8702e-02,  3.3779e-02,  4.4113e-02, -3.1840e-02,\n",
      "        -2.5328e-02,  2.9199e-02, -4.1012e-02,  4.3370e-02,  3.7192e-02,\n",
      "         4.3811e-02, -4.3637e-02,  4.3250e-02, -2.5424e-02,  1.6440e-02,\n",
      "         9.2930e-03,  2.2407e-02, -2.0419e-02,  2.1262e-02, -1.3380e-02,\n",
      "         3.7415e-02, -1.5759e-02, -1.9629e-02, -2.0540e-03, -3.5512e-02,\n",
      "        -1.9919e-02, -2.4166e-02,  1.5658e-02, -1.7744e-02,  2.7294e-02,\n",
      "         1.5809e-02, -3.5911e-02,  4.7238e-03,  3.5321e-02,  1.0152e-03,\n",
      "         2.8667e-02, -1.9346e-02, -3.7296e-02, -2.8623e-02,  3.4509e-02,\n",
      "         4.0426e-03,  1.8133e-02, -2.9510e-02, -3.3644e-02, -3.7553e-03,\n",
      "         1.8069e-02,  4.9546e-03,  1.9215e-02, -7.0378e-03, -4.1967e-02,\n",
      "        -4.2173e-02, -4.2437e-02,  8.0441e-04, -2.0034e-02, -1.9509e-02,\n",
      "        -4.2573e-02,  1.1635e-02, -3.7061e-02, -1.0213e-02,  9.3755e-03,\n",
      "        -3.5422e-02,  3.9723e-02, -1.4544e-02,  2.8527e-02,  2.4253e-02,\n",
      "        -5.6765e-03, -2.3718e-02, -1.3849e-02, -2.1298e-02,  2.1138e-02,\n",
      "         3.2532e-02, -2.4023e-02, -2.5016e-02,  2.7481e-02, -4.9093e-03,\n",
      "         2.9614e-02,  2.0985e-02, -3.3770e-02, -2.6084e-02, -4.2735e-02,\n",
      "         3.6042e-02,  2.2895e-02, -3.7283e-02, -5.8359e-03, -2.2994e-02,\n",
      "         3.5516e-02, -1.3600e-02,  1.0795e-02,  1.5029e-03,  6.0760e-03,\n",
      "         1.1186e-02,  8.9973e-03,  3.8305e-02, -5.7095e-03,  4.1454e-02,\n",
      "         3.8781e-02,  9.5694e-03, -3.3597e-02,  7.9106e-03,  2.3924e-02,\n",
      "         2.9387e-02, -1.3296e-02, -3.6553e-02,  2.3753e-02,  3.0703e-02,\n",
      "        -4.7558e-03, -4.4063e-02,  2.6579e-02, -5.6162e-03, -4.3746e-02,\n",
      "        -2.7969e-02, -3.2275e-03,  3.9790e-02,  2.4368e-02, -4.3819e-03,\n",
      "        -2.5777e-02, -2.9868e-02,  8.0229e-03,  8.8035e-03,  4.2978e-02,\n",
      "        -3.6586e-02, -3.9564e-03, -2.0689e-02, -7.2058e-04, -3.6361e-02,\n",
      "         1.1594e-02, -3.4211e-02, -1.9375e-02, -1.4611e-04,  3.4583e-02,\n",
      "         4.0237e-02, -9.6533e-03, -3.8850e-02,  4.3171e-03, -4.8126e-03,\n",
      "        -3.3538e-03,  3.0677e-02,  3.5340e-02, -3.3441e-02, -3.9261e-02,\n",
      "         9.8424e-03,  6.9954e-03,  2.9180e-02, -4.2872e-02,  7.4332e-03,\n",
      "         3.0806e-02,  2.0422e-02, -4.1457e-02, -2.5810e-02,  3.1029e-02,\n",
      "        -1.6856e-02, -1.6828e-03,  2.6251e-02,  1.3370e-02, -2.1739e-02,\n",
      "        -3.8082e-02, -4.0787e-02, -1.0354e-02, -1.8052e-02, -2.9824e-02,\n",
      "         2.7035e-02, -3.5906e-02, -2.2287e-02,  4.0839e-02, -2.5380e-02,\n",
      "        -4.2769e-02, -2.2076e-02,  5.7518e-03, -2.5494e-02, -2.6937e-02,\n",
      "         2.9460e-02,  3.7382e-02,  2.7211e-02,  8.4235e-03,  4.1829e-02,\n",
      "        -1.0074e-02, -2.4750e-02,  4.0910e-02, -9.3436e-03, -9.0326e-04,\n",
      "        -2.2569e-02,  2.5162e-02, -1.2517e-02, -1.3608e-02,  3.1974e-02,\n",
      "        -3.2894e-02,  2.0397e-02, -3.5290e-02,  2.0211e-03,  4.3922e-02,\n",
      "        -6.3214e-03,  3.2906e-02, -6.4667e-03,  3.1611e-02,  1.1816e-02,\n",
      "         3.1025e-03,  3.5599e-02, -1.3822e-02, -2.4340e-02, -8.7297e-03,\n",
      "        -1.4515e-02,  1.0160e-02, -9.1227e-04,  1.5968e-02, -3.5499e-02,\n",
      "        -8.1833e-04, -1.0508e-02,  3.8320e-03,  2.7889e-02, -2.5089e-02,\n",
      "         4.1066e-02,  1.8702e-02,  2.0330e-02, -7.3188e-03, -1.8312e-02,\n",
      "        -2.3545e-02, -9.0726e-03,  3.5240e-02, -3.2386e-02, -1.0541e-02,\n",
      "        -3.6122e-02, -9.5048e-03, -9.0939e-03, -3.5848e-02,  6.4315e-03,\n",
      "        -3.6720e-02, -4.2751e-02, -3.8821e-02,  1.2677e-02, -1.5172e-02,\n",
      "        -2.4869e-02, -4.1458e-02, -2.5716e-02,  9.9117e-03,  2.1375e-02,\n",
      "         3.1968e-02,  3.9553e-02, -1.5306e-02,  1.3648e-02,  2.1402e-02,\n",
      "        -2.4251e-02,  4.1744e-02, -2.1289e-02, -1.7941e-02, -3.8260e-02,\n",
      "        -1.1595e-02, -9.7332e-03, -1.4733e-02, -1.2559e-04, -4.0175e-02,\n",
      "         9.5568e-03,  2.4508e-02,  2.6807e-02, -1.9743e-02, -2.0078e-02,\n",
      "         8.4869e-03,  2.5053e-02,  2.3384e-02,  2.3367e-02,  2.9641e-02,\n",
      "        -3.7998e-06,  3.0425e-02, -2.3571e-02, -3.7900e-02,  6.6910e-03,\n",
      "        -4.1606e-02,  3.2235e-02, -2.7864e-02, -7.7990e-03, -3.2460e-02,\n",
      "         3.6988e-02, -3.0715e-03, -8.2954e-03,  3.1960e-02,  2.8920e-02,\n",
      "        -3.8386e-02,  4.2273e-02,  1.9033e-02,  3.1788e-02, -5.9496e-03,\n",
      "        -2.1264e-02, -4.3721e-02,  9.5335e-03, -4.1359e-02, -2.5038e-02,\n",
      "        -2.7131e-02,  2.3856e-02, -3.8368e-02,  5.3909e-03,  8.7545e-03,\n",
      "         3.7906e-02, -5.0169e-03, -1.1483e-02, -6.9676e-03,  7.4611e-03,\n",
      "         1.0427e-02, -1.4777e-02,  1.7483e-02, -4.3019e-02,  9.5992e-03,\n",
      "         1.0736e-02,  3.2488e-03,  2.6026e-02, -1.1847e-02,  2.4927e-02,\n",
      "         3.3379e-02,  3.5342e-03,  3.5602e-02,  8.9690e-03, -8.8486e-03,\n",
      "        -9.2954e-03, -3.9771e-02,  5.9516e-03, -2.6953e-02, -5.6426e-03,\n",
      "         2.7503e-02,  2.1842e-02,  1.3325e-02,  1.5181e-02, -6.0122e-04,\n",
      "         6.6233e-03, -1.6806e-02,  3.8975e-02,  1.0709e-04,  1.2905e-03,\n",
      "         3.7200e-02, -2.4513e-02,  1.3978e-02,  2.6283e-02, -3.5812e-03,\n",
      "         8.4892e-03, -1.9721e-02,  5.4475e-03,  2.1095e-02,  2.4723e-02,\n",
      "        -3.3199e-02,  1.4466e-02,  9.3854e-03, -1.1666e-02,  3.8961e-02,\n",
      "        -1.8548e-02,  2.8724e-02,  1.1248e-02, -2.9469e-02, -1.9309e-02,\n",
      "         2.2063e-02,  2.3108e-02,  2.1134e-03, -1.4522e-02,  2.4810e-02,\n",
      "        -1.6932e-02,  4.3090e-02, -3.2469e-02,  3.1950e-02, -1.6690e-03,\n",
      "         6.5339e-03,  5.8431e-04,  2.2885e-02, -2.7789e-02, -3.2920e-02,\n",
      "        -4.2368e-02,  2.6121e-02, -1.7672e-02,  4.1836e-03,  4.3750e-02,\n",
      "         3.1855e-02,  2.8028e-02, -2.0339e-02, -2.2816e-02, -3.6613e-02,\n",
      "         6.8200e-03, -2.7368e-03, -8.6415e-03, -1.7536e-02, -3.3693e-02,\n",
      "        -3.4396e-03,  3.6978e-02, -4.1452e-02, -9.9195e-03,  3.4647e-02,\n",
      "         4.0231e-02,  4.3903e-03,  2.2304e-02, -1.8560e-02,  1.2204e-02,\n",
      "        -2.0922e-02, -2.7063e-02,  4.2584e-02,  2.9390e-02,  2.2267e-02,\n",
      "         1.4912e-02,  3.9035e-02], requires_grad=True))\n",
      "('decoder.layers.0.attn_1.k_linear.weight', Parameter containing:\n",
      "tensor([[ 0.0495,  0.0072,  0.0522,  ..., -0.0390, -0.0716,  0.0478],\n",
      "        [-0.0663,  0.0546, -0.0693,  ...,  0.0464, -0.0093, -0.0405],\n",
      "        [ 0.0720, -0.0382, -0.0354,  ..., -0.0499, -0.0411, -0.0477],\n",
      "        ...,\n",
      "        [ 0.0518,  0.0221,  0.0228,  ..., -0.0473,  0.0421,  0.0659],\n",
      "        [-0.0290, -0.0043,  0.0181,  ..., -0.0266,  0.0291, -0.0184],\n",
      "        [ 0.0211,  0.0273,  0.0045,  ...,  0.0402, -0.0200,  0.0609]],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.0.attn_1.k_linear.bias', Parameter containing:\n",
      "tensor([-0.0106, -0.0335,  0.0361, -0.0170, -0.0066, -0.0016, -0.0365, -0.0389,\n",
      "        -0.0289, -0.0347,  0.0113, -0.0271,  0.0025,  0.0358,  0.0381,  0.0101,\n",
      "         0.0038, -0.0240, -0.0029,  0.0087,  0.0196,  0.0338,  0.0089,  0.0130,\n",
      "         0.0142, -0.0073,  0.0376, -0.0087,  0.0163, -0.0381,  0.0291, -0.0275,\n",
      "         0.0371,  0.0211, -0.0042, -0.0271,  0.0100, -0.0377, -0.0399, -0.0333,\n",
      "         0.0249, -0.0107, -0.0251,  0.0275, -0.0256,  0.0360, -0.0105, -0.0288,\n",
      "        -0.0225, -0.0088, -0.0071,  0.0354,  0.0351,  0.0034, -0.0155,  0.0168,\n",
      "        -0.0288,  0.0421,  0.0419, -0.0052,  0.0226,  0.0222,  0.0329,  0.0294,\n",
      "        -0.0060, -0.0381, -0.0337,  0.0318, -0.0376,  0.0397, -0.0047,  0.0247,\n",
      "         0.0131, -0.0132,  0.0201, -0.0308,  0.0122,  0.0259, -0.0183, -0.0278,\n",
      "        -0.0183, -0.0208,  0.0126, -0.0234,  0.0389, -0.0320, -0.0298, -0.0400,\n",
      "        -0.0268, -0.0394, -0.0353,  0.0012,  0.0113, -0.0346, -0.0093, -0.0315,\n",
      "        -0.0382, -0.0242,  0.0350, -0.0224,  0.0119,  0.0356,  0.0254, -0.0123,\n",
      "         0.0144,  0.0186,  0.0019,  0.0333, -0.0358, -0.0411, -0.0162, -0.0421,\n",
      "         0.0311, -0.0009,  0.0175,  0.0255,  0.0101,  0.0323, -0.0043, -0.0059,\n",
      "        -0.0003, -0.0315,  0.0315,  0.0221,  0.0105, -0.0046,  0.0033,  0.0040,\n",
      "         0.0288, -0.0441,  0.0216, -0.0250,  0.0008,  0.0028, -0.0216, -0.0184,\n",
      "        -0.0130, -0.0415, -0.0172, -0.0122, -0.0236,  0.0163,  0.0144, -0.0416,\n",
      "        -0.0310, -0.0388, -0.0415, -0.0307,  0.0368, -0.0357,  0.0338, -0.0296,\n",
      "         0.0331, -0.0310,  0.0280,  0.0112, -0.0080, -0.0189, -0.0023, -0.0313,\n",
      "        -0.0438, -0.0334,  0.0385,  0.0351,  0.0234, -0.0018,  0.0188,  0.0188,\n",
      "         0.0002, -0.0117, -0.0032,  0.0184,  0.0018, -0.0191, -0.0337, -0.0377,\n",
      "        -0.0422,  0.0151, -0.0302,  0.0253,  0.0004, -0.0193,  0.0080, -0.0326,\n",
      "        -0.0375, -0.0235, -0.0411, -0.0289,  0.0068,  0.0404,  0.0149,  0.0261,\n",
      "         0.0356, -0.0306,  0.0317,  0.0068, -0.0176,  0.0063,  0.0100, -0.0392,\n",
      "         0.0343, -0.0093,  0.0356, -0.0376,  0.0308,  0.0158,  0.0380, -0.0320,\n",
      "        -0.0278, -0.0420,  0.0036,  0.0440,  0.0040,  0.0165, -0.0044,  0.0368,\n",
      "        -0.0089,  0.0390,  0.0036,  0.0280,  0.0340, -0.0289,  0.0016,  0.0334,\n",
      "        -0.0133,  0.0136, -0.0049, -0.0344, -0.0346,  0.0035,  0.0400,  0.0378,\n",
      "        -0.0181, -0.0354, -0.0272,  0.0095, -0.0064,  0.0070, -0.0102, -0.0109,\n",
      "         0.0027,  0.0350, -0.0025, -0.0220,  0.0312,  0.0015,  0.0232,  0.0215,\n",
      "         0.0199,  0.0352, -0.0232, -0.0400,  0.0398,  0.0097,  0.0295, -0.0183,\n",
      "         0.0230, -0.0341, -0.0410, -0.0365,  0.0323,  0.0345, -0.0397,  0.0200,\n",
      "        -0.0288, -0.0435, -0.0181,  0.0342,  0.0338,  0.0294,  0.0309, -0.0390,\n",
      "         0.0239,  0.0176, -0.0059, -0.0438, -0.0173,  0.0420,  0.0203, -0.0034,\n",
      "        -0.0412, -0.0316, -0.0409, -0.0244,  0.0155, -0.0290, -0.0056, -0.0322,\n",
      "        -0.0071, -0.0032, -0.0298, -0.0376, -0.0048, -0.0021, -0.0090,  0.0170,\n",
      "        -0.0086, -0.0063,  0.0212, -0.0059, -0.0198, -0.0279,  0.0120, -0.0379,\n",
      "         0.0113, -0.0298, -0.0200,  0.0189,  0.0288,  0.0297, -0.0274, -0.0200,\n",
      "        -0.0202, -0.0425, -0.0133,  0.0436, -0.0082,  0.0060,  0.0199,  0.0285,\n",
      "         0.0117, -0.0292, -0.0017,  0.0412,  0.0408,  0.0343, -0.0023,  0.0362,\n",
      "         0.0213, -0.0200, -0.0028,  0.0032,  0.0284, -0.0196,  0.0063,  0.0356,\n",
      "         0.0347,  0.0236, -0.0292,  0.0268, -0.0233, -0.0329, -0.0124,  0.0345,\n",
      "        -0.0263,  0.0203,  0.0159,  0.0356, -0.0315,  0.0440, -0.0372, -0.0327,\n",
      "         0.0412, -0.0414,  0.0135,  0.0005,  0.0354, -0.0135, -0.0297, -0.0120,\n",
      "        -0.0188,  0.0244,  0.0260,  0.0177,  0.0367, -0.0204,  0.0347,  0.0134,\n",
      "         0.0293, -0.0058, -0.0242,  0.0405,  0.0093,  0.0309, -0.0434,  0.0199,\n",
      "         0.0279,  0.0082,  0.0433,  0.0151,  0.0154, -0.0422,  0.0181, -0.0336,\n",
      "        -0.0236,  0.0256, -0.0290,  0.0427,  0.0264, -0.0203, -0.0127,  0.0239,\n",
      "        -0.0370,  0.0367,  0.0263,  0.0252, -0.0080,  0.0133, -0.0096, -0.0085,\n",
      "         0.0283,  0.0132,  0.0340,  0.0440, -0.0077,  0.0390,  0.0316,  0.0022,\n",
      "         0.0093,  0.0066,  0.0200, -0.0258,  0.0392,  0.0189,  0.0426,  0.0011,\n",
      "        -0.0426,  0.0130, -0.0287, -0.0218,  0.0044, -0.0107,  0.0315,  0.0135,\n",
      "         0.0240,  0.0076, -0.0101,  0.0189,  0.0171,  0.0063,  0.0174, -0.0287,\n",
      "        -0.0331, -0.0294,  0.0361,  0.0383,  0.0027,  0.0276, -0.0157, -0.0192,\n",
      "        -0.0153, -0.0425, -0.0302, -0.0421, -0.0063,  0.0427, -0.0039,  0.0009,\n",
      "         0.0258,  0.0251,  0.0339, -0.0301, -0.0440, -0.0244,  0.0070, -0.0001,\n",
      "         0.0375, -0.0245, -0.0132, -0.0201, -0.0248, -0.0222, -0.0248,  0.0368,\n",
      "        -0.0322,  0.0259, -0.0272, -0.0358, -0.0029, -0.0027, -0.0274,  0.0419,\n",
      "        -0.0327,  0.0287, -0.0379, -0.0022, -0.0088, -0.0280,  0.0206, -0.0283,\n",
      "        -0.0256, -0.0147, -0.0058, -0.0157, -0.0266,  0.0247, -0.0384,  0.0224,\n",
      "        -0.0074, -0.0054, -0.0053, -0.0251,  0.0376, -0.0071,  0.0080, -0.0431,\n",
      "         0.0377, -0.0122, -0.0051,  0.0128, -0.0138, -0.0171,  0.0335, -0.0229,\n",
      "         0.0117, -0.0129, -0.0343, -0.0126,  0.0279,  0.0235, -0.0105,  0.0089],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.0.attn_1.out.weight', Parameter containing:\n",
      "tensor([[-0.0058, -0.0024,  0.0523,  ..., -0.0007,  0.0296, -0.0471],\n",
      "        [ 0.0663, -0.0651, -0.0296,  ...,  0.0593, -0.0140,  0.0180],\n",
      "        [ 0.0668, -0.0483, -0.0006,  ...,  0.0395,  0.0420,  0.0330],\n",
      "        ...,\n",
      "        [ 0.0092, -0.0739, -0.0598,  ..., -0.0455,  0.0696,  0.0142],\n",
      "        [-0.0523,  0.0196, -0.0452,  ...,  0.0201,  0.0757, -0.0568],\n",
      "        [ 0.0310, -0.0386,  0.0729,  ..., -0.0450,  0.0340,  0.0698]],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.0.attn_1.out.bias', Parameter containing:\n",
      "tensor([ 3.7839e-02, -3.7018e-03,  1.2705e-02,  5.7409e-03,  1.0879e-02,\n",
      "        -3.7578e-02,  3.9128e-02, -1.5581e-02,  1.2646e-02, -3.0819e-02,\n",
      "        -1.2883e-02, -1.2668e-02,  4.2512e-02,  2.1784e-02, -2.8842e-02,\n",
      "         1.7080e-02,  3.1626e-03,  2.6219e-02, -1.1322e-02, -1.2491e-02,\n",
      "        -2.8419e-02, -5.8973e-03,  3.9772e-02, -2.1022e-02, -1.4746e-02,\n",
      "         3.4695e-02, -5.5768e-03,  3.9291e-02, -4.3122e-02, -2.0004e-02,\n",
      "        -9.5473e-03, -2.4289e-02,  2.1035e-02,  3.6293e-02,  1.6386e-02,\n",
      "        -1.2124e-02,  2.6076e-02,  3.4190e-02,  1.2846e-02, -2.0536e-02,\n",
      "         1.5089e-03, -9.9265e-03, -1.9603e-02, -4.4001e-02, -2.3968e-02,\n",
      "         1.0120e-03, -4.0771e-02,  3.7776e-02,  1.0604e-02, -9.5416e-03,\n",
      "         2.5210e-02, -3.8277e-02, -1.0707e-02, -1.9833e-02, -4.2454e-02,\n",
      "         3.3660e-03, -1.0690e-02, -7.6141e-03, -1.3979e-02,  3.6956e-02,\n",
      "        -1.9594e-02,  3.0645e-02, -2.7786e-03, -2.5827e-02, -3.6579e-03,\n",
      "         3.1388e-03, -5.9267e-03,  2.2641e-02, -2.8526e-02,  2.0915e-02,\n",
      "         6.2763e-03,  3.9003e-03, -1.9435e-02,  1.5325e-03,  2.9935e-02,\n",
      "        -1.2033e-02,  2.4308e-02,  2.1456e-02, -2.1186e-02,  2.8821e-02,\n",
      "         1.7941e-02, -1.0436e-02,  2.5927e-02,  2.0454e-02, -1.0062e-03,\n",
      "         3.0160e-02,  3.6351e-02, -4.1463e-02, -1.9028e-02, -2.9625e-02,\n",
      "        -5.3240e-03,  2.7899e-02,  2.8308e-05, -4.0096e-02,  1.5164e-02,\n",
      "        -3.4037e-02,  1.6122e-02, -1.4024e-02, -1.7787e-02, -1.3052e-03,\n",
      "         1.2358e-02,  3.7147e-02,  3.6136e-02, -6.3820e-03,  3.5401e-02,\n",
      "        -3.3256e-02,  3.1483e-02, -8.5437e-04, -7.1094e-04, -1.1096e-02,\n",
      "         1.0526e-02, -9.2313e-03, -1.3451e-02, -2.6074e-02, -8.3067e-05,\n",
      "        -1.6913e-02, -1.8711e-02, -4.3713e-02, -4.1343e-02, -1.8421e-02,\n",
      "         2.3954e-02,  2.0211e-02,  2.8202e-02, -1.7105e-02, -2.5487e-02,\n",
      "        -9.9014e-03, -3.8806e-02, -7.6004e-03,  2.0969e-02,  3.6931e-02,\n",
      "        -1.4410e-03, -4.0333e-02, -4.2124e-02,  1.4265e-02, -9.0077e-03,\n",
      "         1.1511e-02, -2.4008e-02,  2.0176e-02,  1.7666e-02,  1.9711e-02,\n",
      "        -4.8446e-03, -1.5420e-02, -3.2098e-02,  2.0961e-02,  9.8023e-03,\n",
      "        -2.4037e-03, -8.0856e-03,  2.1361e-02, -2.7029e-02, -5.2154e-03,\n",
      "         4.0330e-02, -2.0579e-02,  1.0789e-02, -4.2650e-02, -2.0052e-02,\n",
      "        -3.6944e-03,  2.9029e-02,  2.6934e-02,  4.0697e-02,  2.0187e-02,\n",
      "        -1.8225e-02,  1.7928e-02, -1.5072e-02,  1.5929e-02,  4.2488e-04,\n",
      "         2.0543e-02,  4.4449e-03,  6.0168e-03,  1.6295e-02, -4.0755e-02,\n",
      "         2.8328e-02,  3.2426e-02, -1.8177e-02, -8.6282e-03, -1.3372e-02,\n",
      "        -2.3340e-02,  2.2558e-02,  3.0629e-02, -3.4108e-02, -3.6623e-02,\n",
      "         3.1847e-02,  3.6366e-03,  2.1876e-02,  2.5188e-02,  3.2415e-02,\n",
      "        -1.2889e-02, -3.0134e-02, -2.4812e-02,  3.3797e-02, -1.9183e-02,\n",
      "         2.7279e-03,  2.3212e-02,  3.0049e-02,  1.9188e-03,  1.8296e-02,\n",
      "         1.4108e-03, -1.8820e-02,  3.1205e-02,  2.5982e-02,  3.4831e-02,\n",
      "         2.1161e-02,  8.2867e-03, -4.3613e-02, -2.9696e-02,  1.6419e-02,\n",
      "        -3.2532e-02,  3.1099e-02,  3.6391e-03, -1.9705e-02, -2.1379e-02,\n",
      "        -3.5113e-02,  4.1587e-02,  2.7961e-02, -3.0152e-02, -3.9706e-02,\n",
      "        -2.4921e-02, -3.7570e-02,  3.0100e-02,  4.4029e-02,  4.1892e-02,\n",
      "        -1.9617e-02,  2.3617e-02,  3.9070e-02,  3.4635e-03, -2.1041e-02,\n",
      "         3.5393e-02, -2.6217e-02, -1.4884e-04, -1.8412e-02, -4.1005e-02,\n",
      "        -1.7689e-03,  3.3527e-02,  3.6411e-02,  2.8690e-02, -1.8816e-02,\n",
      "         1.5714e-02,  4.0934e-02,  2.8881e-02, -2.7246e-03,  3.3878e-02,\n",
      "        -4.2989e-02,  6.2526e-03, -3.3316e-02, -3.8292e-02,  2.2879e-02,\n",
      "        -1.5376e-03, -1.8618e-02,  3.5188e-02,  1.2192e-02,  2.9916e-02,\n",
      "        -1.2578e-02, -4.2723e-02,  2.0337e-02,  7.7306e-03, -1.8370e-02,\n",
      "         3.7336e-03, -1.0290e-02, -2.7841e-02,  9.0284e-03, -3.1104e-02,\n",
      "         3.2299e-02, -2.4336e-02,  2.9220e-02,  9.0481e-03,  3.5969e-02,\n",
      "        -2.9724e-02,  3.3838e-02, -3.6708e-02, -4.0903e-02, -3.4185e-02,\n",
      "         1.8821e-02,  2.3787e-02,  1.9415e-02, -3.6539e-02, -1.9024e-02,\n",
      "         3.7109e-02,  1.4463e-02, -3.0274e-02,  4.1409e-02, -4.0898e-03,\n",
      "        -3.3516e-02, -4.6036e-03,  8.7133e-03,  2.3464e-02,  1.9751e-02,\n",
      "        -5.9064e-03, -4.2505e-03,  9.7342e-03,  2.5219e-02,  1.3065e-02,\n",
      "        -9.2196e-03,  1.3164e-02, -4.0987e-02,  4.0561e-02,  3.5339e-02,\n",
      "         1.1895e-02, -2.4634e-02, -2.6308e-02,  5.9312e-03,  2.8902e-02,\n",
      "        -1.9868e-02,  6.2480e-03, -2.0132e-02, -4.2359e-02, -1.1985e-03,\n",
      "         2.4080e-02,  6.4712e-04,  3.5425e-02, -1.5040e-02,  1.9086e-02,\n",
      "        -3.6614e-02,  1.7846e-02,  3.1832e-02,  2.9820e-02,  3.2121e-02,\n",
      "        -2.1304e-02,  5.7210e-03,  2.7981e-03,  2.4141e-02, -4.0983e-02,\n",
      "         1.6688e-02,  1.6418e-03, -7.6768e-04, -4.1464e-02, -2.8838e-02,\n",
      "         1.8139e-02,  1.4434e-02, -2.8173e-02, -1.1949e-02, -4.4710e-03,\n",
      "        -2.0787e-02, -3.0215e-02, -3.0013e-02, -9.7611e-03,  3.8725e-02,\n",
      "        -1.3086e-02,  3.5651e-02,  1.8903e-02, -2.6065e-02,  2.2763e-02,\n",
      "        -2.5496e-02, -2.6008e-02,  1.5823e-02, -1.3017e-02,  4.3910e-02,\n",
      "         3.8262e-02, -3.2322e-02, -8.0930e-03,  1.2753e-02, -4.8396e-03,\n",
      "         1.7272e-02,  2.5733e-02, -6.5666e-03,  2.5794e-02, -3.6283e-02,\n",
      "         2.6007e-02,  4.4026e-02,  2.4074e-02,  6.1610e-03,  2.9763e-02,\n",
      "         4.2216e-02, -1.8794e-02, -3.5323e-02, -1.5159e-02,  2.6060e-02,\n",
      "        -1.8236e-02, -1.3672e-02, -2.9587e-02, -2.1956e-02, -1.5584e-02,\n",
      "        -1.0752e-02, -2.3050e-02,  3.1572e-02, -3.4120e-02, -4.2814e-02,\n",
      "         9.7658e-03,  3.2533e-02, -4.4163e-02, -3.3160e-02, -3.7977e-02,\n",
      "        -3.3005e-03,  3.6559e-02,  3.1347e-02,  3.9940e-03,  6.8300e-03,\n",
      "        -2.4128e-02, -4.1159e-02,  3.3116e-02, -3.6677e-02, -1.2722e-03,\n",
      "        -3.8886e-02,  3.2476e-02,  3.8552e-02,  7.4263e-03, -1.1465e-02,\n",
      "         1.7092e-02, -1.5960e-02,  2.8859e-02, -4.0647e-02,  1.8302e-02,\n",
      "         5.9262e-03,  2.5151e-02,  6.4211e-03,  1.5626e-02,  4.2561e-02,\n",
      "         1.1608e-02, -2.6409e-02,  2.8100e-03, -1.0482e-02, -1.1191e-02,\n",
      "         2.8645e-02,  1.7262e-02, -1.0095e-02,  3.9760e-02, -1.4124e-02,\n",
      "         2.6254e-02, -2.0240e-03,  4.5003e-03,  3.7706e-02,  2.5309e-02,\n",
      "        -3.1340e-04, -1.2833e-02,  1.5723e-02,  1.5785e-02, -2.7601e-02,\n",
      "        -2.4983e-02, -3.1119e-02,  2.2016e-02, -4.1312e-02,  3.9378e-02,\n",
      "        -3.5074e-02, -1.4809e-02, -6.4014e-03, -3.0035e-02,  3.6076e-02,\n",
      "        -2.7925e-03,  3.3335e-02, -2.5651e-02, -3.4761e-02,  3.5990e-02,\n",
      "         2.1713e-02,  2.2837e-02,  3.5131e-02,  3.2066e-02, -2.4614e-02,\n",
      "         3.8843e-02,  3.1795e-02, -1.9043e-02,  2.3963e-02,  4.0848e-02,\n",
      "         4.0062e-02, -3.4095e-02,  1.4334e-02, -2.3565e-02,  1.7387e-02,\n",
      "        -3.2224e-02,  1.2107e-02,  3.4349e-02,  6.0387e-03, -4.7857e-03,\n",
      "         1.8640e-02,  1.2035e-02,  2.4967e-02, -8.5778e-03, -1.9372e-02,\n",
      "        -2.8810e-03,  4.1481e-03, -2.2812e-02,  1.2971e-02,  2.6106e-02,\n",
      "        -1.1923e-02,  4.1689e-02, -3.0033e-02, -3.0815e-02, -1.3618e-02,\n",
      "         5.1665e-03,  1.9530e-02,  4.0235e-02, -2.1787e-02, -2.1926e-02,\n",
      "        -6.6425e-03, -1.7219e-02,  3.5138e-02,  2.4233e-03,  4.3693e-02,\n",
      "        -3.9770e-02,  3.5513e-02,  5.3113e-04, -9.2982e-03,  2.5814e-02,\n",
      "         1.5392e-02, -1.7870e-02,  4.3638e-02,  1.0346e-02,  3.1476e-02,\n",
      "         1.0387e-02, -5.4687e-03,  1.9538e-02,  3.4547e-02, -3.4805e-02,\n",
      "         1.3291e-02,  2.9038e-02,  9.5542e-03, -1.9219e-02, -8.7406e-03,\n",
      "         1.4032e-03, -1.3722e-02, -2.1624e-02,  8.8527e-03, -1.8672e-02,\n",
      "         8.3994e-03,  3.9076e-02], requires_grad=True))\n",
      "('decoder.layers.0.attn_2.q_linear.weight', Parameter containing:\n",
      "tensor([[-0.0195, -0.0229,  0.0645,  ..., -0.0063,  0.0161, -0.0277],\n",
      "        [-0.0625, -0.0527, -0.0004,  ..., -0.0389,  0.0428, -0.0606],\n",
      "        [ 0.0718, -0.0180, -0.0507,  ..., -0.0652,  0.0489,  0.0422],\n",
      "        ...,\n",
      "        [ 0.0110,  0.0353,  0.0758,  ...,  0.0137, -0.0367,  0.0468],\n",
      "        [-0.0542, -0.0760,  0.0562,  ..., -0.0223, -0.0151, -0.0384],\n",
      "        [ 0.0005,  0.0257, -0.0311,  ..., -0.0066,  0.0731,  0.0151]],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.0.attn_2.q_linear.bias', Parameter containing:\n",
      "tensor([-2.5482e-02,  3.0714e-02, -6.0663e-03,  1.1897e-02,  3.0474e-02,\n",
      "         3.0301e-02, -3.1985e-02, -3.7031e-02,  6.0669e-03,  1.9000e-02,\n",
      "        -8.8557e-03,  3.7603e-02, -2.8522e-02,  1.2932e-03, -2.9055e-02,\n",
      "        -3.4601e-02, -4.3047e-02, -1.4095e-02,  1.5802e-02, -3.9407e-02,\n",
      "         2.4407e-03, -1.3368e-02, -3.9014e-04, -2.2069e-02,  1.9206e-02,\n",
      "        -3.5177e-02,  2.9807e-02, -2.7268e-02, -1.2367e-02,  3.2838e-02,\n",
      "         3.7000e-02,  2.3486e-02,  2.9882e-02,  9.1628e-03,  9.7919e-03,\n",
      "         1.0459e-02,  3.0155e-02,  2.0088e-02,  1.2786e-03, -1.6787e-02,\n",
      "         2.5062e-03, -2.2193e-02, -4.0236e-02, -2.6964e-02,  2.3107e-02,\n",
      "         8.4816e-03,  3.3687e-02, -4.1483e-02,  1.1751e-02,  8.9036e-03,\n",
      "        -1.4256e-02,  1.5797e-02,  3.7354e-03,  1.5121e-02,  2.2440e-02,\n",
      "        -3.1762e-02,  2.8043e-02,  4.6021e-04,  6.9918e-03,  4.7586e-04,\n",
      "         4.1453e-02, -3.0235e-02,  2.8530e-02, -1.7765e-02,  3.4188e-02,\n",
      "         2.8130e-02,  2.5389e-02, -1.8493e-02,  1.0648e-02, -2.4989e-02,\n",
      "        -4.3058e-02,  1.7540e-03, -2.0182e-02,  8.9239e-03, -1.3243e-02,\n",
      "         2.0178e-02,  6.5989e-03, -2.3309e-02, -4.3183e-02, -4.2761e-02,\n",
      "         3.4816e-03, -2.1009e-02, -3.2948e-02,  2.3175e-02,  3.2013e-02,\n",
      "        -2.4826e-02,  3.0641e-02, -1.2838e-02, -1.0952e-02, -3.5433e-02,\n",
      "        -2.6887e-02, -1.5637e-03, -1.2366e-02, -3.9255e-02,  6.9197e-03,\n",
      "         3.6531e-03,  4.2345e-03, -2.4022e-02,  3.3792e-02,  1.9723e-02,\n",
      "         6.8231e-03, -2.9152e-02,  3.3015e-02,  3.3625e-02,  4.2973e-02,\n",
      "         2.7527e-02,  1.2121e-02, -8.6875e-03,  2.6844e-02,  2.9824e-03,\n",
      "         2.9995e-02, -1.0474e-02, -8.9787e-03, -1.2459e-02, -2.7010e-02,\n",
      "         3.8908e-02,  3.4352e-02,  2.3738e-02,  4.5970e-03,  8.7368e-03,\n",
      "        -2.6861e-02, -1.3570e-02, -3.1736e-02, -1.7313e-02,  1.5908e-02,\n",
      "        -4.0508e-02,  1.1367e-02,  4.3669e-02, -2.2099e-02,  1.8841e-02,\n",
      "        -1.9892e-02, -2.2990e-02, -5.8053e-03, -2.7299e-02, -5.4369e-03,\n",
      "        -2.0348e-02,  3.6915e-02,  2.4155e-02,  2.2364e-02,  4.3687e-02,\n",
      "         7.4108e-03,  1.9226e-02,  3.9295e-02,  2.9160e-02, -3.3085e-02,\n",
      "         2.6846e-02,  3.2686e-02,  3.9986e-02,  2.7835e-02, -1.0862e-02,\n",
      "         3.4819e-02, -2.6921e-02,  3.2904e-02, -2.9184e-03, -3.4028e-02,\n",
      "         2.2807e-02,  2.9692e-02, -3.3222e-02,  5.1688e-04,  2.7462e-02,\n",
      "        -2.0856e-02, -2.5439e-02, -3.7242e-02, -3.1748e-02, -1.3148e-02,\n",
      "         2.4778e-02, -3.8193e-02,  1.4601e-02,  1.5750e-02, -3.4304e-02,\n",
      "         1.5267e-02,  5.1452e-03, -2.9819e-02,  1.3821e-02, -1.4607e-02,\n",
      "         3.7516e-02, -6.9664e-03, -2.1493e-02,  2.3080e-02, -5.7208e-03,\n",
      "         1.4411e-02,  2.5053e-02, -1.7226e-02,  5.5899e-03, -4.2701e-02,\n",
      "         2.8992e-02, -1.6171e-02, -1.9829e-02,  4.0307e-02, -1.0757e-02,\n",
      "        -3.5512e-02,  4.3222e-02,  1.8056e-02, -9.1843e-03, -1.2304e-02,\n",
      "         7.9703e-03, -3.2069e-02, -2.3790e-03, -3.6655e-02,  2.2201e-02,\n",
      "        -1.0150e-03, -1.3599e-02,  5.3565e-03, -1.1565e-02, -1.7931e-02,\n",
      "         2.7155e-02,  1.3539e-02, -3.4794e-02,  5.1572e-03,  4.3300e-02,\n",
      "         1.3514e-02, -1.4745e-02, -8.0353e-03, -3.4925e-02, -3.3805e-02,\n",
      "        -4.3373e-02, -3.6016e-02, -8.5378e-03, -6.7933e-03,  1.5748e-02,\n",
      "         3.9594e-02,  2.0192e-02, -9.5662e-03, -2.6932e-02, -1.9671e-02,\n",
      "        -2.6813e-02, -2.6472e-02,  4.3858e-02, -1.2740e-02, -2.8625e-02,\n",
      "         8.9670e-03,  8.7612e-04,  6.5643e-03,  2.8251e-02, -3.9254e-03,\n",
      "         2.3640e-02,  2.6098e-02, -3.3330e-02, -6.1374e-03, -2.0684e-02,\n",
      "         8.8539e-05,  2.1978e-02,  6.2222e-03,  8.7017e-03,  4.0476e-02,\n",
      "         2.4504e-02,  1.4804e-02, -1.8249e-02, -4.9935e-03, -3.8450e-02,\n",
      "         1.9103e-03, -3.0672e-02,  1.6742e-02,  3.2058e-03, -3.9342e-02,\n",
      "         2.7563e-02, -3.1311e-02, -3.3850e-02, -3.0881e-02, -2.2993e-02,\n",
      "        -1.7722e-02, -2.1471e-02,  3.6008e-02, -1.0852e-03, -3.3856e-02,\n",
      "         7.8984e-03, -2.8955e-02, -2.8330e-02, -4.0416e-02, -6.6425e-03,\n",
      "        -1.3358e-03, -1.3506e-02,  8.2951e-03,  4.0256e-02,  1.4247e-02,\n",
      "        -2.3410e-02,  1.7502e-02,  3.8977e-02, -5.0270e-04, -2.8937e-02,\n",
      "         1.3463e-02, -3.6178e-02,  2.2233e-02, -5.5845e-03, -3.1568e-02,\n",
      "         4.0650e-02,  3.6801e-03, -4.1122e-02, -1.4874e-02, -1.8676e-02,\n",
      "         3.3474e-02, -3.9274e-02,  3.6345e-04, -3.6430e-02, -4.3088e-02,\n",
      "        -2.9775e-02, -9.3694e-03,  2.5574e-03, -1.0642e-02, -1.2236e-02,\n",
      "        -1.4113e-02, -3.2563e-02, -3.2588e-02,  4.1545e-02,  3.1261e-02,\n",
      "        -1.7577e-02,  1.8762e-02,  1.9510e-02,  1.1460e-03, -3.4310e-02,\n",
      "         1.9508e-02,  2.6744e-02, -1.4071e-02,  2.1795e-02, -7.1898e-03,\n",
      "        -1.0482e-02, -9.6790e-03, -2.2857e-02, -2.3044e-02, -2.8128e-03,\n",
      "         1.2064e-02,  1.0245e-02,  2.1810e-02, -2.9756e-02,  3.3459e-02,\n",
      "        -1.9853e-02, -2.0372e-02,  2.2534e-02, -2.0185e-02,  3.5615e-03,\n",
      "         1.2387e-03, -2.1572e-02,  1.0538e-02, -2.4377e-02,  4.0823e-02,\n",
      "         1.5031e-02,  3.5548e-02,  1.1770e-02, -2.7254e-02,  1.7577e-02,\n",
      "        -4.8988e-03, -3.0110e-03, -2.7723e-02,  3.8042e-02,  1.5410e-02,\n",
      "        -1.4447e-02,  1.5154e-02, -3.4693e-02, -8.3186e-03, -4.2211e-03,\n",
      "        -1.0352e-02,  1.8293e-02,  2.6971e-02, -2.8845e-02, -1.6046e-02,\n",
      "         4.9492e-04, -7.3191e-03,  3.0229e-02, -3.4160e-02, -1.0508e-02,\n",
      "         2.7467e-02, -1.9408e-02, -9.0138e-03, -4.7527e-04,  2.4726e-02,\n",
      "        -1.3950e-02,  3.8254e-02, -3.9737e-02,  3.5363e-02, -2.6827e-02,\n",
      "         3.0671e-02, -1.7320e-02, -7.5885e-03, -3.2979e-02, -1.4071e-02,\n",
      "         5.7309e-03,  3.9878e-02,  2.8429e-02,  5.7303e-03, -2.5669e-02,\n",
      "        -6.9241e-03,  3.5904e-02,  1.4375e-02, -2.4654e-02, -2.2532e-02,\n",
      "        -2.2109e-02, -2.5829e-02, -2.3874e-03, -1.9579e-02,  1.8770e-02,\n",
      "        -4.3753e-03, -4.2310e-02,  6.0061e-03, -3.8300e-03, -3.0297e-02,\n",
      "        -3.2799e-02, -2.1552e-04,  3.2620e-02, -3.5867e-02,  3.0070e-02,\n",
      "        -3.0505e-02,  2.0812e-02, -1.7911e-02, -7.8726e-03,  1.2186e-03,\n",
      "         2.4234e-02,  3.0186e-02,  4.2105e-02,  2.6926e-02, -1.9335e-02,\n",
      "        -5.2480e-03, -4.2212e-02, -4.1683e-03,  1.7388e-02, -2.8718e-02,\n",
      "        -3.3605e-03,  3.7418e-02,  4.3359e-02, -2.8217e-02, -5.4842e-03,\n",
      "        -1.6285e-03, -3.8718e-02, -7.9592e-03,  2.8548e-02, -6.8346e-03,\n",
      "         2.8339e-02, -3.5978e-02, -3.6012e-03,  2.6777e-02, -4.2263e-03,\n",
      "         3.4270e-02,  9.0236e-03, -3.8775e-02, -3.3004e-02, -1.1122e-02,\n",
      "         2.1303e-02,  6.1431e-03,  1.7769e-03, -2.1188e-02, -1.4132e-02,\n",
      "         6.3711e-03, -3.8522e-02, -4.3171e-02,  2.3190e-02,  1.3825e-02,\n",
      "        -4.5304e-03,  2.0327e-03,  4.0388e-02,  2.7408e-02, -4.8812e-03,\n",
      "        -2.4753e-02, -9.2369e-04, -4.8755e-03,  9.9780e-03,  4.0706e-02,\n",
      "         1.6470e-02, -2.4118e-02,  7.6996e-03,  3.6924e-02,  1.7864e-02,\n",
      "         1.8402e-02, -3.9133e-02, -8.7902e-05,  8.1743e-03,  3.9232e-02,\n",
      "         6.9311e-04,  2.2485e-02, -2.8241e-02, -3.9161e-03, -2.9401e-03,\n",
      "        -3.4624e-02, -3.4317e-02,  6.5667e-03, -3.2655e-02, -1.0732e-02,\n",
      "        -1.2539e-02, -4.1604e-02, -3.1626e-02,  3.1179e-02, -8.4892e-03,\n",
      "        -3.1026e-02, -2.0138e-02, -1.2362e-02,  4.6447e-03, -2.7697e-02,\n",
      "        -2.7785e-02,  3.2101e-02,  8.1615e-03, -2.2313e-02, -1.1509e-02,\n",
      "        -4.2134e-02, -2.9346e-02,  2.2162e-02, -1.2102e-02,  6.3674e-03,\n",
      "         1.6563e-02,  3.5800e-02, -2.4614e-02, -2.5821e-02,  3.5310e-03,\n",
      "        -7.8498e-03, -2.2009e-03, -3.6214e-02,  2.6199e-02,  3.1422e-02,\n",
      "        -3.7858e-02,  9.0693e-03,  9.9968e-03, -2.8525e-03, -1.1557e-02,\n",
      "        -1.8754e-02,  4.1551e-02], requires_grad=True))\n",
      "('decoder.layers.0.attn_2.v_linear.weight', Parameter containing:\n",
      "tensor([[-0.0606,  0.0752, -0.0013,  ..., -0.0242, -0.0630, -0.0383],\n",
      "        [ 0.0002,  0.0332,  0.0532,  ...,  0.0004, -0.0341, -0.0611],\n",
      "        [ 0.0561,  0.0297,  0.0581,  ..., -0.0134,  0.0463,  0.0557],\n",
      "        ...,\n",
      "        [ 0.0557,  0.0487, -0.0333,  ...,  0.0642, -0.0276,  0.0727],\n",
      "        [-0.0226,  0.0737,  0.0278,  ..., -0.0370,  0.0263, -0.0627],\n",
      "        [ 0.0108,  0.0274, -0.0606,  ..., -0.0405,  0.0469, -0.0230]],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.0.attn_2.v_linear.bias', Parameter containing:\n",
      "tensor([ 0.0167,  0.0042,  0.0015,  0.0388,  0.0187, -0.0118, -0.0336,  0.0099,\n",
      "        -0.0420, -0.0141, -0.0029, -0.0404, -0.0334, -0.0359, -0.0059,  0.0140,\n",
      "         0.0436,  0.0397, -0.0325, -0.0274,  0.0164,  0.0177,  0.0264,  0.0226,\n",
      "        -0.0126, -0.0265,  0.0262,  0.0333,  0.0210,  0.0020, -0.0091,  0.0335,\n",
      "         0.0143, -0.0260,  0.0352, -0.0002,  0.0263, -0.0074,  0.0011,  0.0039,\n",
      "         0.0222,  0.0327,  0.0053,  0.0314, -0.0343, -0.0066, -0.0367, -0.0377,\n",
      "        -0.0240, -0.0373, -0.0153,  0.0213,  0.0421, -0.0375, -0.0123,  0.0322,\n",
      "         0.0053,  0.0078, -0.0325,  0.0319,  0.0293,  0.0122, -0.0441,  0.0313,\n",
      "        -0.0168, -0.0334, -0.0177, -0.0211,  0.0120,  0.0122,  0.0192, -0.0255,\n",
      "         0.0274,  0.0074, -0.0299, -0.0018, -0.0054,  0.0225,  0.0166,  0.0200,\n",
      "         0.0389, -0.0179,  0.0077,  0.0316, -0.0214,  0.0340,  0.0207, -0.0309,\n",
      "        -0.0297,  0.0280, -0.0361, -0.0095, -0.0432, -0.0416,  0.0032, -0.0337,\n",
      "        -0.0009,  0.0207,  0.0250, -0.0247, -0.0344,  0.0253,  0.0036,  0.0073,\n",
      "         0.0302,  0.0309, -0.0172, -0.0132, -0.0142, -0.0306,  0.0387,  0.0439,\n",
      "         0.0438,  0.0384,  0.0097, -0.0430, -0.0412,  0.0430,  0.0440, -0.0205,\n",
      "        -0.0208, -0.0013, -0.0183, -0.0072, -0.0252, -0.0388, -0.0046, -0.0421,\n",
      "        -0.0370, -0.0193, -0.0167, -0.0160,  0.0354,  0.0114, -0.0112,  0.0100,\n",
      "        -0.0136,  0.0103,  0.0280,  0.0092,  0.0180, -0.0189,  0.0122, -0.0207,\n",
      "         0.0240,  0.0089, -0.0355, -0.0186, -0.0144,  0.0003, -0.0369, -0.0393,\n",
      "         0.0300,  0.0208,  0.0162, -0.0107,  0.0411,  0.0397, -0.0118,  0.0258,\n",
      "        -0.0045,  0.0132, -0.0416, -0.0174,  0.0413, -0.0180, -0.0414,  0.0341,\n",
      "        -0.0138, -0.0403,  0.0196,  0.0224, -0.0296, -0.0175,  0.0045,  0.0081,\n",
      "        -0.0215,  0.0073,  0.0255,  0.0181, -0.0014,  0.0010, -0.0032,  0.0265,\n",
      "         0.0370,  0.0246,  0.0435, -0.0369, -0.0066, -0.0441,  0.0225,  0.0157,\n",
      "        -0.0252, -0.0292, -0.0117,  0.0154,  0.0286, -0.0289, -0.0113, -0.0082,\n",
      "        -0.0354, -0.0051, -0.0087, -0.0435,  0.0171,  0.0253,  0.0225, -0.0164,\n",
      "        -0.0335,  0.0323,  0.0398,  0.0038, -0.0210,  0.0406, -0.0122, -0.0255,\n",
      "        -0.0380,  0.0415, -0.0124,  0.0001, -0.0401, -0.0123, -0.0304, -0.0194,\n",
      "        -0.0171, -0.0205, -0.0216, -0.0151,  0.0165,  0.0283,  0.0424,  0.0104,\n",
      "         0.0223,  0.0219,  0.0021, -0.0414,  0.0196, -0.0257, -0.0032,  0.0015,\n",
      "         0.0228, -0.0246, -0.0095, -0.0440,  0.0276,  0.0260, -0.0148, -0.0113,\n",
      "         0.0418, -0.0098,  0.0401,  0.0179,  0.0381, -0.0257, -0.0217,  0.0157,\n",
      "         0.0104,  0.0261, -0.0302, -0.0292, -0.0259,  0.0309,  0.0211,  0.0300,\n",
      "         0.0187, -0.0353, -0.0097,  0.0374, -0.0071, -0.0308, -0.0290, -0.0106,\n",
      "         0.0079, -0.0231, -0.0441, -0.0060, -0.0260, -0.0150, -0.0220,  0.0228,\n",
      "        -0.0404, -0.0058, -0.0157,  0.0016,  0.0019, -0.0282,  0.0330,  0.0222,\n",
      "         0.0438,  0.0400, -0.0148, -0.0361, -0.0079,  0.0227,  0.0147,  0.0324,\n",
      "        -0.0111, -0.0244,  0.0005, -0.0420,  0.0375,  0.0128,  0.0003, -0.0033,\n",
      "        -0.0160, -0.0352, -0.0330, -0.0392,  0.0117,  0.0017, -0.0236,  0.0113,\n",
      "        -0.0304, -0.0013, -0.0393,  0.0262, -0.0300, -0.0287,  0.0212,  0.0087,\n",
      "        -0.0187,  0.0075, -0.0155, -0.0077,  0.0216, -0.0251,  0.0141, -0.0240,\n",
      "         0.0162, -0.0137,  0.0250, -0.0289,  0.0429, -0.0307,  0.0034,  0.0132,\n",
      "        -0.0066, -0.0215, -0.0202, -0.0271,  0.0214, -0.0135,  0.0088, -0.0221,\n",
      "         0.0409,  0.0305,  0.0338, -0.0435,  0.0018, -0.0418, -0.0248,  0.0275,\n",
      "        -0.0431, -0.0280,  0.0086, -0.0258, -0.0439, -0.0355,  0.0266, -0.0384,\n",
      "        -0.0399,  0.0415,  0.0367,  0.0146,  0.0094, -0.0206, -0.0385,  0.0410,\n",
      "         0.0399, -0.0081, -0.0418, -0.0313, -0.0020, -0.0208,  0.0026,  0.0059,\n",
      "        -0.0279,  0.0322, -0.0364,  0.0296,  0.0322,  0.0278,  0.0336,  0.0023,\n",
      "         0.0440, -0.0152, -0.0177, -0.0420, -0.0439, -0.0325,  0.0030, -0.0178,\n",
      "        -0.0155,  0.0434,  0.0231, -0.0358,  0.0389, -0.0166,  0.0279, -0.0338,\n",
      "         0.0145,  0.0273,  0.0010,  0.0229,  0.0094, -0.0368,  0.0353,  0.0271,\n",
      "         0.0282, -0.0217, -0.0328,  0.0095, -0.0145,  0.0027,  0.0020, -0.0412,\n",
      "        -0.0333, -0.0059, -0.0130, -0.0050,  0.0051,  0.0133, -0.0201,  0.0342,\n",
      "         0.0379,  0.0279, -0.0338, -0.0395, -0.0087, -0.0252, -0.0001,  0.0384,\n",
      "        -0.0092, -0.0075, -0.0266,  0.0025,  0.0374,  0.0178,  0.0158, -0.0215,\n",
      "         0.0182,  0.0408,  0.0125, -0.0376,  0.0108, -0.0173,  0.0095, -0.0098,\n",
      "         0.0190,  0.0354, -0.0251, -0.0249, -0.0439, -0.0336, -0.0390,  0.0118,\n",
      "         0.0367,  0.0428, -0.0021,  0.0026,  0.0053,  0.0286,  0.0015, -0.0079,\n",
      "         0.0387, -0.0308, -0.0211, -0.0171,  0.0177, -0.0025,  0.0324,  0.0328,\n",
      "         0.0098, -0.0228, -0.0238,  0.0199,  0.0256,  0.0193,  0.0276, -0.0256,\n",
      "         0.0065, -0.0156,  0.0254, -0.0032,  0.0089, -0.0423,  0.0218, -0.0107,\n",
      "         0.0007, -0.0398, -0.0075, -0.0005, -0.0367,  0.0330,  0.0313, -0.0047,\n",
      "         0.0097,  0.0409,  0.0381,  0.0154,  0.0031, -0.0259, -0.0302, -0.0050,\n",
      "        -0.0280,  0.0109, -0.0369, -0.0140, -0.0192,  0.0334,  0.0013, -0.0253],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.0.attn_2.k_linear.weight', Parameter containing:\n",
      "tensor([[ 0.0747,  0.0693,  0.0081,  ...,  0.0523, -0.0487,  0.0739],\n",
      "        [-0.0389,  0.0496,  0.0476,  ...,  0.0358, -0.0721,  0.0108],\n",
      "        [ 0.0587, -0.0548,  0.0437,  ...,  0.0152,  0.0016, -0.0181],\n",
      "        ...,\n",
      "        [-0.0364, -0.0146,  0.0275,  ...,  0.0175, -0.0489,  0.0599],\n",
      "        [ 0.0644, -0.0754, -0.0031,  ...,  0.0181, -0.0660, -0.0730],\n",
      "        [ 0.0717,  0.0146,  0.0101,  ...,  0.0518,  0.0156, -0.0264]],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.0.attn_2.k_linear.bias', Parameter containing:\n",
      "tensor([ 1.3938e-02, -3.7332e-02, -4.0067e-02, -7.7398e-03,  3.7865e-02,\n",
      "         3.7442e-02, -3.1719e-02,  1.4165e-02, -1.3083e-02, -4.0572e-02,\n",
      "        -1.0891e-02,  1.9268e-02,  2.2367e-03, -1.5541e-02, -1.5248e-02,\n",
      "         3.2123e-02,  3.7047e-02, -1.9573e-02,  2.2611e-02, -2.7711e-02,\n",
      "        -1.1166e-02,  2.6144e-02,  1.1372e-02, -3.4058e-02, -5.2031e-03,\n",
      "         2.7762e-03,  3.4416e-02,  1.9784e-02,  2.1214e-02,  1.0560e-02,\n",
      "        -2.4544e-02,  4.0631e-02,  1.1471e-02,  3.9509e-02, -2.4048e-02,\n",
      "        -2.8959e-02, -2.8755e-02, -2.6552e-02,  4.0530e-02,  2.2012e-02,\n",
      "         2.5531e-02,  2.6780e-03,  1.9982e-02, -2.0746e-02,  4.3522e-02,\n",
      "        -2.4086e-02,  2.2699e-02,  2.4684e-02,  1.2928e-02, -1.1247e-02,\n",
      "         8.9669e-03, -2.2107e-02, -1.3124e-02, -2.1815e-02,  4.2267e-02,\n",
      "         3.5852e-02, -1.9017e-02, -3.1487e-02,  6.7544e-03,  1.2353e-02,\n",
      "        -1.6820e-02,  1.5105e-03, -2.6420e-02,  2.5815e-02,  1.8752e-02,\n",
      "         5.9032e-03,  8.2256e-03,  1.9890e-02, -1.3120e-03,  1.5450e-02,\n",
      "        -2.0813e-04, -3.6867e-02,  3.1581e-02, -4.2273e-02, -3.8674e-02,\n",
      "         2.0190e-02, -3.5987e-02, -8.0832e-03,  3.3706e-02, -1.4890e-02,\n",
      "        -4.3016e-02,  2.2034e-02, -2.2999e-02,  3.8586e-02,  2.8095e-02,\n",
      "        -2.3731e-02, -4.4727e-03,  9.8589e-03,  3.0196e-02,  3.3746e-02,\n",
      "         1.4102e-02, -2.9597e-02,  5.1847e-03, -3.1812e-02, -3.2524e-02,\n",
      "         1.5927e-02,  5.2262e-03, -1.4796e-02,  2.6893e-02,  2.1504e-02,\n",
      "         1.2162e-03, -2.9461e-02, -8.2447e-03, -1.3766e-02,  4.3670e-02,\n",
      "         2.2534e-02, -8.2402e-03,  1.3567e-02, -1.4638e-02, -3.8840e-02,\n",
      "        -1.0086e-02, -1.5539e-02,  1.2620e-02,  4.1148e-02,  2.6367e-02,\n",
      "        -1.5349e-02, -3.4769e-03, -2.6867e-02, -1.0960e-02,  3.3831e-02,\n",
      "        -1.9616e-02, -5.4006e-03,  4.6570e-03,  3.6762e-02,  2.1673e-02,\n",
      "        -3.0693e-05,  1.4730e-02, -2.6605e-02, -3.0572e-02,  1.7292e-02,\n",
      "        -1.2778e-02, -5.2169e-03,  5.4665e-04,  4.3658e-02, -2.2695e-02,\n",
      "         2.1813e-02,  2.1311e-02, -3.6541e-02,  2.0255e-02, -5.3841e-03,\n",
      "         3.8453e-03, -1.7646e-02, -1.2161e-03,  2.9495e-02,  3.0144e-02,\n",
      "         2.2463e-02,  7.3353e-03,  1.6860e-02, -2.1675e-02,  5.6475e-03,\n",
      "         2.9098e-03,  1.0751e-02,  2.8389e-02,  3.0835e-02, -8.2080e-03,\n",
      "         1.0898e-02, -5.2218e-03, -3.1584e-02,  2.2036e-02,  2.1354e-02,\n",
      "         4.0707e-02, -1.1129e-02,  3.4521e-02, -1.0409e-02, -8.2819e-04,\n",
      "        -1.8052e-02,  4.3793e-02,  1.8036e-02, -3.8044e-02, -2.9508e-02,\n",
      "         2.7679e-02,  3.1041e-02,  1.0423e-02,  2.8182e-02,  4.1788e-03,\n",
      "         6.1217e-04, -2.9267e-03, -3.0371e-02, -1.2052e-02,  7.6905e-03,\n",
      "        -4.3121e-02, -2.2400e-02,  1.1063e-02, -4.8389e-03,  2.9504e-02,\n",
      "        -1.4690e-02, -4.1097e-02, -1.7985e-02,  2.6780e-02,  3.0236e-02,\n",
      "         2.8071e-02, -3.9107e-02, -1.5111e-02,  5.1241e-03,  4.3484e-02,\n",
      "        -4.2470e-02,  3.5656e-02, -3.1900e-02,  1.3611e-02, -8.6843e-03,\n",
      "         6.2164e-03,  4.0238e-02,  3.5091e-02, -2.7689e-02,  2.8936e-02,\n",
      "        -9.3379e-03,  3.0636e-02, -8.7199e-03,  3.9968e-02,  2.5985e-02,\n",
      "        -3.2293e-02, -2.4484e-02,  3.6208e-02, -3.0700e-02, -1.9345e-02,\n",
      "         2.4478e-02,  2.3904e-02, -3.0927e-02, -3.3487e-02,  2.9518e-02,\n",
      "        -4.2648e-02, -3.4386e-02, -4.3801e-02,  3.9256e-02,  5.5716e-03,\n",
      "        -2.0797e-02, -3.8741e-02, -8.1682e-03, -1.8243e-02, -4.0012e-02,\n",
      "        -4.2483e-02,  2.7821e-02, -8.5296e-03,  4.1980e-02,  4.1232e-02,\n",
      "         3.1933e-02,  1.6851e-02,  1.7261e-02, -7.9419e-03,  2.6870e-02,\n",
      "         3.4684e-02, -4.5694e-04,  2.9625e-02, -3.1271e-02, -2.8850e-02,\n",
      "         1.5638e-02, -3.1055e-02,  8.5698e-03, -7.5822e-03, -1.8197e-02,\n",
      "        -1.7339e-02, -4.0982e-03, -2.4417e-02,  1.4339e-02, -2.3912e-02,\n",
      "        -4.2143e-02, -3.5163e-02,  4.4215e-03,  6.1657e-03,  2.7215e-02,\n",
      "        -8.7577e-03,  3.9615e-02, -4.2073e-04,  3.6038e-02,  9.0713e-03,\n",
      "         4.2430e-02, -1.2232e-02,  4.2699e-03,  1.2593e-02, -4.1661e-02,\n",
      "         2.5124e-02,  1.2154e-02, -2.6966e-02, -7.6992e-03,  3.7570e-02,\n",
      "        -1.4733e-02, -1.7789e-02,  5.4201e-03,  1.9172e-02, -2.0125e-02,\n",
      "        -3.5029e-02, -2.0505e-02, -1.5556e-02, -1.9945e-02,  2.2554e-02,\n",
      "         3.0309e-02, -4.6763e-03,  3.7393e-02,  1.4364e-02,  4.1287e-02,\n",
      "         3.7036e-02,  1.9832e-02,  1.5466e-02, -3.8335e-02,  1.8149e-02,\n",
      "        -1.3149e-02,  1.7274e-02, -1.3069e-02,  8.8808e-03, -1.3395e-02,\n",
      "        -1.2294e-02, -2.7799e-02, -1.3331e-02,  1.6545e-02, -3.7033e-03,\n",
      "        -3.7474e-02, -1.4192e-02,  1.8914e-02,  9.8568e-03, -9.3391e-03,\n",
      "        -4.1212e-02,  2.6474e-03,  1.8734e-02,  5.1248e-03, -5.8549e-03,\n",
      "         3.0118e-02, -1.0323e-02, -3.1572e-02,  2.9264e-02,  2.2993e-02,\n",
      "        -1.9774e-02, -3.7136e-02, -2.2545e-02,  3.6116e-02, -2.4400e-02,\n",
      "        -2.7735e-02,  1.1793e-03,  5.2318e-03,  8.5438e-03,  3.7425e-02,\n",
      "         2.0773e-02, -1.5894e-02,  1.7811e-03, -1.5192e-02, -2.9946e-02,\n",
      "        -3.8766e-02,  1.5813e-02, -5.6081e-03, -1.2233e-02, -5.8939e-04,\n",
      "        -1.7280e-02, -1.3693e-02, -4.3886e-02,  1.0366e-02, -4.2284e-02,\n",
      "        -2.4589e-02,  1.7155e-02, -3.0993e-02,  3.7894e-02, -2.4970e-02,\n",
      "        -9.8428e-03,  2.6930e-02,  1.8965e-02,  7.2971e-03,  1.1563e-02,\n",
      "        -2.5452e-02,  2.5158e-02,  2.1054e-02, -1.2459e-02,  1.0937e-02,\n",
      "         6.6902e-03,  4.2686e-02,  2.2985e-02, -3.7236e-02,  1.6621e-02,\n",
      "        -2.8605e-02, -1.7182e-02,  4.3439e-03,  3.1389e-02, -7.8019e-03,\n",
      "         2.9094e-02,  3.4010e-02, -1.4820e-02,  3.2440e-02, -2.4144e-02,\n",
      "         3.8620e-02, -2.9471e-02,  1.5017e-02, -2.5478e-02,  4.0641e-02,\n",
      "         1.9551e-02,  2.2706e-03,  3.8346e-02, -1.4723e-02,  6.4172e-03,\n",
      "         4.2597e-02, -1.1546e-02, -7.6626e-03, -3.3314e-02,  1.8813e-02,\n",
      "        -7.8689e-03, -3.0922e-02,  4.2355e-02,  1.0830e-02,  1.0255e-02,\n",
      "        -7.1867e-03, -2.0717e-02, -4.1965e-02,  2.5313e-02, -4.5262e-03,\n",
      "         3.4911e-02, -4.2448e-02,  4.3478e-02,  1.0231e-02,  9.3298e-03,\n",
      "         3.8511e-02,  1.1964e-02,  3.7712e-03, -7.8408e-03,  3.5335e-02,\n",
      "        -4.8520e-03,  9.6342e-03, -1.7491e-02,  2.1993e-02,  7.0694e-03,\n",
      "         2.9217e-02,  3.7878e-02, -1.6037e-02, -7.3108e-03, -3.5376e-02,\n",
      "         2.6896e-02,  3.9222e-02,  4.1938e-03, -2.1535e-03,  3.2693e-03,\n",
      "        -2.8668e-02,  1.3619e-02,  3.8551e-02, -1.0891e-02, -5.7712e-03,\n",
      "         4.5346e-03, -2.2775e-02,  2.1719e-02,  3.9838e-02,  1.4422e-02,\n",
      "         2.6602e-02,  4.3508e-02,  3.9355e-02,  4.2580e-02, -4.4051e-03,\n",
      "        -3.6674e-04, -2.4921e-03, -1.5541e-02,  3.8356e-02, -3.4502e-02,\n",
      "        -2.4381e-02,  1.9189e-02, -1.1995e-02,  2.7764e-02, -2.2565e-02,\n",
      "        -7.4358e-03, -7.1326e-03,  1.4016e-02, -1.7147e-03, -1.9489e-02,\n",
      "         3.1670e-02, -1.7100e-02,  1.7816e-02,  1.9530e-02,  3.3414e-02,\n",
      "        -2.7442e-02, -3.9697e-02, -1.5534e-02, -3.6892e-02, -9.1591e-03,\n",
      "         1.8394e-02,  1.7870e-02,  4.1531e-02, -1.2706e-02,  2.4354e-02,\n",
      "        -2.2333e-02,  9.0439e-04,  1.7876e-02,  3.1721e-02, -6.1151e-03,\n",
      "        -3.8680e-02,  1.6291e-02, -2.8952e-02, -1.7962e-02, -7.8490e-03,\n",
      "         3.8529e-02, -2.6850e-02, -1.7676e-03, -1.5404e-02,  1.6372e-02,\n",
      "         5.0040e-03,  3.9369e-02, -2.9330e-03, -1.1268e-02, -4.2890e-02,\n",
      "        -3.7718e-02,  3.1598e-02,  1.6663e-02,  1.0581e-03,  3.2305e-02,\n",
      "        -5.4008e-03,  9.5059e-03, -4.2184e-02,  3.6090e-02, -8.0770e-03,\n",
      "        -3.9813e-02, -3.5535e-02,  2.4808e-02,  3.5216e-02,  2.1829e-02,\n",
      "        -1.8571e-02, -3.8547e-02,  2.7334e-02, -2.3735e-02,  3.3273e-02,\n",
      "        -3.7305e-02, -2.1206e-02], requires_grad=True))\n",
      "('decoder.layers.0.attn_2.out.weight', Parameter containing:\n",
      "tensor([[ 0.0197, -0.0353,  0.0648,  ..., -0.0643,  0.0369,  0.0088],\n",
      "        [ 0.0234, -0.0504,  0.0739,  ...,  0.0076,  0.0554, -0.0497],\n",
      "        [-0.0105,  0.0017, -0.0667,  ..., -0.0215,  0.0544,  0.0741],\n",
      "        ...,\n",
      "        [ 0.0088,  0.0664, -0.0116,  ..., -0.0710, -0.0568,  0.0267],\n",
      "        [ 0.0433,  0.0371,  0.0449,  ...,  0.0369,  0.0684, -0.0320],\n",
      "        [-0.0575, -0.0012, -0.0524,  ...,  0.0515, -0.0225,  0.0349]],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.0.attn_2.out.bias', Parameter containing:\n",
      "tensor([ 1.4056e-02,  2.0874e-02,  3.3965e-03, -3.1730e-02,  2.9779e-02,\n",
      "        -4.3988e-02, -2.3156e-02,  1.4102e-02, -1.6498e-02, -1.9250e-02,\n",
      "        -4.2273e-02,  2.0919e-02,  1.6226e-02, -2.9263e-02, -2.3179e-02,\n",
      "        -4.3025e-02, -8.1737e-03, -1.5506e-02, -1.9767e-02, -5.0250e-04,\n",
      "         1.1700e-02,  1.9448e-02,  3.8432e-02, -2.0513e-02, -1.0191e-02,\n",
      "        -1.6044e-02,  7.6916e-03, -2.1946e-02,  4.6894e-03, -3.6405e-02,\n",
      "         1.2010e-02, -2.6045e-02, -4.2237e-02,  3.0476e-02, -8.5821e-03,\n",
      "        -1.1207e-02, -3.1158e-02,  2.4001e-02,  2.2269e-02,  6.4540e-03,\n",
      "         3.0533e-02,  3.0177e-02, -2.8226e-02,  7.1540e-03, -4.0130e-02,\n",
      "         4.3901e-02, -9.5443e-03, -2.5154e-02,  4.3367e-02,  4.0514e-02,\n",
      "         4.1943e-02, -1.6267e-02,  4.1005e-02, -1.7704e-02,  5.8744e-03,\n",
      "        -3.3617e-02,  3.7693e-02, -1.1158e-03,  5.7159e-03,  2.1817e-02,\n",
      "        -2.8660e-03,  2.9846e-02,  5.6177e-03, -4.3425e-02,  8.1169e-03,\n",
      "         2.0446e-03, -1.8192e-02,  1.0095e-02, -2.8736e-02,  1.2707e-02,\n",
      "        -4.2721e-02,  3.0219e-02, -1.6707e-03, -3.6217e-02,  5.0435e-03,\n",
      "        -1.9322e-02,  3.1590e-02,  1.8986e-02, -7.4347e-03,  1.8413e-02,\n",
      "        -2.8042e-02,  1.3672e-02,  1.7254e-02, -6.0115e-03, -4.0697e-02,\n",
      "         2.7535e-02,  4.2856e-02,  4.1500e-03,  1.4013e-02,  1.8661e-02,\n",
      "        -3.5843e-02, -4.1678e-02, -3.6963e-02, -1.8803e-02,  2.1886e-02,\n",
      "         4.9858e-03, -2.2327e-02, -7.8904e-03,  3.0899e-02, -4.7389e-05,\n",
      "         1.4214e-02, -2.5455e-02, -2.0859e-02,  4.2878e-02, -4.3380e-04,\n",
      "        -1.5235e-02, -2.7738e-02,  3.7637e-02, -2.7990e-02, -2.9279e-02,\n",
      "        -2.5764e-02,  1.4985e-02,  3.5597e-02,  2.8387e-02,  7.6963e-03,\n",
      "        -2.3127e-03,  1.9161e-02, -5.7107e-03, -2.1427e-03, -4.5732e-03,\n",
      "         3.2229e-02,  1.1896e-02,  3.2455e-02, -1.8313e-02,  1.0591e-02,\n",
      "         2.1273e-02,  3.5979e-02, -6.5122e-03, -3.0913e-02,  1.7920e-02,\n",
      "        -1.6519e-02,  1.5692e-02,  9.1088e-03, -3.2060e-02, -3.9517e-03,\n",
      "         1.6146e-02, -3.4412e-02,  5.6832e-03,  2.7198e-02,  1.7252e-02,\n",
      "         3.1542e-03, -7.5240e-03,  4.7315e-03, -4.1879e-02,  3.3532e-02,\n",
      "        -1.1804e-02, -1.9852e-02, -2.8333e-02,  1.7652e-02,  3.1110e-02,\n",
      "        -4.3096e-02, -1.9701e-02,  1.1734e-02, -1.2663e-02, -1.7450e-02,\n",
      "         4.5092e-03,  4.0296e-02,  2.4904e-03, -3.0449e-02, -4.3632e-02,\n",
      "        -1.4662e-02,  4.1325e-02, -3.9078e-02,  1.3720e-02, -2.5442e-02,\n",
      "         1.7487e-02, -1.3275e-02, -2.8786e-02, -1.1052e-02, -9.1655e-04,\n",
      "        -4.7027e-03, -8.1901e-03, -7.4845e-03, -4.3467e-03,  4.0239e-02,\n",
      "         3.2214e-02,  1.3521e-02, -3.2104e-02, -8.0880e-03,  3.8030e-02,\n",
      "         3.1686e-03, -2.8298e-02,  1.2721e-02,  1.4113e-02,  1.1211e-03,\n",
      "        -2.2352e-02, -3.0828e-02, -4.0330e-02, -8.5657e-03, -3.3473e-02,\n",
      "        -3.7113e-02,  3.6371e-02,  3.1534e-02,  3.3063e-02,  5.2473e-03,\n",
      "        -3.1464e-02, -3.6215e-02,  3.5801e-02, -9.5474e-03,  1.4800e-02,\n",
      "         3.2077e-02,  4.2881e-02,  4.1535e-02, -3.4903e-02,  3.9371e-03,\n",
      "        -3.9929e-02,  2.5904e-02,  5.6941e-03,  3.0712e-02, -3.6517e-02,\n",
      "        -2.2270e-02,  1.4478e-02, -2.3288e-02,  3.9204e-02,  2.3246e-02,\n",
      "        -8.7329e-03,  8.2090e-03, -3.3447e-03,  2.2749e-02, -4.3356e-02,\n",
      "         2.2720e-02, -3.9966e-02, -7.8652e-03, -2.1168e-02, -3.9023e-02,\n",
      "        -3.3049e-02, -4.2774e-02,  3.5911e-02, -5.2516e-03,  1.6130e-02,\n",
      "        -3.0408e-02,  4.4498e-03,  1.1175e-02,  1.8723e-02,  3.8899e-03,\n",
      "        -1.2170e-02, -1.8175e-02,  2.2498e-02,  2.6228e-02,  1.0036e-02,\n",
      "         2.2643e-03, -1.8865e-02,  1.6380e-02, -2.2472e-02,  3.2933e-03,\n",
      "         4.2167e-02, -8.3198e-03, -6.4596e-03,  1.4758e-02, -2.6311e-02,\n",
      "        -6.3776e-03, -1.1435e-02,  3.8632e-02,  2.9511e-02,  8.8953e-03,\n",
      "         4.3311e-02,  3.6489e-02,  7.8113e-03,  2.5850e-02,  9.8955e-03,\n",
      "         1.5586e-03,  4.2984e-02,  1.6323e-03, -2.5637e-02,  1.4014e-02,\n",
      "         3.6111e-02,  2.3891e-02, -2.9202e-02,  1.6570e-02, -2.7983e-02,\n",
      "        -2.0258e-02, -3.0795e-02,  1.8135e-02, -1.2053e-02,  5.2127e-03,\n",
      "        -2.8835e-02, -1.9080e-02, -2.4133e-02,  2.0777e-02,  8.4654e-03,\n",
      "         1.1894e-02,  7.2905e-04,  2.1443e-02, -8.1144e-03,  3.0683e-03,\n",
      "        -3.5241e-02,  2.5207e-02,  2.8300e-02,  3.0467e-02,  2.8313e-03,\n",
      "        -5.1923e-03,  4.3179e-02, -3.9128e-02, -3.3871e-02, -3.6201e-02,\n",
      "         4.3766e-02, -3.1378e-02,  2.8898e-02,  2.3408e-02,  1.3268e-02,\n",
      "        -2.2160e-02,  2.8718e-02,  3.5830e-02, -2.2655e-02,  4.1663e-02,\n",
      "        -2.5791e-02, -2.0651e-02, -1.8924e-03, -2.5536e-02,  3.6500e-02,\n",
      "        -1.7811e-02, -9.6746e-04, -2.0752e-02, -3.0177e-02,  3.2534e-02,\n",
      "        -3.3232e-02, -1.7719e-02, -2.1857e-02,  4.3932e-02,  2.9078e-02,\n",
      "         2.6213e-02, -1.4930e-02,  2.3781e-02,  3.6151e-02, -2.1943e-02,\n",
      "        -1.8310e-03, -2.5986e-02, -6.7993e-04,  4.2851e-02, -4.2403e-02,\n",
      "         4.3957e-02,  3.6505e-04, -4.2566e-02,  2.6133e-02,  6.5186e-03,\n",
      "        -1.9095e-02,  2.3232e-02,  7.6239e-03, -5.5927e-03, -1.9249e-02,\n",
      "         1.0966e-02, -4.4987e-03, -2.3148e-02, -3.3766e-02,  2.0819e-02,\n",
      "         1.6355e-02, -1.0197e-02,  2.1571e-02,  2.6334e-02, -6.8923e-03,\n",
      "         3.3215e-02, -4.0255e-03,  2.2764e-02, -2.1716e-02, -3.7606e-02,\n",
      "         9.6058e-03, -3.1546e-02,  3.0820e-02,  1.5526e-02, -1.5590e-02,\n",
      "         1.9551e-02,  1.0805e-02, -3.9117e-02,  3.2034e-02, -3.6719e-02,\n",
      "        -2.1708e-02, -3.0403e-02,  1.4945e-02, -7.1232e-03,  2.7699e-02,\n",
      "         3.8043e-02,  3.5670e-02, -1.2247e-02, -4.2287e-02, -5.3044e-03,\n",
      "        -6.8860e-03,  2.6200e-02,  3.0016e-02,  1.9633e-02, -3.8184e-02,\n",
      "        -3.4061e-02,  3.0144e-02,  8.7462e-03, -1.2371e-02,  2.6655e-02,\n",
      "        -1.4152e-02, -4.3891e-03,  1.9022e-02,  3.4370e-02, -3.1060e-02,\n",
      "        -1.3843e-02,  3.6473e-02,  3.9059e-02, -1.2045e-02, -1.0980e-02,\n",
      "        -9.9266e-03, -1.5521e-02,  1.3812e-02,  7.9203e-03,  2.4146e-02,\n",
      "         7.8065e-03, -3.4363e-02,  4.0921e-02, -2.3592e-02,  3.3116e-02,\n",
      "         1.3875e-02,  4.6371e-03, -4.1601e-02,  1.6008e-02,  3.9200e-03,\n",
      "        -2.6714e-02,  1.7089e-02,  1.2689e-02, -3.2948e-02,  5.9805e-04,\n",
      "        -1.5209e-02, -2.0659e-02,  2.3035e-02, -1.4886e-02, -1.7114e-02,\n",
      "        -8.7224e-03, -3.1439e-02, -2.2899e-03,  3.9228e-02, -2.0310e-02,\n",
      "         3.4840e-02,  1.7478e-02, -5.6636e-03,  2.1206e-02,  7.6029e-03,\n",
      "         2.6843e-02, -9.9170e-03,  3.2315e-02,  1.0239e-02,  1.2266e-02,\n",
      "        -4.3607e-02, -1.5945e-03, -4.1460e-02,  2.2292e-02,  7.9599e-03,\n",
      "         3.7253e-02,  1.7076e-02,  1.4066e-02, -2.8210e-02,  3.7140e-02,\n",
      "         3.8897e-02,  3.1388e-02, -3.7580e-02,  4.2634e-02, -2.7214e-02,\n",
      "         3.4931e-02, -2.5598e-02, -5.5835e-03,  8.3988e-03,  2.2569e-02,\n",
      "        -6.5762e-03,  1.2878e-02,  2.8412e-02, -2.1962e-02, -2.2196e-02,\n",
      "        -1.8291e-02, -3.7446e-02,  2.0958e-02,  3.4629e-02, -4.0189e-02,\n",
      "        -2.8465e-02, -6.7563e-03, -3.5228e-02, -1.3178e-02,  2.7376e-02,\n",
      "        -3.6505e-02,  9.6425e-03, -3.8505e-02, -3.5175e-02,  2.4472e-02,\n",
      "        -2.9303e-04, -2.4456e-02, -1.9947e-03, -3.5710e-02, -3.6998e-02,\n",
      "         1.0814e-03, -1.4302e-02, -8.1771e-03,  2.3509e-02, -4.4086e-02,\n",
      "        -6.0948e-03,  2.0816e-02,  3.4353e-02, -1.4344e-02,  4.3040e-02,\n",
      "         4.3445e-02,  1.1666e-02,  1.1393e-02,  1.5811e-02,  2.2940e-02,\n",
      "        -4.0904e-02, -9.8304e-04,  3.6053e-02, -1.3012e-02, -2.2612e-02,\n",
      "        -8.2351e-03, -2.1675e-02, -3.4542e-02, -4.8833e-04,  5.7909e-03,\n",
      "         1.2883e-02,  2.3061e-02, -2.6718e-02, -4.1337e-02,  1.9644e-02,\n",
      "        -2.9915e-02, -2.8460e-02], requires_grad=True))\n",
      "('decoder.layers.0.ff.linear_1.weight', Parameter containing:\n",
      "tensor([[-0.0361, -0.0255,  0.0217,  ...,  0.0244, -0.0360,  0.0424],\n",
      "        [ 0.0160, -0.0219, -0.0308,  ..., -0.0441, -0.0338, -0.0006],\n",
      "        [-0.0003,  0.0285,  0.0185,  ..., -0.0387,  0.0384,  0.0107],\n",
      "        ...,\n",
      "        [ 0.0176, -0.0275, -0.0339,  ..., -0.0121, -0.0365,  0.0299],\n",
      "        [ 0.0329, -0.0133, -0.0176,  ..., -0.0337, -0.0380,  0.0362],\n",
      "        [-0.0273,  0.0441,  0.0178,  ...,  0.0349,  0.0153, -0.0325]],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.0.ff.linear_1.bias', Parameter containing:\n",
      "tensor([ 0.0384, -0.0290, -0.0322,  ...,  0.0400, -0.0094, -0.0251],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.0.ff.linear_2.weight', Parameter containing:\n",
      "tensor([[-0.0459,  0.0221,  0.0140,  ..., -0.0003, -0.0314,  0.0255],\n",
      "        [ 0.0332,  0.0440, -0.0382,  ...,  0.0112, -0.0060, -0.0058],\n",
      "        [ 0.0375, -0.0164,  0.0443,  ...,  0.0108, -0.0177,  0.0460],\n",
      "        ...,\n",
      "        [-0.0097,  0.0226,  0.0086,  ..., -0.0232,  0.0433,  0.0288],\n",
      "        [-0.0302,  0.0279, -0.0388,  ...,  0.0261, -0.0042, -0.0196],\n",
      "        [ 0.0442,  0.0379, -0.0072,  ...,  0.0086, -0.0081,  0.0352]],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.0.ff.linear_2.bias', Parameter containing:\n",
      "tensor([-4.0199e-03,  4.9445e-04, -3.9172e-03,  2.4727e-03, -7.2851e-03,\n",
      "         3.2565e-03,  1.3141e-02, -1.2643e-02,  1.8128e-02, -7.0269e-03,\n",
      "         4.2464e-04,  3.9034e-03,  1.6957e-02, -1.5771e-02, -1.0167e-02,\n",
      "         1.1240e-02,  1.7235e-02, -2.0048e-02, -6.9507e-03, -1.4854e-02,\n",
      "        -1.8692e-02,  1.8861e-02,  1.4123e-02,  1.5159e-02, -1.6544e-02,\n",
      "         5.7571e-03,  8.4974e-03, -1.5382e-02,  1.1530e-02, -1.0514e-02,\n",
      "        -1.7382e-02, -2.1732e-02,  9.8501e-03,  1.9825e-02,  2.0286e-02,\n",
      "        -9.1625e-03,  8.0442e-03,  1.4489e-02,  6.5699e-03,  1.5260e-03,\n",
      "         1.4166e-02, -8.5941e-03,  2.9449e-03,  3.5485e-03,  8.1282e-03,\n",
      "        -6.4223e-03,  1.1488e-02,  6.8673e-03,  1.5951e-02, -1.4181e-02,\n",
      "         8.1290e-03, -2.0260e-02,  1.3747e-02, -1.6542e-02, -2.2091e-02,\n",
      "         4.8278e-03, -6.5973e-03, -3.2997e-03, -5.8900e-03,  3.5182e-03,\n",
      "         2.1547e-03, -2.0211e-02, -1.2798e-02, -2.7538e-03, -1.1926e-02,\n",
      "         1.4928e-02,  1.9336e-02,  2.6905e-03, -1.4914e-03, -3.0465e-03,\n",
      "        -6.6079e-03, -1.1373e-02, -1.3177e-02,  5.6175e-03, -2.8976e-03,\n",
      "         9.6631e-03, -1.2687e-02, -1.4496e-02,  1.0473e-02, -4.9142e-03,\n",
      "        -2.2024e-03,  1.0632e-02,  3.1562e-03, -1.2723e-02,  1.2751e-02,\n",
      "         1.0730e-02,  5.9565e-03,  1.9626e-02,  2.2094e-02,  1.0407e-02,\n",
      "        -7.6022e-03, -2.7055e-05, -7.3672e-03,  1.6808e-02, -2.0677e-02,\n",
      "         4.7542e-04, -5.3157e-03, -1.1312e-02,  1.5082e-02,  1.7224e-02,\n",
      "        -5.2409e-03,  5.7623e-03,  1.6166e-03,  4.0550e-03,  8.9260e-03,\n",
      "         2.4608e-03,  1.4025e-02, -1.4796e-02, -1.4235e-02,  7.9003e-03,\n",
      "         1.5542e-02, -1.9148e-02,  1.9081e-02, -2.1613e-02,  2.7593e-03,\n",
      "        -5.6320e-03,  2.0658e-02,  1.5888e-02,  1.9595e-02, -1.2082e-02,\n",
      "        -1.6910e-02, -2.0334e-02,  1.1168e-02, -8.6940e-04,  1.7961e-02,\n",
      "        -8.1677e-03,  1.3020e-02, -3.5910e-03, -9.2135e-03,  2.1471e-02,\n",
      "         1.9926e-02,  1.2801e-02,  1.4551e-02,  6.0618e-03, -1.0529e-02,\n",
      "         2.1950e-02, -2.0453e-02,  1.4784e-02, -1.4081e-02,  2.1564e-02,\n",
      "         3.5225e-03, -5.8149e-03, -1.6047e-02,  1.3413e-02, -1.1337e-02,\n",
      "         1.5108e-02, -4.4502e-03,  1.0052e-02,  1.7180e-02,  1.9942e-02,\n",
      "         4.8717e-03, -1.9814e-02, -6.8282e-03, -6.5165e-05, -5.5513e-03,\n",
      "         1.9182e-02, -9.3982e-03,  1.5634e-02, -1.6869e-02, -3.7324e-03,\n",
      "        -1.8274e-02, -5.4519e-03,  2.0650e-02, -1.7744e-02,  2.6689e-03,\n",
      "        -1.3575e-02, -5.5121e-03, -4.2633e-03,  9.3554e-03,  1.6953e-02,\n",
      "         1.8315e-02, -2.0903e-02, -4.6363e-03, -1.1099e-02, -6.8866e-03,\n",
      "        -5.4197e-03, -2.0986e-02, -2.6764e-03,  8.7599e-03,  5.2748e-03,\n",
      "         6.1005e-03, -6.1615e-03, -9.1340e-03,  8.4477e-03,  1.7603e-03,\n",
      "         1.0896e-02,  1.2022e-02, -8.3808e-03,  2.1454e-02,  1.7656e-02,\n",
      "         8.6594e-03,  1.3527e-03,  1.2885e-02,  6.8660e-03, -1.5086e-02,\n",
      "        -5.4202e-03, -1.8641e-02,  6.8052e-03,  9.4074e-03,  2.0030e-02,\n",
      "         5.5149e-03,  1.0534e-04,  7.6480e-03, -1.5204e-04, -7.9349e-07,\n",
      "        -2.0287e-02,  2.1385e-02, -1.9136e-02,  1.1639e-02,  9.7419e-03,\n",
      "        -4.4113e-03,  1.8748e-02, -1.4318e-02, -1.4980e-02, -1.2158e-03,\n",
      "         5.2835e-03, -1.4733e-02,  2.7585e-03, -2.0830e-02, -1.8405e-02,\n",
      "         1.5495e-03, -1.1267e-02,  2.3357e-03,  2.0864e-02,  2.8660e-03,\n",
      "        -1.0960e-03,  1.2216e-02, -1.8672e-02,  1.7592e-03,  1.2764e-02,\n",
      "         1.2132e-02, -7.1912e-03,  1.6800e-02,  1.3069e-02, -1.3148e-02,\n",
      "         1.4573e-02,  2.8715e-03,  2.0803e-02, -7.2615e-03, -1.0161e-02,\n",
      "         1.2753e-02, -2.0779e-02,  1.7249e-02, -1.4401e-02, -1.9130e-02,\n",
      "        -2.0988e-02,  1.5178e-02,  1.3086e-02, -7.8106e-03,  1.7013e-02,\n",
      "         3.7028e-03,  7.2102e-03, -5.2252e-04,  9.5734e-03,  1.1858e-02,\n",
      "         1.4477e-02,  1.6781e-02,  8.1645e-03, -1.0204e-02, -2.1199e-02,\n",
      "         1.3487e-02,  1.4331e-02, -1.9201e-02,  1.6801e-02, -5.1640e-03,\n",
      "        -1.7426e-03, -3.8452e-03, -1.2795e-02,  1.0413e-02,  1.3966e-02,\n",
      "         1.8683e-02,  1.6941e-03,  9.3503e-03,  1.6162e-02, -6.3178e-03,\n",
      "         4.4133e-03, -3.4203e-03,  1.0484e-02, -7.1017e-04,  9.4598e-03,\n",
      "        -2.0204e-02, -2.0107e-02,  2.5083e-03,  8.3851e-03, -1.9862e-02,\n",
      "         4.8527e-03,  1.4231e-02, -1.7422e-02, -2.0956e-02, -1.8594e-02,\n",
      "        -2.9168e-03, -3.4994e-03, -2.6037e-03, -1.2180e-02,  1.5206e-02,\n",
      "         9.9711e-03, -2.1857e-02, -2.4316e-03,  1.5755e-02,  2.5229e-03,\n",
      "        -1.4600e-03,  6.4640e-03, -9.9567e-03, -7.6713e-03,  4.6481e-03,\n",
      "        -1.3432e-02, -9.6215e-04, -1.7980e-02, -4.3536e-03, -2.1856e-02,\n",
      "         1.3498e-02,  2.1612e-02, -2.1612e-02, -1.5483e-03,  1.5359e-02,\n",
      "         7.9671e-03,  6.7616e-03,  1.6998e-02, -2.1767e-02, -1.0319e-02,\n",
      "         1.0907e-03, -8.5077e-03, -1.7858e-02,  1.4256e-02,  2.0443e-02,\n",
      "        -5.3807e-03,  1.2593e-02,  1.6884e-02, -5.8211e-03, -1.6715e-02,\n",
      "        -1.5428e-03, -1.1626e-02,  1.0790e-02, -1.2740e-02,  1.6344e-03,\n",
      "         3.6901e-03,  1.3699e-03,  9.7039e-03,  2.0418e-02, -2.0491e-02,\n",
      "         8.6580e-03,  1.3925e-02, -3.6534e-03,  1.4250e-02,  6.7095e-03,\n",
      "         1.7591e-02,  1.2725e-02, -1.7933e-02,  3.7724e-03, -2.1177e-03,\n",
      "        -1.3914e-02, -1.0259e-02, -1.4303e-02,  1.9562e-02,  1.0596e-02,\n",
      "         1.4452e-02, -1.6153e-02,  1.1114e-02,  1.4265e-02,  5.0792e-05,\n",
      "         7.9900e-03,  1.0635e-02,  1.9737e-02,  7.2163e-03,  1.6895e-02,\n",
      "        -9.0557e-03, -1.9822e-02, -2.1515e-02,  8.5566e-03, -1.6989e-02,\n",
      "         2.0656e-02,  6.2071e-03, -1.3287e-02,  1.0540e-02,  5.2316e-03,\n",
      "         4.4203e-03,  6.2810e-03,  1.9001e-03, -1.6010e-03,  9.3815e-03,\n",
      "        -2.2253e-03,  1.5361e-02, -1.7092e-02,  4.2335e-03,  5.8510e-04,\n",
      "        -1.1602e-03, -1.6037e-02, -2.5539e-03,  4.6269e-03,  1.6918e-02,\n",
      "         1.5227e-02, -3.7441e-03, -5.9720e-03, -1.9809e-02,  9.7906e-03,\n",
      "         2.1430e-02,  9.0645e-03,  1.3000e-02, -1.1856e-02,  1.0904e-03,\n",
      "         1.7781e-02, -9.5568e-03, -9.6942e-03, -8.2937e-03,  2.1392e-02,\n",
      "        -1.7484e-02, -1.6180e-02, -2.0065e-02, -1.8979e-02, -1.3277e-02,\n",
      "        -1.6161e-02,  1.9679e-02, -5.1667e-03, -4.4583e-03, -1.7812e-02,\n",
      "         1.8093e-02,  1.4854e-02, -1.7533e-02,  1.4009e-03, -2.1413e-02,\n",
      "         1.7183e-02, -2.0979e-02, -8.5615e-04, -5.5006e-04, -1.2955e-02,\n",
      "        -7.8758e-03,  1.9492e-02, -5.8536e-03, -1.6261e-02,  6.4225e-03,\n",
      "        -2.0637e-02, -2.0941e-02,  7.3173e-03, -2.1080e-02, -1.3876e-02,\n",
      "        -9.3030e-03, -4.8558e-03, -1.3596e-02, -9.7747e-03,  1.3873e-02,\n",
      "         3.8516e-03, -1.8572e-03,  1.1640e-02,  1.0089e-04, -5.8198e-03,\n",
      "        -1.2926e-02,  1.9318e-02,  1.0793e-02,  2.0721e-02,  3.0270e-03,\n",
      "        -2.2447e-03, -4.3719e-03,  1.9900e-02, -1.0645e-02, -2.1897e-02,\n",
      "        -4.3418e-03,  1.0624e-03,  1.8304e-02,  1.6515e-02,  2.0251e-02,\n",
      "         7.7765e-03, -6.3551e-03,  2.0200e-02, -1.3868e-03,  1.6538e-02,\n",
      "         1.3912e-02,  2.0116e-03,  2.0886e-02,  2.1243e-02,  1.4439e-02,\n",
      "        -1.3165e-02,  1.9357e-02, -5.8192e-03, -9.5289e-03,  1.2441e-02,\n",
      "         2.1788e-02,  5.9322e-03,  1.5761e-02,  1.7161e-02, -1.4129e-02,\n",
      "         1.0596e-02,  1.5234e-02,  2.1129e-02, -2.0502e-02, -5.1833e-03,\n",
      "        -1.0743e-02, -2.1228e-02,  9.5807e-04,  1.2438e-02, -1.5239e-02,\n",
      "        -1.9277e-02, -5.1763e-03, -9.0465e-03, -5.5660e-04, -1.9258e-02,\n",
      "         4.2584e-03,  5.2707e-03,  2.3938e-03, -1.4657e-02, -7.2563e-03,\n",
      "        -1.4100e-05, -5.2961e-04,  1.5059e-02, -3.4171e-03,  3.8046e-03,\n",
      "         1.2397e-02,  2.0989e-02, -1.1071e-02, -1.3993e-02, -1.5584e-02,\n",
      "        -6.4148e-03,  1.1393e-02], requires_grad=True))\n",
      "('decoder.layers.1.norm_1.alpha', Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True))\n",
      "('decoder.layers.1.norm_1.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))\n",
      "('decoder.layers.1.norm_2.alpha', Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True))\n",
      "('decoder.layers.1.norm_2.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))\n",
      "('decoder.layers.1.norm_3.alpha', Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True))\n",
      "('decoder.layers.1.norm_3.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))\n",
      "('decoder.layers.1.attn_1.q_linear.weight', Parameter containing:\n",
      "tensor([[-0.0038, -0.0743,  0.0153,  ...,  0.0228, -0.0742,  0.0715],\n",
      "        [-0.0078,  0.0049,  0.0134,  ..., -0.0441,  0.0217,  0.0160],\n",
      "        [-0.0159, -0.0308, -0.0286,  ...,  0.0732,  0.0146,  0.0546],\n",
      "        ...,\n",
      "        [-0.0529, -0.0711, -0.0567,  ..., -0.0648, -0.0004,  0.0245],\n",
      "        [-0.0239, -0.0358, -0.0463,  ..., -0.0151,  0.0758, -0.0591],\n",
      "        [-0.0693,  0.0472,  0.0394,  ..., -0.0025, -0.0741, -0.0311]],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.1.attn_1.q_linear.bias', Parameter containing:\n",
      "tensor([-0.0276,  0.0436,  0.0295,  0.0235, -0.0370,  0.0072,  0.0084, -0.0324,\n",
      "        -0.0252,  0.0325,  0.0047,  0.0264, -0.0405,  0.0010,  0.0099, -0.0214,\n",
      "        -0.0054, -0.0366,  0.0221, -0.0203,  0.0141,  0.0044,  0.0327,  0.0372,\n",
      "         0.0085,  0.0027,  0.0303,  0.0350,  0.0332,  0.0142, -0.0140, -0.0074,\n",
      "         0.0272, -0.0271, -0.0069, -0.0129, -0.0182,  0.0097,  0.0407, -0.0198,\n",
      "        -0.0211, -0.0036, -0.0144,  0.0120, -0.0138,  0.0012, -0.0058,  0.0219,\n",
      "         0.0019, -0.0426,  0.0185, -0.0203, -0.0411, -0.0155, -0.0117, -0.0226,\n",
      "         0.0418, -0.0416, -0.0154,  0.0296, -0.0137, -0.0051,  0.0070, -0.0067,\n",
      "         0.0420,  0.0420,  0.0169,  0.0436, -0.0073,  0.0019, -0.0138, -0.0121,\n",
      "        -0.0211,  0.0369, -0.0352,  0.0078,  0.0407, -0.0406, -0.0316, -0.0367,\n",
      "        -0.0066, -0.0313,  0.0232, -0.0430,  0.0110, -0.0093, -0.0326,  0.0022,\n",
      "        -0.0272, -0.0142,  0.0053,  0.0231,  0.0134,  0.0051, -0.0046,  0.0437,\n",
      "         0.0181,  0.0240,  0.0205,  0.0018,  0.0341, -0.0368, -0.0023,  0.0311,\n",
      "        -0.0309,  0.0185, -0.0408,  0.0178,  0.0146, -0.0324, -0.0143, -0.0334,\n",
      "         0.0364, -0.0359,  0.0146,  0.0099,  0.0288, -0.0123,  0.0059, -0.0401,\n",
      "        -0.0094,  0.0221, -0.0073, -0.0174,  0.0320, -0.0023,  0.0018, -0.0095,\n",
      "         0.0204, -0.0368,  0.0375, -0.0293,  0.0111, -0.0368,  0.0384, -0.0241,\n",
      "         0.0145,  0.0260, -0.0184,  0.0140,  0.0284,  0.0307, -0.0012,  0.0119,\n",
      "         0.0439, -0.0396,  0.0147,  0.0328, -0.0350, -0.0043, -0.0392,  0.0095,\n",
      "        -0.0350,  0.0227,  0.0110,  0.0405,  0.0159, -0.0055,  0.0039, -0.0020,\n",
      "         0.0137, -0.0108,  0.0425,  0.0213, -0.0098, -0.0394,  0.0107,  0.0127,\n",
      "         0.0329,  0.0143,  0.0059, -0.0315, -0.0416, -0.0360,  0.0237,  0.0098,\n",
      "         0.0014,  0.0049, -0.0378, -0.0280, -0.0012,  0.0169,  0.0114, -0.0368,\n",
      "        -0.0157,  0.0335,  0.0389,  0.0104,  0.0338, -0.0326,  0.0181,  0.0161,\n",
      "         0.0293, -0.0338,  0.0171, -0.0093, -0.0340,  0.0192, -0.0126,  0.0100,\n",
      "         0.0164,  0.0048, -0.0091, -0.0100,  0.0366,  0.0003,  0.0413, -0.0160,\n",
      "         0.0426, -0.0144, -0.0071,  0.0169, -0.0232, -0.0226, -0.0029, -0.0286,\n",
      "         0.0042,  0.0349,  0.0048, -0.0071,  0.0322, -0.0025,  0.0212, -0.0371,\n",
      "        -0.0038,  0.0126, -0.0155,  0.0393, -0.0123,  0.0306,  0.0407, -0.0027,\n",
      "        -0.0126, -0.0122,  0.0356,  0.0353,  0.0055,  0.0153,  0.0010, -0.0398,\n",
      "        -0.0250, -0.0033, -0.0405, -0.0038,  0.0342, -0.0080, -0.0425,  0.0005,\n",
      "         0.0383, -0.0077,  0.0361, -0.0167,  0.0303,  0.0404, -0.0111,  0.0042,\n",
      "        -0.0305, -0.0405,  0.0404,  0.0075, -0.0282, -0.0331, -0.0096, -0.0280,\n",
      "        -0.0175, -0.0057, -0.0430,  0.0292,  0.0437, -0.0220, -0.0414,  0.0188,\n",
      "        -0.0044, -0.0281,  0.0315, -0.0413,  0.0209,  0.0094,  0.0139,  0.0127,\n",
      "        -0.0014,  0.0328,  0.0376,  0.0103,  0.0263, -0.0012,  0.0277,  0.0347,\n",
      "         0.0254, -0.0159,  0.0133,  0.0321,  0.0255, -0.0349, -0.0026,  0.0028,\n",
      "         0.0154, -0.0411,  0.0371,  0.0245, -0.0257, -0.0149, -0.0365, -0.0389,\n",
      "        -0.0030, -0.0187,  0.0440,  0.0041, -0.0275, -0.0440, -0.0046, -0.0424,\n",
      "         0.0171,  0.0432,  0.0010,  0.0356,  0.0114, -0.0115,  0.0150, -0.0101,\n",
      "        -0.0393,  0.0010, -0.0008,  0.0362,  0.0002,  0.0202,  0.0098,  0.0080,\n",
      "         0.0196,  0.0422,  0.0223, -0.0156,  0.0334, -0.0141,  0.0206, -0.0068,\n",
      "         0.0122,  0.0417,  0.0308, -0.0251,  0.0423, -0.0099, -0.0351, -0.0101,\n",
      "         0.0427,  0.0213, -0.0087,  0.0345, -0.0061,  0.0251, -0.0377, -0.0346,\n",
      "         0.0225,  0.0420,  0.0322,  0.0147, -0.0139,  0.0120,  0.0371,  0.0171,\n",
      "        -0.0296,  0.0278,  0.0202,  0.0167, -0.0435,  0.0155, -0.0221,  0.0262,\n",
      "        -0.0361,  0.0414, -0.0138,  0.0324,  0.0004,  0.0419, -0.0284,  0.0169,\n",
      "         0.0394,  0.0161,  0.0099, -0.0438, -0.0027,  0.0355, -0.0268, -0.0213,\n",
      "         0.0257, -0.0017, -0.0268,  0.0017,  0.0381, -0.0186, -0.0326,  0.0149,\n",
      "        -0.0305,  0.0007, -0.0238,  0.0292, -0.0439, -0.0035, -0.0127, -0.0172,\n",
      "        -0.0055,  0.0381, -0.0251, -0.0078,  0.0394,  0.0234,  0.0350,  0.0234,\n",
      "        -0.0313,  0.0173,  0.0365, -0.0401,  0.0079,  0.0400, -0.0289,  0.0004,\n",
      "        -0.0440, -0.0439, -0.0363, -0.0355, -0.0282,  0.0086, -0.0297, -0.0259,\n",
      "         0.0084, -0.0219, -0.0086,  0.0014, -0.0124,  0.0073, -0.0135, -0.0192,\n",
      "        -0.0235,  0.0294,  0.0148,  0.0299,  0.0254,  0.0238, -0.0359,  0.0271,\n",
      "         0.0071,  0.0025,  0.0270,  0.0151, -0.0272, -0.0275,  0.0148, -0.0143,\n",
      "        -0.0327, -0.0064,  0.0029,  0.0088,  0.0409,  0.0239,  0.0299, -0.0146,\n",
      "        -0.0402, -0.0216,  0.0097, -0.0111,  0.0288, -0.0162, -0.0241,  0.0053,\n",
      "        -0.0172,  0.0256, -0.0249, -0.0373,  0.0352,  0.0013,  0.0420,  0.0406,\n",
      "        -0.0061, -0.0390, -0.0010, -0.0208, -0.0154, -0.0343, -0.0179, -0.0262,\n",
      "        -0.0025, -0.0055, -0.0401,  0.0167,  0.0110,  0.0375, -0.0055,  0.0267,\n",
      "         0.0321, -0.0082,  0.0220,  0.0284,  0.0196, -0.0344, -0.0198, -0.0133,\n",
      "         0.0084, -0.0247,  0.0225, -0.0338,  0.0082, -0.0287, -0.0271, -0.0025,\n",
      "         0.0163, -0.0053, -0.0163,  0.0104,  0.0345, -0.0272, -0.0388,  0.0023],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.1.attn_1.v_linear.weight', Parameter containing:\n",
      "tensor([[-3.0352e-02, -6.8160e-02, -6.4848e-02,  ..., -3.3828e-03,\n",
      "         -7.6662e-03,  7.3809e-02],\n",
      "        [-4.2884e-02, -7.0674e-02,  3.2063e-02,  ...,  1.9040e-02,\n",
      "          3.5660e-02,  6.2223e-02],\n",
      "        [-1.2990e-02, -2.0700e-02,  3.6346e-02,  ...,  2.0796e-02,\n",
      "         -4.5603e-02, -6.5351e-02],\n",
      "        ...,\n",
      "        [-1.2025e-02, -1.0029e-02,  4.7558e-02,  ..., -9.8526e-05,\n",
      "         -1.3084e-02,  4.9479e-02],\n",
      "        [-6.4098e-02,  1.8237e-02, -6.3699e-04,  ..., -6.0751e-02,\n",
      "          4.2977e-02, -7.0992e-03],\n",
      "        [-5.3874e-02, -6.0794e-02,  6.9813e-03,  ...,  8.4589e-03,\n",
      "         -3.0164e-04, -1.0027e-03]], requires_grad=True))\n",
      "('decoder.layers.1.attn_1.v_linear.bias', Parameter containing:\n",
      "tensor([ 7.6864e-03,  2.4679e-03,  1.7577e-02, -2.1017e-02,  4.3056e-02,\n",
      "         2.5087e-02,  2.5152e-02, -1.3269e-02, -2.3054e-02,  4.2896e-02,\n",
      "        -1.8359e-02, -3.8088e-03, -2.5711e-03,  3.4326e-03,  1.6270e-02,\n",
      "        -5.0351e-03,  7.0423e-03, -7.8722e-03, -2.4609e-02, -6.8483e-03,\n",
      "        -1.4784e-02, -2.3725e-02, -2.0732e-02, -1.1557e-02,  3.9233e-02,\n",
      "        -4.7620e-03,  1.9538e-02, -4.1420e-02, -3.5757e-02,  2.6071e-02,\n",
      "        -2.9061e-02, -2.2952e-02, -1.2534e-02,  1.8338e-03,  5.4436e-03,\n",
      "         4.3617e-02, -5.0393e-03, -4.2454e-02, -3.4783e-02,  2.5328e-02,\n",
      "        -3.3186e-02, -6.4352e-03,  3.5670e-02, -3.2353e-03, -2.2845e-02,\n",
      "        -2.1957e-02, -3.0922e-02, -8.1887e-04, -1.1087e-02, -2.5720e-02,\n",
      "         4.2626e-02,  1.6772e-02,  1.2278e-02, -3.1026e-02, -6.3714e-03,\n",
      "        -4.2191e-02, -2.6561e-02, -3.2580e-02, -2.9507e-02,  3.3328e-02,\n",
      "        -2.2422e-02,  3.0108e-03,  3.7260e-03, -2.1786e-02, -6.3530e-03,\n",
      "        -5.1625e-03, -2.9086e-02, -2.2336e-02,  2.1983e-02, -5.1379e-03,\n",
      "         9.9927e-03,  4.3463e-02,  2.6271e-02, -4.6421e-03,  3.3183e-02,\n",
      "        -1.8169e-02,  3.8871e-02,  5.5561e-03,  1.2095e-02,  2.8116e-02,\n",
      "         1.9631e-02,  9.5049e-03,  3.8501e-02,  3.2243e-03, -2.8107e-02,\n",
      "         1.4170e-02,  1.2076e-03, -6.9066e-03, -4.4671e-03,  1.7440e-02,\n",
      "        -1.4973e-02,  2.0242e-02, -3.3910e-02, -1.1958e-02,  2.4406e-02,\n",
      "        -2.6287e-02, -2.7726e-03, -4.1987e-02,  1.2548e-02,  2.5995e-04,\n",
      "        -3.3298e-03, -2.4417e-02, -9.9068e-03, -2.7871e-02,  4.3174e-02,\n",
      "         7.4704e-03, -2.3004e-02, -7.3931e-03, -3.1627e-02,  4.0111e-03,\n",
      "        -1.0932e-02, -1.5317e-02, -2.3558e-02,  2.0372e-02, -4.3263e-02,\n",
      "        -1.3874e-02, -7.6232e-03,  3.6927e-02,  2.2058e-02,  1.7281e-02,\n",
      "         6.1286e-03, -2.8702e-02,  3.3779e-02,  4.4113e-02, -3.1840e-02,\n",
      "        -2.5328e-02,  2.9199e-02, -4.1012e-02,  4.3370e-02,  3.7192e-02,\n",
      "         4.3811e-02, -4.3637e-02,  4.3250e-02, -2.5424e-02,  1.6440e-02,\n",
      "         9.2930e-03,  2.2407e-02, -2.0419e-02,  2.1262e-02, -1.3380e-02,\n",
      "         3.7415e-02, -1.5759e-02, -1.9629e-02, -2.0540e-03, -3.5512e-02,\n",
      "        -1.9919e-02, -2.4166e-02,  1.5658e-02, -1.7744e-02,  2.7294e-02,\n",
      "         1.5809e-02, -3.5911e-02,  4.7238e-03,  3.5321e-02,  1.0152e-03,\n",
      "         2.8667e-02, -1.9346e-02, -3.7296e-02, -2.8623e-02,  3.4509e-02,\n",
      "         4.0426e-03,  1.8133e-02, -2.9510e-02, -3.3644e-02, -3.7553e-03,\n",
      "         1.8069e-02,  4.9546e-03,  1.9215e-02, -7.0378e-03, -4.1967e-02,\n",
      "        -4.2173e-02, -4.2437e-02,  8.0441e-04, -2.0034e-02, -1.9509e-02,\n",
      "        -4.2573e-02,  1.1635e-02, -3.7061e-02, -1.0213e-02,  9.3755e-03,\n",
      "        -3.5422e-02,  3.9723e-02, -1.4544e-02,  2.8527e-02,  2.4253e-02,\n",
      "        -5.6765e-03, -2.3718e-02, -1.3849e-02, -2.1298e-02,  2.1138e-02,\n",
      "         3.2532e-02, -2.4023e-02, -2.5016e-02,  2.7481e-02, -4.9093e-03,\n",
      "         2.9614e-02,  2.0985e-02, -3.3770e-02, -2.6084e-02, -4.2735e-02,\n",
      "         3.6042e-02,  2.2895e-02, -3.7283e-02, -5.8359e-03, -2.2994e-02,\n",
      "         3.5516e-02, -1.3600e-02,  1.0795e-02,  1.5029e-03,  6.0760e-03,\n",
      "         1.1186e-02,  8.9973e-03,  3.8305e-02, -5.7095e-03,  4.1454e-02,\n",
      "         3.8781e-02,  9.5694e-03, -3.3597e-02,  7.9106e-03,  2.3924e-02,\n",
      "         2.9387e-02, -1.3296e-02, -3.6553e-02,  2.3753e-02,  3.0703e-02,\n",
      "        -4.7558e-03, -4.4063e-02,  2.6579e-02, -5.6162e-03, -4.3746e-02,\n",
      "        -2.7969e-02, -3.2275e-03,  3.9790e-02,  2.4368e-02, -4.3819e-03,\n",
      "        -2.5777e-02, -2.9868e-02,  8.0229e-03,  8.8035e-03,  4.2978e-02,\n",
      "        -3.6586e-02, -3.9564e-03, -2.0689e-02, -7.2058e-04, -3.6361e-02,\n",
      "         1.1594e-02, -3.4211e-02, -1.9375e-02, -1.4611e-04,  3.4583e-02,\n",
      "         4.0237e-02, -9.6533e-03, -3.8850e-02,  4.3171e-03, -4.8126e-03,\n",
      "        -3.3538e-03,  3.0677e-02,  3.5340e-02, -3.3441e-02, -3.9261e-02,\n",
      "         9.8424e-03,  6.9954e-03,  2.9180e-02, -4.2872e-02,  7.4332e-03,\n",
      "         3.0806e-02,  2.0422e-02, -4.1457e-02, -2.5810e-02,  3.1029e-02,\n",
      "        -1.6856e-02, -1.6828e-03,  2.6251e-02,  1.3370e-02, -2.1739e-02,\n",
      "        -3.8082e-02, -4.0787e-02, -1.0354e-02, -1.8052e-02, -2.9824e-02,\n",
      "         2.7035e-02, -3.5906e-02, -2.2287e-02,  4.0839e-02, -2.5380e-02,\n",
      "        -4.2769e-02, -2.2076e-02,  5.7518e-03, -2.5494e-02, -2.6937e-02,\n",
      "         2.9460e-02,  3.7382e-02,  2.7211e-02,  8.4235e-03,  4.1829e-02,\n",
      "        -1.0074e-02, -2.4750e-02,  4.0910e-02, -9.3436e-03, -9.0326e-04,\n",
      "        -2.2569e-02,  2.5162e-02, -1.2517e-02, -1.3608e-02,  3.1974e-02,\n",
      "        -3.2894e-02,  2.0397e-02, -3.5290e-02,  2.0211e-03,  4.3922e-02,\n",
      "        -6.3214e-03,  3.2906e-02, -6.4667e-03,  3.1611e-02,  1.1816e-02,\n",
      "         3.1025e-03,  3.5599e-02, -1.3822e-02, -2.4340e-02, -8.7297e-03,\n",
      "        -1.4515e-02,  1.0160e-02, -9.1227e-04,  1.5968e-02, -3.5499e-02,\n",
      "        -8.1833e-04, -1.0508e-02,  3.8320e-03,  2.7889e-02, -2.5089e-02,\n",
      "         4.1066e-02,  1.8702e-02,  2.0330e-02, -7.3188e-03, -1.8312e-02,\n",
      "        -2.3545e-02, -9.0726e-03,  3.5240e-02, -3.2386e-02, -1.0541e-02,\n",
      "        -3.6122e-02, -9.5048e-03, -9.0939e-03, -3.5848e-02,  6.4315e-03,\n",
      "        -3.6720e-02, -4.2751e-02, -3.8821e-02,  1.2677e-02, -1.5172e-02,\n",
      "        -2.4869e-02, -4.1458e-02, -2.5716e-02,  9.9117e-03,  2.1375e-02,\n",
      "         3.1968e-02,  3.9553e-02, -1.5306e-02,  1.3648e-02,  2.1402e-02,\n",
      "        -2.4251e-02,  4.1744e-02, -2.1289e-02, -1.7941e-02, -3.8260e-02,\n",
      "        -1.1595e-02, -9.7332e-03, -1.4733e-02, -1.2559e-04, -4.0175e-02,\n",
      "         9.5568e-03,  2.4508e-02,  2.6807e-02, -1.9743e-02, -2.0078e-02,\n",
      "         8.4869e-03,  2.5053e-02,  2.3384e-02,  2.3367e-02,  2.9641e-02,\n",
      "        -3.7998e-06,  3.0425e-02, -2.3571e-02, -3.7900e-02,  6.6910e-03,\n",
      "        -4.1606e-02,  3.2235e-02, -2.7864e-02, -7.7990e-03, -3.2460e-02,\n",
      "         3.6988e-02, -3.0715e-03, -8.2954e-03,  3.1960e-02,  2.8920e-02,\n",
      "        -3.8386e-02,  4.2273e-02,  1.9033e-02,  3.1788e-02, -5.9496e-03,\n",
      "        -2.1264e-02, -4.3721e-02,  9.5335e-03, -4.1359e-02, -2.5038e-02,\n",
      "        -2.7131e-02,  2.3856e-02, -3.8368e-02,  5.3909e-03,  8.7545e-03,\n",
      "         3.7906e-02, -5.0169e-03, -1.1483e-02, -6.9676e-03,  7.4611e-03,\n",
      "         1.0427e-02, -1.4777e-02,  1.7483e-02, -4.3019e-02,  9.5992e-03,\n",
      "         1.0736e-02,  3.2488e-03,  2.6026e-02, -1.1847e-02,  2.4927e-02,\n",
      "         3.3379e-02,  3.5342e-03,  3.5602e-02,  8.9690e-03, -8.8486e-03,\n",
      "        -9.2954e-03, -3.9771e-02,  5.9516e-03, -2.6953e-02, -5.6426e-03,\n",
      "         2.7503e-02,  2.1842e-02,  1.3325e-02,  1.5181e-02, -6.0122e-04,\n",
      "         6.6233e-03, -1.6806e-02,  3.8975e-02,  1.0709e-04,  1.2905e-03,\n",
      "         3.7200e-02, -2.4513e-02,  1.3978e-02,  2.6283e-02, -3.5812e-03,\n",
      "         8.4892e-03, -1.9721e-02,  5.4475e-03,  2.1095e-02,  2.4723e-02,\n",
      "        -3.3199e-02,  1.4466e-02,  9.3854e-03, -1.1666e-02,  3.8961e-02,\n",
      "        -1.8548e-02,  2.8724e-02,  1.1248e-02, -2.9469e-02, -1.9309e-02,\n",
      "         2.2063e-02,  2.3108e-02,  2.1134e-03, -1.4522e-02,  2.4810e-02,\n",
      "        -1.6932e-02,  4.3090e-02, -3.2469e-02,  3.1950e-02, -1.6690e-03,\n",
      "         6.5339e-03,  5.8431e-04,  2.2885e-02, -2.7789e-02, -3.2920e-02,\n",
      "        -4.2368e-02,  2.6121e-02, -1.7672e-02,  4.1836e-03,  4.3750e-02,\n",
      "         3.1855e-02,  2.8028e-02, -2.0339e-02, -2.2816e-02, -3.6613e-02,\n",
      "         6.8200e-03, -2.7368e-03, -8.6415e-03, -1.7536e-02, -3.3693e-02,\n",
      "        -3.4396e-03,  3.6978e-02, -4.1452e-02, -9.9195e-03,  3.4647e-02,\n",
      "         4.0231e-02,  4.3903e-03,  2.2304e-02, -1.8560e-02,  1.2204e-02,\n",
      "        -2.0922e-02, -2.7063e-02,  4.2584e-02,  2.9390e-02,  2.2267e-02,\n",
      "         1.4912e-02,  3.9035e-02], requires_grad=True))\n",
      "('decoder.layers.1.attn_1.k_linear.weight', Parameter containing:\n",
      "tensor([[ 0.0283,  0.0370, -0.0430,  ...,  0.0753, -0.0018,  0.0153],\n",
      "        [-0.0106, -0.0047,  0.0481,  ..., -0.0681,  0.0368,  0.0563],\n",
      "        [ 0.0038, -0.0005,  0.0202,  ...,  0.0297,  0.0301, -0.0093],\n",
      "        ...,\n",
      "        [-0.0010, -0.0220,  0.0710,  ...,  0.0584,  0.0295, -0.0546],\n",
      "        [ 0.0322, -0.0040, -0.0414,  ...,  0.0092, -0.0637, -0.0600],\n",
      "        [-0.0119,  0.0304, -0.0609,  ...,  0.0524,  0.0320, -0.0243]],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.1.attn_1.k_linear.bias', Parameter containing:\n",
      "tensor([-0.0106, -0.0335,  0.0361, -0.0170, -0.0066, -0.0016, -0.0365, -0.0389,\n",
      "        -0.0289, -0.0347,  0.0113, -0.0271,  0.0025,  0.0358,  0.0381,  0.0101,\n",
      "         0.0038, -0.0240, -0.0029,  0.0087,  0.0196,  0.0338,  0.0089,  0.0130,\n",
      "         0.0142, -0.0073,  0.0376, -0.0087,  0.0163, -0.0381,  0.0291, -0.0275,\n",
      "         0.0371,  0.0211, -0.0042, -0.0271,  0.0100, -0.0377, -0.0399, -0.0333,\n",
      "         0.0249, -0.0107, -0.0251,  0.0275, -0.0256,  0.0360, -0.0105, -0.0288,\n",
      "        -0.0225, -0.0088, -0.0071,  0.0354,  0.0351,  0.0034, -0.0155,  0.0168,\n",
      "        -0.0288,  0.0421,  0.0419, -0.0052,  0.0226,  0.0222,  0.0329,  0.0294,\n",
      "        -0.0060, -0.0381, -0.0337,  0.0318, -0.0376,  0.0397, -0.0047,  0.0247,\n",
      "         0.0131, -0.0132,  0.0201, -0.0308,  0.0122,  0.0259, -0.0183, -0.0278,\n",
      "        -0.0183, -0.0208,  0.0126, -0.0234,  0.0389, -0.0320, -0.0298, -0.0400,\n",
      "        -0.0268, -0.0394, -0.0353,  0.0012,  0.0113, -0.0346, -0.0093, -0.0315,\n",
      "        -0.0382, -0.0242,  0.0350, -0.0224,  0.0119,  0.0356,  0.0254, -0.0123,\n",
      "         0.0144,  0.0186,  0.0019,  0.0333, -0.0358, -0.0411, -0.0162, -0.0421,\n",
      "         0.0311, -0.0009,  0.0175,  0.0255,  0.0101,  0.0323, -0.0043, -0.0059,\n",
      "        -0.0003, -0.0315,  0.0315,  0.0221,  0.0105, -0.0046,  0.0033,  0.0040,\n",
      "         0.0288, -0.0441,  0.0216, -0.0250,  0.0008,  0.0028, -0.0216, -0.0184,\n",
      "        -0.0130, -0.0415, -0.0172, -0.0122, -0.0236,  0.0163,  0.0144, -0.0416,\n",
      "        -0.0310, -0.0388, -0.0415, -0.0307,  0.0368, -0.0357,  0.0338, -0.0296,\n",
      "         0.0331, -0.0310,  0.0280,  0.0112, -0.0080, -0.0189, -0.0023, -0.0313,\n",
      "        -0.0438, -0.0334,  0.0385,  0.0351,  0.0234, -0.0018,  0.0188,  0.0188,\n",
      "         0.0002, -0.0117, -0.0032,  0.0184,  0.0018, -0.0191, -0.0337, -0.0377,\n",
      "        -0.0422,  0.0151, -0.0302,  0.0253,  0.0004, -0.0193,  0.0080, -0.0326,\n",
      "        -0.0375, -0.0235, -0.0411, -0.0289,  0.0068,  0.0404,  0.0149,  0.0261,\n",
      "         0.0356, -0.0306,  0.0317,  0.0068, -0.0176,  0.0063,  0.0100, -0.0392,\n",
      "         0.0343, -0.0093,  0.0356, -0.0376,  0.0308,  0.0158,  0.0380, -0.0320,\n",
      "        -0.0278, -0.0420,  0.0036,  0.0440,  0.0040,  0.0165, -0.0044,  0.0368,\n",
      "        -0.0089,  0.0390,  0.0036,  0.0280,  0.0340, -0.0289,  0.0016,  0.0334,\n",
      "        -0.0133,  0.0136, -0.0049, -0.0344, -0.0346,  0.0035,  0.0400,  0.0378,\n",
      "        -0.0181, -0.0354, -0.0272,  0.0095, -0.0064,  0.0070, -0.0102, -0.0109,\n",
      "         0.0027,  0.0350, -0.0025, -0.0220,  0.0312,  0.0015,  0.0232,  0.0215,\n",
      "         0.0199,  0.0352, -0.0232, -0.0400,  0.0398,  0.0097,  0.0295, -0.0183,\n",
      "         0.0230, -0.0341, -0.0410, -0.0365,  0.0323,  0.0345, -0.0397,  0.0200,\n",
      "        -0.0288, -0.0435, -0.0181,  0.0342,  0.0338,  0.0294,  0.0309, -0.0390,\n",
      "         0.0239,  0.0176, -0.0059, -0.0438, -0.0173,  0.0420,  0.0203, -0.0034,\n",
      "        -0.0412, -0.0316, -0.0409, -0.0244,  0.0155, -0.0290, -0.0056, -0.0322,\n",
      "        -0.0071, -0.0032, -0.0298, -0.0376, -0.0048, -0.0021, -0.0090,  0.0170,\n",
      "        -0.0086, -0.0063,  0.0212, -0.0059, -0.0198, -0.0279,  0.0120, -0.0379,\n",
      "         0.0113, -0.0298, -0.0200,  0.0189,  0.0288,  0.0297, -0.0274, -0.0200,\n",
      "        -0.0202, -0.0425, -0.0133,  0.0436, -0.0082,  0.0060,  0.0199,  0.0285,\n",
      "         0.0117, -0.0292, -0.0017,  0.0412,  0.0408,  0.0343, -0.0023,  0.0362,\n",
      "         0.0213, -0.0200, -0.0028,  0.0032,  0.0284, -0.0196,  0.0063,  0.0356,\n",
      "         0.0347,  0.0236, -0.0292,  0.0268, -0.0233, -0.0329, -0.0124,  0.0345,\n",
      "        -0.0263,  0.0203,  0.0159,  0.0356, -0.0315,  0.0440, -0.0372, -0.0327,\n",
      "         0.0412, -0.0414,  0.0135,  0.0005,  0.0354, -0.0135, -0.0297, -0.0120,\n",
      "        -0.0188,  0.0244,  0.0260,  0.0177,  0.0367, -0.0204,  0.0347,  0.0134,\n",
      "         0.0293, -0.0058, -0.0242,  0.0405,  0.0093,  0.0309, -0.0434,  0.0199,\n",
      "         0.0279,  0.0082,  0.0433,  0.0151,  0.0154, -0.0422,  0.0181, -0.0336,\n",
      "        -0.0236,  0.0256, -0.0290,  0.0427,  0.0264, -0.0203, -0.0127,  0.0239,\n",
      "        -0.0370,  0.0367,  0.0263,  0.0252, -0.0080,  0.0133, -0.0096, -0.0085,\n",
      "         0.0283,  0.0132,  0.0340,  0.0440, -0.0077,  0.0390,  0.0316,  0.0022,\n",
      "         0.0093,  0.0066,  0.0200, -0.0258,  0.0392,  0.0189,  0.0426,  0.0011,\n",
      "        -0.0426,  0.0130, -0.0287, -0.0218,  0.0044, -0.0107,  0.0315,  0.0135,\n",
      "         0.0240,  0.0076, -0.0101,  0.0189,  0.0171,  0.0063,  0.0174, -0.0287,\n",
      "        -0.0331, -0.0294,  0.0361,  0.0383,  0.0027,  0.0276, -0.0157, -0.0192,\n",
      "        -0.0153, -0.0425, -0.0302, -0.0421, -0.0063,  0.0427, -0.0039,  0.0009,\n",
      "         0.0258,  0.0251,  0.0339, -0.0301, -0.0440, -0.0244,  0.0070, -0.0001,\n",
      "         0.0375, -0.0245, -0.0132, -0.0201, -0.0248, -0.0222, -0.0248,  0.0368,\n",
      "        -0.0322,  0.0259, -0.0272, -0.0358, -0.0029, -0.0027, -0.0274,  0.0419,\n",
      "        -0.0327,  0.0287, -0.0379, -0.0022, -0.0088, -0.0280,  0.0206, -0.0283,\n",
      "        -0.0256, -0.0147, -0.0058, -0.0157, -0.0266,  0.0247, -0.0384,  0.0224,\n",
      "        -0.0074, -0.0054, -0.0053, -0.0251,  0.0376, -0.0071,  0.0080, -0.0431,\n",
      "         0.0377, -0.0122, -0.0051,  0.0128, -0.0138, -0.0171,  0.0335, -0.0229,\n",
      "         0.0117, -0.0129, -0.0343, -0.0126,  0.0279,  0.0235, -0.0105,  0.0089],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.1.attn_1.out.weight', Parameter containing:\n",
      "tensor([[ 7.3373e-02, -7.3032e-02, -6.5134e-04,  ...,  1.4217e-02,\n",
      "         -1.4250e-02,  5.7126e-02],\n",
      "        [-7.3077e-02,  6.9313e-02,  6.2441e-02,  ...,  6.7285e-02,\n",
      "          3.7114e-02, -6.5947e-02],\n",
      "        [-1.7222e-02,  3.0668e-02, -6.3895e-03,  ..., -7.0422e-02,\n",
      "          1.5575e-02, -4.1821e-02],\n",
      "        ...,\n",
      "        [ 6.4366e-02, -1.1293e-02,  3.1948e-02,  ...,  4.6952e-02,\n",
      "          3.2375e-02, -7.0384e-02],\n",
      "        [-6.9531e-02,  3.9757e-02,  1.1803e-02,  ...,  3.2619e-05,\n",
      "         -5.1291e-02,  9.9221e-03],\n",
      "        [ 5.5679e-02, -1.2341e-02, -6.0461e-02,  ..., -2.8701e-02,\n",
      "          2.0715e-02, -4.8438e-02]], requires_grad=True))\n",
      "('decoder.layers.1.attn_1.out.bias', Parameter containing:\n",
      "tensor([ 3.7839e-02, -3.7018e-03,  1.2705e-02,  5.7409e-03,  1.0879e-02,\n",
      "        -3.7578e-02,  3.9128e-02, -1.5581e-02,  1.2646e-02, -3.0819e-02,\n",
      "        -1.2883e-02, -1.2668e-02,  4.2512e-02,  2.1784e-02, -2.8842e-02,\n",
      "         1.7080e-02,  3.1626e-03,  2.6219e-02, -1.1322e-02, -1.2491e-02,\n",
      "        -2.8419e-02, -5.8973e-03,  3.9772e-02, -2.1022e-02, -1.4746e-02,\n",
      "         3.4695e-02, -5.5768e-03,  3.9291e-02, -4.3122e-02, -2.0004e-02,\n",
      "        -9.5473e-03, -2.4289e-02,  2.1035e-02,  3.6293e-02,  1.6386e-02,\n",
      "        -1.2124e-02,  2.6076e-02,  3.4190e-02,  1.2846e-02, -2.0536e-02,\n",
      "         1.5089e-03, -9.9265e-03, -1.9603e-02, -4.4001e-02, -2.3968e-02,\n",
      "         1.0120e-03, -4.0771e-02,  3.7776e-02,  1.0604e-02, -9.5416e-03,\n",
      "         2.5210e-02, -3.8277e-02, -1.0707e-02, -1.9833e-02, -4.2454e-02,\n",
      "         3.3660e-03, -1.0690e-02, -7.6141e-03, -1.3979e-02,  3.6956e-02,\n",
      "        -1.9594e-02,  3.0645e-02, -2.7786e-03, -2.5827e-02, -3.6579e-03,\n",
      "         3.1388e-03, -5.9267e-03,  2.2641e-02, -2.8526e-02,  2.0915e-02,\n",
      "         6.2763e-03,  3.9003e-03, -1.9435e-02,  1.5325e-03,  2.9935e-02,\n",
      "        -1.2033e-02,  2.4308e-02,  2.1456e-02, -2.1186e-02,  2.8821e-02,\n",
      "         1.7941e-02, -1.0436e-02,  2.5927e-02,  2.0454e-02, -1.0062e-03,\n",
      "         3.0160e-02,  3.6351e-02, -4.1463e-02, -1.9028e-02, -2.9625e-02,\n",
      "        -5.3240e-03,  2.7899e-02,  2.8308e-05, -4.0096e-02,  1.5164e-02,\n",
      "        -3.4037e-02,  1.6122e-02, -1.4024e-02, -1.7787e-02, -1.3052e-03,\n",
      "         1.2358e-02,  3.7147e-02,  3.6136e-02, -6.3820e-03,  3.5401e-02,\n",
      "        -3.3256e-02,  3.1483e-02, -8.5437e-04, -7.1094e-04, -1.1096e-02,\n",
      "         1.0526e-02, -9.2313e-03, -1.3451e-02, -2.6074e-02, -8.3067e-05,\n",
      "        -1.6913e-02, -1.8711e-02, -4.3713e-02, -4.1343e-02, -1.8421e-02,\n",
      "         2.3954e-02,  2.0211e-02,  2.8202e-02, -1.7105e-02, -2.5487e-02,\n",
      "        -9.9014e-03, -3.8806e-02, -7.6004e-03,  2.0969e-02,  3.6931e-02,\n",
      "        -1.4410e-03, -4.0333e-02, -4.2124e-02,  1.4265e-02, -9.0077e-03,\n",
      "         1.1511e-02, -2.4008e-02,  2.0176e-02,  1.7666e-02,  1.9711e-02,\n",
      "        -4.8446e-03, -1.5420e-02, -3.2098e-02,  2.0961e-02,  9.8023e-03,\n",
      "        -2.4037e-03, -8.0856e-03,  2.1361e-02, -2.7029e-02, -5.2154e-03,\n",
      "         4.0330e-02, -2.0579e-02,  1.0789e-02, -4.2650e-02, -2.0052e-02,\n",
      "        -3.6944e-03,  2.9029e-02,  2.6934e-02,  4.0697e-02,  2.0187e-02,\n",
      "        -1.8225e-02,  1.7928e-02, -1.5072e-02,  1.5929e-02,  4.2488e-04,\n",
      "         2.0543e-02,  4.4449e-03,  6.0168e-03,  1.6295e-02, -4.0755e-02,\n",
      "         2.8328e-02,  3.2426e-02, -1.8177e-02, -8.6282e-03, -1.3372e-02,\n",
      "        -2.3340e-02,  2.2558e-02,  3.0629e-02, -3.4108e-02, -3.6623e-02,\n",
      "         3.1847e-02,  3.6366e-03,  2.1876e-02,  2.5188e-02,  3.2415e-02,\n",
      "        -1.2889e-02, -3.0134e-02, -2.4812e-02,  3.3797e-02, -1.9183e-02,\n",
      "         2.7279e-03,  2.3212e-02,  3.0049e-02,  1.9188e-03,  1.8296e-02,\n",
      "         1.4108e-03, -1.8820e-02,  3.1205e-02,  2.5982e-02,  3.4831e-02,\n",
      "         2.1161e-02,  8.2867e-03, -4.3613e-02, -2.9696e-02,  1.6419e-02,\n",
      "        -3.2532e-02,  3.1099e-02,  3.6391e-03, -1.9705e-02, -2.1379e-02,\n",
      "        -3.5113e-02,  4.1587e-02,  2.7961e-02, -3.0152e-02, -3.9706e-02,\n",
      "        -2.4921e-02, -3.7570e-02,  3.0100e-02,  4.4029e-02,  4.1892e-02,\n",
      "        -1.9617e-02,  2.3617e-02,  3.9070e-02,  3.4635e-03, -2.1041e-02,\n",
      "         3.5393e-02, -2.6217e-02, -1.4884e-04, -1.8412e-02, -4.1005e-02,\n",
      "        -1.7689e-03,  3.3527e-02,  3.6411e-02,  2.8690e-02, -1.8816e-02,\n",
      "         1.5714e-02,  4.0934e-02,  2.8881e-02, -2.7246e-03,  3.3878e-02,\n",
      "        -4.2989e-02,  6.2526e-03, -3.3316e-02, -3.8292e-02,  2.2879e-02,\n",
      "        -1.5376e-03, -1.8618e-02,  3.5188e-02,  1.2192e-02,  2.9916e-02,\n",
      "        -1.2578e-02, -4.2723e-02,  2.0337e-02,  7.7306e-03, -1.8370e-02,\n",
      "         3.7336e-03, -1.0290e-02, -2.7841e-02,  9.0284e-03, -3.1104e-02,\n",
      "         3.2299e-02, -2.4336e-02,  2.9220e-02,  9.0481e-03,  3.5969e-02,\n",
      "        -2.9724e-02,  3.3838e-02, -3.6708e-02, -4.0903e-02, -3.4185e-02,\n",
      "         1.8821e-02,  2.3787e-02,  1.9415e-02, -3.6539e-02, -1.9024e-02,\n",
      "         3.7109e-02,  1.4463e-02, -3.0274e-02,  4.1409e-02, -4.0898e-03,\n",
      "        -3.3516e-02, -4.6036e-03,  8.7133e-03,  2.3464e-02,  1.9751e-02,\n",
      "        -5.9064e-03, -4.2505e-03,  9.7342e-03,  2.5219e-02,  1.3065e-02,\n",
      "        -9.2196e-03,  1.3164e-02, -4.0987e-02,  4.0561e-02,  3.5339e-02,\n",
      "         1.1895e-02, -2.4634e-02, -2.6308e-02,  5.9312e-03,  2.8902e-02,\n",
      "        -1.9868e-02,  6.2480e-03, -2.0132e-02, -4.2359e-02, -1.1985e-03,\n",
      "         2.4080e-02,  6.4712e-04,  3.5425e-02, -1.5040e-02,  1.9086e-02,\n",
      "        -3.6614e-02,  1.7846e-02,  3.1832e-02,  2.9820e-02,  3.2121e-02,\n",
      "        -2.1304e-02,  5.7210e-03,  2.7981e-03,  2.4141e-02, -4.0983e-02,\n",
      "         1.6688e-02,  1.6418e-03, -7.6768e-04, -4.1464e-02, -2.8838e-02,\n",
      "         1.8139e-02,  1.4434e-02, -2.8173e-02, -1.1949e-02, -4.4710e-03,\n",
      "        -2.0787e-02, -3.0215e-02, -3.0013e-02, -9.7611e-03,  3.8725e-02,\n",
      "        -1.3086e-02,  3.5651e-02,  1.8903e-02, -2.6065e-02,  2.2763e-02,\n",
      "        -2.5496e-02, -2.6008e-02,  1.5823e-02, -1.3017e-02,  4.3910e-02,\n",
      "         3.8262e-02, -3.2322e-02, -8.0930e-03,  1.2753e-02, -4.8396e-03,\n",
      "         1.7272e-02,  2.5733e-02, -6.5666e-03,  2.5794e-02, -3.6283e-02,\n",
      "         2.6007e-02,  4.4026e-02,  2.4074e-02,  6.1610e-03,  2.9763e-02,\n",
      "         4.2216e-02, -1.8794e-02, -3.5323e-02, -1.5159e-02,  2.6060e-02,\n",
      "        -1.8236e-02, -1.3672e-02, -2.9587e-02, -2.1956e-02, -1.5584e-02,\n",
      "        -1.0752e-02, -2.3050e-02,  3.1572e-02, -3.4120e-02, -4.2814e-02,\n",
      "         9.7658e-03,  3.2533e-02, -4.4163e-02, -3.3160e-02, -3.7977e-02,\n",
      "        -3.3005e-03,  3.6559e-02,  3.1347e-02,  3.9940e-03,  6.8300e-03,\n",
      "        -2.4128e-02, -4.1159e-02,  3.3116e-02, -3.6677e-02, -1.2722e-03,\n",
      "        -3.8886e-02,  3.2476e-02,  3.8552e-02,  7.4263e-03, -1.1465e-02,\n",
      "         1.7092e-02, -1.5960e-02,  2.8859e-02, -4.0647e-02,  1.8302e-02,\n",
      "         5.9262e-03,  2.5151e-02,  6.4211e-03,  1.5626e-02,  4.2561e-02,\n",
      "         1.1608e-02, -2.6409e-02,  2.8100e-03, -1.0482e-02, -1.1191e-02,\n",
      "         2.8645e-02,  1.7262e-02, -1.0095e-02,  3.9760e-02, -1.4124e-02,\n",
      "         2.6254e-02, -2.0240e-03,  4.5003e-03,  3.7706e-02,  2.5309e-02,\n",
      "        -3.1340e-04, -1.2833e-02,  1.5723e-02,  1.5785e-02, -2.7601e-02,\n",
      "        -2.4983e-02, -3.1119e-02,  2.2016e-02, -4.1312e-02,  3.9378e-02,\n",
      "        -3.5074e-02, -1.4809e-02, -6.4014e-03, -3.0035e-02,  3.6076e-02,\n",
      "        -2.7925e-03,  3.3335e-02, -2.5651e-02, -3.4761e-02,  3.5990e-02,\n",
      "         2.1713e-02,  2.2837e-02,  3.5131e-02,  3.2066e-02, -2.4614e-02,\n",
      "         3.8843e-02,  3.1795e-02, -1.9043e-02,  2.3963e-02,  4.0848e-02,\n",
      "         4.0062e-02, -3.4095e-02,  1.4334e-02, -2.3565e-02,  1.7387e-02,\n",
      "        -3.2224e-02,  1.2107e-02,  3.4349e-02,  6.0387e-03, -4.7857e-03,\n",
      "         1.8640e-02,  1.2035e-02,  2.4967e-02, -8.5778e-03, -1.9372e-02,\n",
      "        -2.8810e-03,  4.1481e-03, -2.2812e-02,  1.2971e-02,  2.6106e-02,\n",
      "        -1.1923e-02,  4.1689e-02, -3.0033e-02, -3.0815e-02, -1.3618e-02,\n",
      "         5.1665e-03,  1.9530e-02,  4.0235e-02, -2.1787e-02, -2.1926e-02,\n",
      "        -6.6425e-03, -1.7219e-02,  3.5138e-02,  2.4233e-03,  4.3693e-02,\n",
      "        -3.9770e-02,  3.5513e-02,  5.3113e-04, -9.2982e-03,  2.5814e-02,\n",
      "         1.5392e-02, -1.7870e-02,  4.3638e-02,  1.0346e-02,  3.1476e-02,\n",
      "         1.0387e-02, -5.4687e-03,  1.9538e-02,  3.4547e-02, -3.4805e-02,\n",
      "         1.3291e-02,  2.9038e-02,  9.5542e-03, -1.9219e-02, -8.7406e-03,\n",
      "         1.4032e-03, -1.3722e-02, -2.1624e-02,  8.8527e-03, -1.8672e-02,\n",
      "         8.3994e-03,  3.9076e-02], requires_grad=True))\n",
      "('decoder.layers.1.attn_2.q_linear.weight', Parameter containing:\n",
      "tensor([[-0.0652,  0.0256,  0.0558,  ...,  0.0027,  0.0386, -0.0458],\n",
      "        [ 0.0482,  0.0545, -0.0502,  ..., -0.0218,  0.0114,  0.0278],\n",
      "        [ 0.0461, -0.0213, -0.0443,  ...,  0.0648, -0.0132,  0.0561],\n",
      "        ...,\n",
      "        [-0.0404,  0.0432,  0.0576,  ...,  0.0053,  0.0571,  0.0134],\n",
      "        [ 0.0128,  0.0107,  0.0063,  ...,  0.0388, -0.0389,  0.0603],\n",
      "        [ 0.0626, -0.0553, -0.0485,  ..., -0.0153, -0.0549, -0.0507]],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.1.attn_2.q_linear.bias', Parameter containing:\n",
      "tensor([-2.5482e-02,  3.0714e-02, -6.0663e-03,  1.1897e-02,  3.0474e-02,\n",
      "         3.0301e-02, -3.1985e-02, -3.7031e-02,  6.0669e-03,  1.9000e-02,\n",
      "        -8.8557e-03,  3.7603e-02, -2.8522e-02,  1.2932e-03, -2.9055e-02,\n",
      "        -3.4601e-02, -4.3047e-02, -1.4095e-02,  1.5802e-02, -3.9407e-02,\n",
      "         2.4407e-03, -1.3368e-02, -3.9014e-04, -2.2069e-02,  1.9206e-02,\n",
      "        -3.5177e-02,  2.9807e-02, -2.7268e-02, -1.2367e-02,  3.2838e-02,\n",
      "         3.7000e-02,  2.3486e-02,  2.9882e-02,  9.1628e-03,  9.7919e-03,\n",
      "         1.0459e-02,  3.0155e-02,  2.0088e-02,  1.2786e-03, -1.6787e-02,\n",
      "         2.5062e-03, -2.2193e-02, -4.0236e-02, -2.6964e-02,  2.3107e-02,\n",
      "         8.4816e-03,  3.3687e-02, -4.1483e-02,  1.1751e-02,  8.9036e-03,\n",
      "        -1.4256e-02,  1.5797e-02,  3.7354e-03,  1.5121e-02,  2.2440e-02,\n",
      "        -3.1762e-02,  2.8043e-02,  4.6021e-04,  6.9918e-03,  4.7586e-04,\n",
      "         4.1453e-02, -3.0235e-02,  2.8530e-02, -1.7765e-02,  3.4188e-02,\n",
      "         2.8130e-02,  2.5389e-02, -1.8493e-02,  1.0648e-02, -2.4989e-02,\n",
      "        -4.3058e-02,  1.7540e-03, -2.0182e-02,  8.9239e-03, -1.3243e-02,\n",
      "         2.0178e-02,  6.5989e-03, -2.3309e-02, -4.3183e-02, -4.2761e-02,\n",
      "         3.4816e-03, -2.1009e-02, -3.2948e-02,  2.3175e-02,  3.2013e-02,\n",
      "        -2.4826e-02,  3.0641e-02, -1.2838e-02, -1.0952e-02, -3.5433e-02,\n",
      "        -2.6887e-02, -1.5637e-03, -1.2366e-02, -3.9255e-02,  6.9197e-03,\n",
      "         3.6531e-03,  4.2345e-03, -2.4022e-02,  3.3792e-02,  1.9723e-02,\n",
      "         6.8231e-03, -2.9152e-02,  3.3015e-02,  3.3625e-02,  4.2973e-02,\n",
      "         2.7527e-02,  1.2121e-02, -8.6875e-03,  2.6844e-02,  2.9824e-03,\n",
      "         2.9995e-02, -1.0474e-02, -8.9787e-03, -1.2459e-02, -2.7010e-02,\n",
      "         3.8908e-02,  3.4352e-02,  2.3738e-02,  4.5970e-03,  8.7368e-03,\n",
      "        -2.6861e-02, -1.3570e-02, -3.1736e-02, -1.7313e-02,  1.5908e-02,\n",
      "        -4.0508e-02,  1.1367e-02,  4.3669e-02, -2.2099e-02,  1.8841e-02,\n",
      "        -1.9892e-02, -2.2990e-02, -5.8053e-03, -2.7299e-02, -5.4369e-03,\n",
      "        -2.0348e-02,  3.6915e-02,  2.4155e-02,  2.2364e-02,  4.3687e-02,\n",
      "         7.4108e-03,  1.9226e-02,  3.9295e-02,  2.9160e-02, -3.3085e-02,\n",
      "         2.6846e-02,  3.2686e-02,  3.9986e-02,  2.7835e-02, -1.0862e-02,\n",
      "         3.4819e-02, -2.6921e-02,  3.2904e-02, -2.9184e-03, -3.4028e-02,\n",
      "         2.2807e-02,  2.9692e-02, -3.3222e-02,  5.1688e-04,  2.7462e-02,\n",
      "        -2.0856e-02, -2.5439e-02, -3.7242e-02, -3.1748e-02, -1.3148e-02,\n",
      "         2.4778e-02, -3.8193e-02,  1.4601e-02,  1.5750e-02, -3.4304e-02,\n",
      "         1.5267e-02,  5.1452e-03, -2.9819e-02,  1.3821e-02, -1.4607e-02,\n",
      "         3.7516e-02, -6.9664e-03, -2.1493e-02,  2.3080e-02, -5.7208e-03,\n",
      "         1.4411e-02,  2.5053e-02, -1.7226e-02,  5.5899e-03, -4.2701e-02,\n",
      "         2.8992e-02, -1.6171e-02, -1.9829e-02,  4.0307e-02, -1.0757e-02,\n",
      "        -3.5512e-02,  4.3222e-02,  1.8056e-02, -9.1843e-03, -1.2304e-02,\n",
      "         7.9703e-03, -3.2069e-02, -2.3790e-03, -3.6655e-02,  2.2201e-02,\n",
      "        -1.0150e-03, -1.3599e-02,  5.3565e-03, -1.1565e-02, -1.7931e-02,\n",
      "         2.7155e-02,  1.3539e-02, -3.4794e-02,  5.1572e-03,  4.3300e-02,\n",
      "         1.3514e-02, -1.4745e-02, -8.0353e-03, -3.4925e-02, -3.3805e-02,\n",
      "        -4.3373e-02, -3.6016e-02, -8.5378e-03, -6.7933e-03,  1.5748e-02,\n",
      "         3.9594e-02,  2.0192e-02, -9.5662e-03, -2.6932e-02, -1.9671e-02,\n",
      "        -2.6813e-02, -2.6472e-02,  4.3858e-02, -1.2740e-02, -2.8625e-02,\n",
      "         8.9670e-03,  8.7612e-04,  6.5643e-03,  2.8251e-02, -3.9254e-03,\n",
      "         2.3640e-02,  2.6098e-02, -3.3330e-02, -6.1374e-03, -2.0684e-02,\n",
      "         8.8539e-05,  2.1978e-02,  6.2222e-03,  8.7017e-03,  4.0476e-02,\n",
      "         2.4504e-02,  1.4804e-02, -1.8249e-02, -4.9935e-03, -3.8450e-02,\n",
      "         1.9103e-03, -3.0672e-02,  1.6742e-02,  3.2058e-03, -3.9342e-02,\n",
      "         2.7563e-02, -3.1311e-02, -3.3850e-02, -3.0881e-02, -2.2993e-02,\n",
      "        -1.7722e-02, -2.1471e-02,  3.6008e-02, -1.0852e-03, -3.3856e-02,\n",
      "         7.8984e-03, -2.8955e-02, -2.8330e-02, -4.0416e-02, -6.6425e-03,\n",
      "        -1.3358e-03, -1.3506e-02,  8.2951e-03,  4.0256e-02,  1.4247e-02,\n",
      "        -2.3410e-02,  1.7502e-02,  3.8977e-02, -5.0270e-04, -2.8937e-02,\n",
      "         1.3463e-02, -3.6178e-02,  2.2233e-02, -5.5845e-03, -3.1568e-02,\n",
      "         4.0650e-02,  3.6801e-03, -4.1122e-02, -1.4874e-02, -1.8676e-02,\n",
      "         3.3474e-02, -3.9274e-02,  3.6345e-04, -3.6430e-02, -4.3088e-02,\n",
      "        -2.9775e-02, -9.3694e-03,  2.5574e-03, -1.0642e-02, -1.2236e-02,\n",
      "        -1.4113e-02, -3.2563e-02, -3.2588e-02,  4.1545e-02,  3.1261e-02,\n",
      "        -1.7577e-02,  1.8762e-02,  1.9510e-02,  1.1460e-03, -3.4310e-02,\n",
      "         1.9508e-02,  2.6744e-02, -1.4071e-02,  2.1795e-02, -7.1898e-03,\n",
      "        -1.0482e-02, -9.6790e-03, -2.2857e-02, -2.3044e-02, -2.8128e-03,\n",
      "         1.2064e-02,  1.0245e-02,  2.1810e-02, -2.9756e-02,  3.3459e-02,\n",
      "        -1.9853e-02, -2.0372e-02,  2.2534e-02, -2.0185e-02,  3.5615e-03,\n",
      "         1.2387e-03, -2.1572e-02,  1.0538e-02, -2.4377e-02,  4.0823e-02,\n",
      "         1.5031e-02,  3.5548e-02,  1.1770e-02, -2.7254e-02,  1.7577e-02,\n",
      "        -4.8988e-03, -3.0110e-03, -2.7723e-02,  3.8042e-02,  1.5410e-02,\n",
      "        -1.4447e-02,  1.5154e-02, -3.4693e-02, -8.3186e-03, -4.2211e-03,\n",
      "        -1.0352e-02,  1.8293e-02,  2.6971e-02, -2.8845e-02, -1.6046e-02,\n",
      "         4.9492e-04, -7.3191e-03,  3.0229e-02, -3.4160e-02, -1.0508e-02,\n",
      "         2.7467e-02, -1.9408e-02, -9.0138e-03, -4.7527e-04,  2.4726e-02,\n",
      "        -1.3950e-02,  3.8254e-02, -3.9737e-02,  3.5363e-02, -2.6827e-02,\n",
      "         3.0671e-02, -1.7320e-02, -7.5885e-03, -3.2979e-02, -1.4071e-02,\n",
      "         5.7309e-03,  3.9878e-02,  2.8429e-02,  5.7303e-03, -2.5669e-02,\n",
      "        -6.9241e-03,  3.5904e-02,  1.4375e-02, -2.4654e-02, -2.2532e-02,\n",
      "        -2.2109e-02, -2.5829e-02, -2.3874e-03, -1.9579e-02,  1.8770e-02,\n",
      "        -4.3753e-03, -4.2310e-02,  6.0061e-03, -3.8300e-03, -3.0297e-02,\n",
      "        -3.2799e-02, -2.1552e-04,  3.2620e-02, -3.5867e-02,  3.0070e-02,\n",
      "        -3.0505e-02,  2.0812e-02, -1.7911e-02, -7.8726e-03,  1.2186e-03,\n",
      "         2.4234e-02,  3.0186e-02,  4.2105e-02,  2.6926e-02, -1.9335e-02,\n",
      "        -5.2480e-03, -4.2212e-02, -4.1683e-03,  1.7388e-02, -2.8718e-02,\n",
      "        -3.3605e-03,  3.7418e-02,  4.3359e-02, -2.8217e-02, -5.4842e-03,\n",
      "        -1.6285e-03, -3.8718e-02, -7.9592e-03,  2.8548e-02, -6.8346e-03,\n",
      "         2.8339e-02, -3.5978e-02, -3.6012e-03,  2.6777e-02, -4.2263e-03,\n",
      "         3.4270e-02,  9.0236e-03, -3.8775e-02, -3.3004e-02, -1.1122e-02,\n",
      "         2.1303e-02,  6.1431e-03,  1.7769e-03, -2.1188e-02, -1.4132e-02,\n",
      "         6.3711e-03, -3.8522e-02, -4.3171e-02,  2.3190e-02,  1.3825e-02,\n",
      "        -4.5304e-03,  2.0327e-03,  4.0388e-02,  2.7408e-02, -4.8812e-03,\n",
      "        -2.4753e-02, -9.2369e-04, -4.8755e-03,  9.9780e-03,  4.0706e-02,\n",
      "         1.6470e-02, -2.4118e-02,  7.6996e-03,  3.6924e-02,  1.7864e-02,\n",
      "         1.8402e-02, -3.9133e-02, -8.7902e-05,  8.1743e-03,  3.9232e-02,\n",
      "         6.9311e-04,  2.2485e-02, -2.8241e-02, -3.9161e-03, -2.9401e-03,\n",
      "        -3.4624e-02, -3.4317e-02,  6.5667e-03, -3.2655e-02, -1.0732e-02,\n",
      "        -1.2539e-02, -4.1604e-02, -3.1626e-02,  3.1179e-02, -8.4892e-03,\n",
      "        -3.1026e-02, -2.0138e-02, -1.2362e-02,  4.6447e-03, -2.7697e-02,\n",
      "        -2.7785e-02,  3.2101e-02,  8.1615e-03, -2.2313e-02, -1.1509e-02,\n",
      "        -4.2134e-02, -2.9346e-02,  2.2162e-02, -1.2102e-02,  6.3674e-03,\n",
      "         1.6563e-02,  3.5800e-02, -2.4614e-02, -2.5821e-02,  3.5310e-03,\n",
      "        -7.8498e-03, -2.2009e-03, -3.6214e-02,  2.6199e-02,  3.1422e-02,\n",
      "        -3.7858e-02,  9.0693e-03,  9.9968e-03, -2.8525e-03, -1.1557e-02,\n",
      "        -1.8754e-02,  4.1551e-02], requires_grad=True))\n",
      "('decoder.layers.1.attn_2.v_linear.weight', Parameter containing:\n",
      "tensor([[-0.0146, -0.0251,  0.0320,  ...,  0.0418, -0.0640,  0.0105],\n",
      "        [ 0.0654,  0.0407,  0.0477,  ..., -0.0391, -0.0106,  0.0694],\n",
      "        [ 0.0563, -0.0095, -0.0215,  ...,  0.0704,  0.0417, -0.0179],\n",
      "        ...,\n",
      "        [ 0.0396,  0.0714, -0.0563,  ...,  0.0124,  0.0605,  0.0194],\n",
      "        [-0.0723,  0.0303, -0.0588,  ...,  0.0299,  0.0386, -0.0706],\n",
      "        [-0.0681, -0.0330,  0.0168,  ...,  0.0628,  0.0558, -0.0363]],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.1.attn_2.v_linear.bias', Parameter containing:\n",
      "tensor([ 0.0167,  0.0042,  0.0015,  0.0388,  0.0187, -0.0118, -0.0336,  0.0099,\n",
      "        -0.0420, -0.0141, -0.0029, -0.0404, -0.0334, -0.0359, -0.0059,  0.0140,\n",
      "         0.0436,  0.0397, -0.0325, -0.0274,  0.0164,  0.0177,  0.0264,  0.0226,\n",
      "        -0.0126, -0.0265,  0.0262,  0.0333,  0.0210,  0.0020, -0.0091,  0.0335,\n",
      "         0.0143, -0.0260,  0.0352, -0.0002,  0.0263, -0.0074,  0.0011,  0.0039,\n",
      "         0.0222,  0.0327,  0.0053,  0.0314, -0.0343, -0.0066, -0.0367, -0.0377,\n",
      "        -0.0240, -0.0373, -0.0153,  0.0213,  0.0421, -0.0375, -0.0123,  0.0322,\n",
      "         0.0053,  0.0078, -0.0325,  0.0319,  0.0293,  0.0122, -0.0441,  0.0313,\n",
      "        -0.0168, -0.0334, -0.0177, -0.0211,  0.0120,  0.0122,  0.0192, -0.0255,\n",
      "         0.0274,  0.0074, -0.0299, -0.0018, -0.0054,  0.0225,  0.0166,  0.0200,\n",
      "         0.0389, -0.0179,  0.0077,  0.0316, -0.0214,  0.0340,  0.0207, -0.0309,\n",
      "        -0.0297,  0.0280, -0.0361, -0.0095, -0.0432, -0.0416,  0.0032, -0.0337,\n",
      "        -0.0009,  0.0207,  0.0250, -0.0247, -0.0344,  0.0253,  0.0036,  0.0073,\n",
      "         0.0302,  0.0309, -0.0172, -0.0132, -0.0142, -0.0306,  0.0387,  0.0439,\n",
      "         0.0438,  0.0384,  0.0097, -0.0430, -0.0412,  0.0430,  0.0440, -0.0205,\n",
      "        -0.0208, -0.0013, -0.0183, -0.0072, -0.0252, -0.0388, -0.0046, -0.0421,\n",
      "        -0.0370, -0.0193, -0.0167, -0.0160,  0.0354,  0.0114, -0.0112,  0.0100,\n",
      "        -0.0136,  0.0103,  0.0280,  0.0092,  0.0180, -0.0189,  0.0122, -0.0207,\n",
      "         0.0240,  0.0089, -0.0355, -0.0186, -0.0144,  0.0003, -0.0369, -0.0393,\n",
      "         0.0300,  0.0208,  0.0162, -0.0107,  0.0411,  0.0397, -0.0118,  0.0258,\n",
      "        -0.0045,  0.0132, -0.0416, -0.0174,  0.0413, -0.0180, -0.0414,  0.0341,\n",
      "        -0.0138, -0.0403,  0.0196,  0.0224, -0.0296, -0.0175,  0.0045,  0.0081,\n",
      "        -0.0215,  0.0073,  0.0255,  0.0181, -0.0014,  0.0010, -0.0032,  0.0265,\n",
      "         0.0370,  0.0246,  0.0435, -0.0369, -0.0066, -0.0441,  0.0225,  0.0157,\n",
      "        -0.0252, -0.0292, -0.0117,  0.0154,  0.0286, -0.0289, -0.0113, -0.0082,\n",
      "        -0.0354, -0.0051, -0.0087, -0.0435,  0.0171,  0.0253,  0.0225, -0.0164,\n",
      "        -0.0335,  0.0323,  0.0398,  0.0038, -0.0210,  0.0406, -0.0122, -0.0255,\n",
      "        -0.0380,  0.0415, -0.0124,  0.0001, -0.0401, -0.0123, -0.0304, -0.0194,\n",
      "        -0.0171, -0.0205, -0.0216, -0.0151,  0.0165,  0.0283,  0.0424,  0.0104,\n",
      "         0.0223,  0.0219,  0.0021, -0.0414,  0.0196, -0.0257, -0.0032,  0.0015,\n",
      "         0.0228, -0.0246, -0.0095, -0.0440,  0.0276,  0.0260, -0.0148, -0.0113,\n",
      "         0.0418, -0.0098,  0.0401,  0.0179,  0.0381, -0.0257, -0.0217,  0.0157,\n",
      "         0.0104,  0.0261, -0.0302, -0.0292, -0.0259,  0.0309,  0.0211,  0.0300,\n",
      "         0.0187, -0.0353, -0.0097,  0.0374, -0.0071, -0.0308, -0.0290, -0.0106,\n",
      "         0.0079, -0.0231, -0.0441, -0.0060, -0.0260, -0.0150, -0.0220,  0.0228,\n",
      "        -0.0404, -0.0058, -0.0157,  0.0016,  0.0019, -0.0282,  0.0330,  0.0222,\n",
      "         0.0438,  0.0400, -0.0148, -0.0361, -0.0079,  0.0227,  0.0147,  0.0324,\n",
      "        -0.0111, -0.0244,  0.0005, -0.0420,  0.0375,  0.0128,  0.0003, -0.0033,\n",
      "        -0.0160, -0.0352, -0.0330, -0.0392,  0.0117,  0.0017, -0.0236,  0.0113,\n",
      "        -0.0304, -0.0013, -0.0393,  0.0262, -0.0300, -0.0287,  0.0212,  0.0087,\n",
      "        -0.0187,  0.0075, -0.0155, -0.0077,  0.0216, -0.0251,  0.0141, -0.0240,\n",
      "         0.0162, -0.0137,  0.0250, -0.0289,  0.0429, -0.0307,  0.0034,  0.0132,\n",
      "        -0.0066, -0.0215, -0.0202, -0.0271,  0.0214, -0.0135,  0.0088, -0.0221,\n",
      "         0.0409,  0.0305,  0.0338, -0.0435,  0.0018, -0.0418, -0.0248,  0.0275,\n",
      "        -0.0431, -0.0280,  0.0086, -0.0258, -0.0439, -0.0355,  0.0266, -0.0384,\n",
      "        -0.0399,  0.0415,  0.0367,  0.0146,  0.0094, -0.0206, -0.0385,  0.0410,\n",
      "         0.0399, -0.0081, -0.0418, -0.0313, -0.0020, -0.0208,  0.0026,  0.0059,\n",
      "        -0.0279,  0.0322, -0.0364,  0.0296,  0.0322,  0.0278,  0.0336,  0.0023,\n",
      "         0.0440, -0.0152, -0.0177, -0.0420, -0.0439, -0.0325,  0.0030, -0.0178,\n",
      "        -0.0155,  0.0434,  0.0231, -0.0358,  0.0389, -0.0166,  0.0279, -0.0338,\n",
      "         0.0145,  0.0273,  0.0010,  0.0229,  0.0094, -0.0368,  0.0353,  0.0271,\n",
      "         0.0282, -0.0217, -0.0328,  0.0095, -0.0145,  0.0027,  0.0020, -0.0412,\n",
      "        -0.0333, -0.0059, -0.0130, -0.0050,  0.0051,  0.0133, -0.0201,  0.0342,\n",
      "         0.0379,  0.0279, -0.0338, -0.0395, -0.0087, -0.0252, -0.0001,  0.0384,\n",
      "        -0.0092, -0.0075, -0.0266,  0.0025,  0.0374,  0.0178,  0.0158, -0.0215,\n",
      "         0.0182,  0.0408,  0.0125, -0.0376,  0.0108, -0.0173,  0.0095, -0.0098,\n",
      "         0.0190,  0.0354, -0.0251, -0.0249, -0.0439, -0.0336, -0.0390,  0.0118,\n",
      "         0.0367,  0.0428, -0.0021,  0.0026,  0.0053,  0.0286,  0.0015, -0.0079,\n",
      "         0.0387, -0.0308, -0.0211, -0.0171,  0.0177, -0.0025,  0.0324,  0.0328,\n",
      "         0.0098, -0.0228, -0.0238,  0.0199,  0.0256,  0.0193,  0.0276, -0.0256,\n",
      "         0.0065, -0.0156,  0.0254, -0.0032,  0.0089, -0.0423,  0.0218, -0.0107,\n",
      "         0.0007, -0.0398, -0.0075, -0.0005, -0.0367,  0.0330,  0.0313, -0.0047,\n",
      "         0.0097,  0.0409,  0.0381,  0.0154,  0.0031, -0.0259, -0.0302, -0.0050,\n",
      "        -0.0280,  0.0109, -0.0369, -0.0140, -0.0192,  0.0334,  0.0013, -0.0253],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.1.attn_2.k_linear.weight', Parameter containing:\n",
      "tensor([[ 5.7845e-02,  4.9421e-02, -4.5159e-03,  ..., -3.2298e-02,\n",
      "         -2.2433e-02,  1.1560e-02],\n",
      "        [ 3.8682e-02,  4.1236e-02,  1.3444e-02,  ..., -3.1523e-02,\n",
      "         -1.1840e-02,  1.1196e-02],\n",
      "        [ 6.9600e-02, -1.5200e-02, -1.8501e-02,  ..., -1.4545e-03,\n",
      "          5.1795e-02, -5.4985e-05],\n",
      "        ...,\n",
      "        [-3.9986e-02, -6.2209e-02, -2.3042e-02,  ...,  3.6547e-02,\n",
      "         -2.3914e-02,  3.5810e-02],\n",
      "        [ 3.7167e-02,  4.3343e-02,  1.8113e-02,  ...,  2.4492e-02,\n",
      "          1.1687e-02, -1.3350e-03],\n",
      "        [ 5.0377e-02,  1.4222e-02,  4.1224e-02,  ...,  5.0509e-02,\n",
      "          7.2345e-02, -5.8379e-02]], requires_grad=True))\n",
      "('decoder.layers.1.attn_2.k_linear.bias', Parameter containing:\n",
      "tensor([ 1.3938e-02, -3.7332e-02, -4.0067e-02, -7.7398e-03,  3.7865e-02,\n",
      "         3.7442e-02, -3.1719e-02,  1.4165e-02, -1.3083e-02, -4.0572e-02,\n",
      "        -1.0891e-02,  1.9268e-02,  2.2367e-03, -1.5541e-02, -1.5248e-02,\n",
      "         3.2123e-02,  3.7047e-02, -1.9573e-02,  2.2611e-02, -2.7711e-02,\n",
      "        -1.1166e-02,  2.6144e-02,  1.1372e-02, -3.4058e-02, -5.2031e-03,\n",
      "         2.7762e-03,  3.4416e-02,  1.9784e-02,  2.1214e-02,  1.0560e-02,\n",
      "        -2.4544e-02,  4.0631e-02,  1.1471e-02,  3.9509e-02, -2.4048e-02,\n",
      "        -2.8959e-02, -2.8755e-02, -2.6552e-02,  4.0530e-02,  2.2012e-02,\n",
      "         2.5531e-02,  2.6780e-03,  1.9982e-02, -2.0746e-02,  4.3522e-02,\n",
      "        -2.4086e-02,  2.2699e-02,  2.4684e-02,  1.2928e-02, -1.1247e-02,\n",
      "         8.9669e-03, -2.2107e-02, -1.3124e-02, -2.1815e-02,  4.2267e-02,\n",
      "         3.5852e-02, -1.9017e-02, -3.1487e-02,  6.7544e-03,  1.2353e-02,\n",
      "        -1.6820e-02,  1.5105e-03, -2.6420e-02,  2.5815e-02,  1.8752e-02,\n",
      "         5.9032e-03,  8.2256e-03,  1.9890e-02, -1.3120e-03,  1.5450e-02,\n",
      "        -2.0813e-04, -3.6867e-02,  3.1581e-02, -4.2273e-02, -3.8674e-02,\n",
      "         2.0190e-02, -3.5987e-02, -8.0832e-03,  3.3706e-02, -1.4890e-02,\n",
      "        -4.3016e-02,  2.2034e-02, -2.2999e-02,  3.8586e-02,  2.8095e-02,\n",
      "        -2.3731e-02, -4.4727e-03,  9.8589e-03,  3.0196e-02,  3.3746e-02,\n",
      "         1.4102e-02, -2.9597e-02,  5.1847e-03, -3.1812e-02, -3.2524e-02,\n",
      "         1.5927e-02,  5.2262e-03, -1.4796e-02,  2.6893e-02,  2.1504e-02,\n",
      "         1.2162e-03, -2.9461e-02, -8.2447e-03, -1.3766e-02,  4.3670e-02,\n",
      "         2.2534e-02, -8.2402e-03,  1.3567e-02, -1.4638e-02, -3.8840e-02,\n",
      "        -1.0086e-02, -1.5539e-02,  1.2620e-02,  4.1148e-02,  2.6367e-02,\n",
      "        -1.5349e-02, -3.4769e-03, -2.6867e-02, -1.0960e-02,  3.3831e-02,\n",
      "        -1.9616e-02, -5.4006e-03,  4.6570e-03,  3.6762e-02,  2.1673e-02,\n",
      "        -3.0693e-05,  1.4730e-02, -2.6605e-02, -3.0572e-02,  1.7292e-02,\n",
      "        -1.2778e-02, -5.2169e-03,  5.4665e-04,  4.3658e-02, -2.2695e-02,\n",
      "         2.1813e-02,  2.1311e-02, -3.6541e-02,  2.0255e-02, -5.3841e-03,\n",
      "         3.8453e-03, -1.7646e-02, -1.2161e-03,  2.9495e-02,  3.0144e-02,\n",
      "         2.2463e-02,  7.3353e-03,  1.6860e-02, -2.1675e-02,  5.6475e-03,\n",
      "         2.9098e-03,  1.0751e-02,  2.8389e-02,  3.0835e-02, -8.2080e-03,\n",
      "         1.0898e-02, -5.2218e-03, -3.1584e-02,  2.2036e-02,  2.1354e-02,\n",
      "         4.0707e-02, -1.1129e-02,  3.4521e-02, -1.0409e-02, -8.2819e-04,\n",
      "        -1.8052e-02,  4.3793e-02,  1.8036e-02, -3.8044e-02, -2.9508e-02,\n",
      "         2.7679e-02,  3.1041e-02,  1.0423e-02,  2.8182e-02,  4.1788e-03,\n",
      "         6.1217e-04, -2.9267e-03, -3.0371e-02, -1.2052e-02,  7.6905e-03,\n",
      "        -4.3121e-02, -2.2400e-02,  1.1063e-02, -4.8389e-03,  2.9504e-02,\n",
      "        -1.4690e-02, -4.1097e-02, -1.7985e-02,  2.6780e-02,  3.0236e-02,\n",
      "         2.8071e-02, -3.9107e-02, -1.5111e-02,  5.1241e-03,  4.3484e-02,\n",
      "        -4.2470e-02,  3.5656e-02, -3.1900e-02,  1.3611e-02, -8.6843e-03,\n",
      "         6.2164e-03,  4.0238e-02,  3.5091e-02, -2.7689e-02,  2.8936e-02,\n",
      "        -9.3379e-03,  3.0636e-02, -8.7199e-03,  3.9968e-02,  2.5985e-02,\n",
      "        -3.2293e-02, -2.4484e-02,  3.6208e-02, -3.0700e-02, -1.9345e-02,\n",
      "         2.4478e-02,  2.3904e-02, -3.0927e-02, -3.3487e-02,  2.9518e-02,\n",
      "        -4.2648e-02, -3.4386e-02, -4.3801e-02,  3.9256e-02,  5.5716e-03,\n",
      "        -2.0797e-02, -3.8741e-02, -8.1682e-03, -1.8243e-02, -4.0012e-02,\n",
      "        -4.2483e-02,  2.7821e-02, -8.5296e-03,  4.1980e-02,  4.1232e-02,\n",
      "         3.1933e-02,  1.6851e-02,  1.7261e-02, -7.9419e-03,  2.6870e-02,\n",
      "         3.4684e-02, -4.5694e-04,  2.9625e-02, -3.1271e-02, -2.8850e-02,\n",
      "         1.5638e-02, -3.1055e-02,  8.5698e-03, -7.5822e-03, -1.8197e-02,\n",
      "        -1.7339e-02, -4.0982e-03, -2.4417e-02,  1.4339e-02, -2.3912e-02,\n",
      "        -4.2143e-02, -3.5163e-02,  4.4215e-03,  6.1657e-03,  2.7215e-02,\n",
      "        -8.7577e-03,  3.9615e-02, -4.2073e-04,  3.6038e-02,  9.0713e-03,\n",
      "         4.2430e-02, -1.2232e-02,  4.2699e-03,  1.2593e-02, -4.1661e-02,\n",
      "         2.5124e-02,  1.2154e-02, -2.6966e-02, -7.6992e-03,  3.7570e-02,\n",
      "        -1.4733e-02, -1.7789e-02,  5.4201e-03,  1.9172e-02, -2.0125e-02,\n",
      "        -3.5029e-02, -2.0505e-02, -1.5556e-02, -1.9945e-02,  2.2554e-02,\n",
      "         3.0309e-02, -4.6763e-03,  3.7393e-02,  1.4364e-02,  4.1287e-02,\n",
      "         3.7036e-02,  1.9832e-02,  1.5466e-02, -3.8335e-02,  1.8149e-02,\n",
      "        -1.3149e-02,  1.7274e-02, -1.3069e-02,  8.8808e-03, -1.3395e-02,\n",
      "        -1.2294e-02, -2.7799e-02, -1.3331e-02,  1.6545e-02, -3.7033e-03,\n",
      "        -3.7474e-02, -1.4192e-02,  1.8914e-02,  9.8568e-03, -9.3391e-03,\n",
      "        -4.1212e-02,  2.6474e-03,  1.8734e-02,  5.1248e-03, -5.8549e-03,\n",
      "         3.0118e-02, -1.0323e-02, -3.1572e-02,  2.9264e-02,  2.2993e-02,\n",
      "        -1.9774e-02, -3.7136e-02, -2.2545e-02,  3.6116e-02, -2.4400e-02,\n",
      "        -2.7735e-02,  1.1793e-03,  5.2318e-03,  8.5438e-03,  3.7425e-02,\n",
      "         2.0773e-02, -1.5894e-02,  1.7811e-03, -1.5192e-02, -2.9946e-02,\n",
      "        -3.8766e-02,  1.5813e-02, -5.6081e-03, -1.2233e-02, -5.8939e-04,\n",
      "        -1.7280e-02, -1.3693e-02, -4.3886e-02,  1.0366e-02, -4.2284e-02,\n",
      "        -2.4589e-02,  1.7155e-02, -3.0993e-02,  3.7894e-02, -2.4970e-02,\n",
      "        -9.8428e-03,  2.6930e-02,  1.8965e-02,  7.2971e-03,  1.1563e-02,\n",
      "        -2.5452e-02,  2.5158e-02,  2.1054e-02, -1.2459e-02,  1.0937e-02,\n",
      "         6.6902e-03,  4.2686e-02,  2.2985e-02, -3.7236e-02,  1.6621e-02,\n",
      "        -2.8605e-02, -1.7182e-02,  4.3439e-03,  3.1389e-02, -7.8019e-03,\n",
      "         2.9094e-02,  3.4010e-02, -1.4820e-02,  3.2440e-02, -2.4144e-02,\n",
      "         3.8620e-02, -2.9471e-02,  1.5017e-02, -2.5478e-02,  4.0641e-02,\n",
      "         1.9551e-02,  2.2706e-03,  3.8346e-02, -1.4723e-02,  6.4172e-03,\n",
      "         4.2597e-02, -1.1546e-02, -7.6626e-03, -3.3314e-02,  1.8813e-02,\n",
      "        -7.8689e-03, -3.0922e-02,  4.2355e-02,  1.0830e-02,  1.0255e-02,\n",
      "        -7.1867e-03, -2.0717e-02, -4.1965e-02,  2.5313e-02, -4.5262e-03,\n",
      "         3.4911e-02, -4.2448e-02,  4.3478e-02,  1.0231e-02,  9.3298e-03,\n",
      "         3.8511e-02,  1.1964e-02,  3.7712e-03, -7.8408e-03,  3.5335e-02,\n",
      "        -4.8520e-03,  9.6342e-03, -1.7491e-02,  2.1993e-02,  7.0694e-03,\n",
      "         2.9217e-02,  3.7878e-02, -1.6037e-02, -7.3108e-03, -3.5376e-02,\n",
      "         2.6896e-02,  3.9222e-02,  4.1938e-03, -2.1535e-03,  3.2693e-03,\n",
      "        -2.8668e-02,  1.3619e-02,  3.8551e-02, -1.0891e-02, -5.7712e-03,\n",
      "         4.5346e-03, -2.2775e-02,  2.1719e-02,  3.9838e-02,  1.4422e-02,\n",
      "         2.6602e-02,  4.3508e-02,  3.9355e-02,  4.2580e-02, -4.4051e-03,\n",
      "        -3.6674e-04, -2.4921e-03, -1.5541e-02,  3.8356e-02, -3.4502e-02,\n",
      "        -2.4381e-02,  1.9189e-02, -1.1995e-02,  2.7764e-02, -2.2565e-02,\n",
      "        -7.4358e-03, -7.1326e-03,  1.4016e-02, -1.7147e-03, -1.9489e-02,\n",
      "         3.1670e-02, -1.7100e-02,  1.7816e-02,  1.9530e-02,  3.3414e-02,\n",
      "        -2.7442e-02, -3.9697e-02, -1.5534e-02, -3.6892e-02, -9.1591e-03,\n",
      "         1.8394e-02,  1.7870e-02,  4.1531e-02, -1.2706e-02,  2.4354e-02,\n",
      "        -2.2333e-02,  9.0439e-04,  1.7876e-02,  3.1721e-02, -6.1151e-03,\n",
      "        -3.8680e-02,  1.6291e-02, -2.8952e-02, -1.7962e-02, -7.8490e-03,\n",
      "         3.8529e-02, -2.6850e-02, -1.7676e-03, -1.5404e-02,  1.6372e-02,\n",
      "         5.0040e-03,  3.9369e-02, -2.9330e-03, -1.1268e-02, -4.2890e-02,\n",
      "        -3.7718e-02,  3.1598e-02,  1.6663e-02,  1.0581e-03,  3.2305e-02,\n",
      "        -5.4008e-03,  9.5059e-03, -4.2184e-02,  3.6090e-02, -8.0770e-03,\n",
      "        -3.9813e-02, -3.5535e-02,  2.4808e-02,  3.5216e-02,  2.1829e-02,\n",
      "        -1.8571e-02, -3.8547e-02,  2.7334e-02, -2.3735e-02,  3.3273e-02,\n",
      "        -3.7305e-02, -2.1206e-02], requires_grad=True))\n",
      "('decoder.layers.1.attn_2.out.weight', Parameter containing:\n",
      "tensor([[ 0.0042,  0.0559,  0.0003,  ..., -0.0542,  0.0117,  0.0736],\n",
      "        [ 0.0320, -0.0340, -0.0659,  ..., -0.0375,  0.0653,  0.0123],\n",
      "        [ 0.0044,  0.0205,  0.0115,  ..., -0.0751,  0.0724,  0.0287],\n",
      "        ...,\n",
      "        [-0.0599,  0.0073, -0.0609,  ...,  0.0286,  0.0165,  0.0066],\n",
      "        [-0.0630,  0.0353,  0.0697,  ..., -0.0667, -0.0587,  0.0232],\n",
      "        [ 0.0267,  0.0577, -0.0586,  ..., -0.0240,  0.0756, -0.0264]],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.1.attn_2.out.bias', Parameter containing:\n",
      "tensor([ 1.4056e-02,  2.0874e-02,  3.3965e-03, -3.1730e-02,  2.9779e-02,\n",
      "        -4.3988e-02, -2.3156e-02,  1.4102e-02, -1.6498e-02, -1.9250e-02,\n",
      "        -4.2273e-02,  2.0919e-02,  1.6226e-02, -2.9263e-02, -2.3179e-02,\n",
      "        -4.3025e-02, -8.1737e-03, -1.5506e-02, -1.9767e-02, -5.0250e-04,\n",
      "         1.1700e-02,  1.9448e-02,  3.8432e-02, -2.0513e-02, -1.0191e-02,\n",
      "        -1.6044e-02,  7.6916e-03, -2.1946e-02,  4.6894e-03, -3.6405e-02,\n",
      "         1.2010e-02, -2.6045e-02, -4.2237e-02,  3.0476e-02, -8.5821e-03,\n",
      "        -1.1207e-02, -3.1158e-02,  2.4001e-02,  2.2269e-02,  6.4540e-03,\n",
      "         3.0533e-02,  3.0177e-02, -2.8226e-02,  7.1540e-03, -4.0130e-02,\n",
      "         4.3901e-02, -9.5443e-03, -2.5154e-02,  4.3367e-02,  4.0514e-02,\n",
      "         4.1943e-02, -1.6267e-02,  4.1005e-02, -1.7704e-02,  5.8744e-03,\n",
      "        -3.3617e-02,  3.7693e-02, -1.1158e-03,  5.7159e-03,  2.1817e-02,\n",
      "        -2.8660e-03,  2.9846e-02,  5.6177e-03, -4.3425e-02,  8.1169e-03,\n",
      "         2.0446e-03, -1.8192e-02,  1.0095e-02, -2.8736e-02,  1.2707e-02,\n",
      "        -4.2721e-02,  3.0219e-02, -1.6707e-03, -3.6217e-02,  5.0435e-03,\n",
      "        -1.9322e-02,  3.1590e-02,  1.8986e-02, -7.4347e-03,  1.8413e-02,\n",
      "        -2.8042e-02,  1.3672e-02,  1.7254e-02, -6.0115e-03, -4.0697e-02,\n",
      "         2.7535e-02,  4.2856e-02,  4.1500e-03,  1.4013e-02,  1.8661e-02,\n",
      "        -3.5843e-02, -4.1678e-02, -3.6963e-02, -1.8803e-02,  2.1886e-02,\n",
      "         4.9858e-03, -2.2327e-02, -7.8904e-03,  3.0899e-02, -4.7389e-05,\n",
      "         1.4214e-02, -2.5455e-02, -2.0859e-02,  4.2878e-02, -4.3380e-04,\n",
      "        -1.5235e-02, -2.7738e-02,  3.7637e-02, -2.7990e-02, -2.9279e-02,\n",
      "        -2.5764e-02,  1.4985e-02,  3.5597e-02,  2.8387e-02,  7.6963e-03,\n",
      "        -2.3127e-03,  1.9161e-02, -5.7107e-03, -2.1427e-03, -4.5732e-03,\n",
      "         3.2229e-02,  1.1896e-02,  3.2455e-02, -1.8313e-02,  1.0591e-02,\n",
      "         2.1273e-02,  3.5979e-02, -6.5122e-03, -3.0913e-02,  1.7920e-02,\n",
      "        -1.6519e-02,  1.5692e-02,  9.1088e-03, -3.2060e-02, -3.9517e-03,\n",
      "         1.6146e-02, -3.4412e-02,  5.6832e-03,  2.7198e-02,  1.7252e-02,\n",
      "         3.1542e-03, -7.5240e-03,  4.7315e-03, -4.1879e-02,  3.3532e-02,\n",
      "        -1.1804e-02, -1.9852e-02, -2.8333e-02,  1.7652e-02,  3.1110e-02,\n",
      "        -4.3096e-02, -1.9701e-02,  1.1734e-02, -1.2663e-02, -1.7450e-02,\n",
      "         4.5092e-03,  4.0296e-02,  2.4904e-03, -3.0449e-02, -4.3632e-02,\n",
      "        -1.4662e-02,  4.1325e-02, -3.9078e-02,  1.3720e-02, -2.5442e-02,\n",
      "         1.7487e-02, -1.3275e-02, -2.8786e-02, -1.1052e-02, -9.1655e-04,\n",
      "        -4.7027e-03, -8.1901e-03, -7.4845e-03, -4.3467e-03,  4.0239e-02,\n",
      "         3.2214e-02,  1.3521e-02, -3.2104e-02, -8.0880e-03,  3.8030e-02,\n",
      "         3.1686e-03, -2.8298e-02,  1.2721e-02,  1.4113e-02,  1.1211e-03,\n",
      "        -2.2352e-02, -3.0828e-02, -4.0330e-02, -8.5657e-03, -3.3473e-02,\n",
      "        -3.7113e-02,  3.6371e-02,  3.1534e-02,  3.3063e-02,  5.2473e-03,\n",
      "        -3.1464e-02, -3.6215e-02,  3.5801e-02, -9.5474e-03,  1.4800e-02,\n",
      "         3.2077e-02,  4.2881e-02,  4.1535e-02, -3.4903e-02,  3.9371e-03,\n",
      "        -3.9929e-02,  2.5904e-02,  5.6941e-03,  3.0712e-02, -3.6517e-02,\n",
      "        -2.2270e-02,  1.4478e-02, -2.3288e-02,  3.9204e-02,  2.3246e-02,\n",
      "        -8.7329e-03,  8.2090e-03, -3.3447e-03,  2.2749e-02, -4.3356e-02,\n",
      "         2.2720e-02, -3.9966e-02, -7.8652e-03, -2.1168e-02, -3.9023e-02,\n",
      "        -3.3049e-02, -4.2774e-02,  3.5911e-02, -5.2516e-03,  1.6130e-02,\n",
      "        -3.0408e-02,  4.4498e-03,  1.1175e-02,  1.8723e-02,  3.8899e-03,\n",
      "        -1.2170e-02, -1.8175e-02,  2.2498e-02,  2.6228e-02,  1.0036e-02,\n",
      "         2.2643e-03, -1.8865e-02,  1.6380e-02, -2.2472e-02,  3.2933e-03,\n",
      "         4.2167e-02, -8.3198e-03, -6.4596e-03,  1.4758e-02, -2.6311e-02,\n",
      "        -6.3776e-03, -1.1435e-02,  3.8632e-02,  2.9511e-02,  8.8953e-03,\n",
      "         4.3311e-02,  3.6489e-02,  7.8113e-03,  2.5850e-02,  9.8955e-03,\n",
      "         1.5586e-03,  4.2984e-02,  1.6323e-03, -2.5637e-02,  1.4014e-02,\n",
      "         3.6111e-02,  2.3891e-02, -2.9202e-02,  1.6570e-02, -2.7983e-02,\n",
      "        -2.0258e-02, -3.0795e-02,  1.8135e-02, -1.2053e-02,  5.2127e-03,\n",
      "        -2.8835e-02, -1.9080e-02, -2.4133e-02,  2.0777e-02,  8.4654e-03,\n",
      "         1.1894e-02,  7.2905e-04,  2.1443e-02, -8.1144e-03,  3.0683e-03,\n",
      "        -3.5241e-02,  2.5207e-02,  2.8300e-02,  3.0467e-02,  2.8313e-03,\n",
      "        -5.1923e-03,  4.3179e-02, -3.9128e-02, -3.3871e-02, -3.6201e-02,\n",
      "         4.3766e-02, -3.1378e-02,  2.8898e-02,  2.3408e-02,  1.3268e-02,\n",
      "        -2.2160e-02,  2.8718e-02,  3.5830e-02, -2.2655e-02,  4.1663e-02,\n",
      "        -2.5791e-02, -2.0651e-02, -1.8924e-03, -2.5536e-02,  3.6500e-02,\n",
      "        -1.7811e-02, -9.6746e-04, -2.0752e-02, -3.0177e-02,  3.2534e-02,\n",
      "        -3.3232e-02, -1.7719e-02, -2.1857e-02,  4.3932e-02,  2.9078e-02,\n",
      "         2.6213e-02, -1.4930e-02,  2.3781e-02,  3.6151e-02, -2.1943e-02,\n",
      "        -1.8310e-03, -2.5986e-02, -6.7993e-04,  4.2851e-02, -4.2403e-02,\n",
      "         4.3957e-02,  3.6505e-04, -4.2566e-02,  2.6133e-02,  6.5186e-03,\n",
      "        -1.9095e-02,  2.3232e-02,  7.6239e-03, -5.5927e-03, -1.9249e-02,\n",
      "         1.0966e-02, -4.4987e-03, -2.3148e-02, -3.3766e-02,  2.0819e-02,\n",
      "         1.6355e-02, -1.0197e-02,  2.1571e-02,  2.6334e-02, -6.8923e-03,\n",
      "         3.3215e-02, -4.0255e-03,  2.2764e-02, -2.1716e-02, -3.7606e-02,\n",
      "         9.6058e-03, -3.1546e-02,  3.0820e-02,  1.5526e-02, -1.5590e-02,\n",
      "         1.9551e-02,  1.0805e-02, -3.9117e-02,  3.2034e-02, -3.6719e-02,\n",
      "        -2.1708e-02, -3.0403e-02,  1.4945e-02, -7.1232e-03,  2.7699e-02,\n",
      "         3.8043e-02,  3.5670e-02, -1.2247e-02, -4.2287e-02, -5.3044e-03,\n",
      "        -6.8860e-03,  2.6200e-02,  3.0016e-02,  1.9633e-02, -3.8184e-02,\n",
      "        -3.4061e-02,  3.0144e-02,  8.7462e-03, -1.2371e-02,  2.6655e-02,\n",
      "        -1.4152e-02, -4.3891e-03,  1.9022e-02,  3.4370e-02, -3.1060e-02,\n",
      "        -1.3843e-02,  3.6473e-02,  3.9059e-02, -1.2045e-02, -1.0980e-02,\n",
      "        -9.9266e-03, -1.5521e-02,  1.3812e-02,  7.9203e-03,  2.4146e-02,\n",
      "         7.8065e-03, -3.4363e-02,  4.0921e-02, -2.3592e-02,  3.3116e-02,\n",
      "         1.3875e-02,  4.6371e-03, -4.1601e-02,  1.6008e-02,  3.9200e-03,\n",
      "        -2.6714e-02,  1.7089e-02,  1.2689e-02, -3.2948e-02,  5.9805e-04,\n",
      "        -1.5209e-02, -2.0659e-02,  2.3035e-02, -1.4886e-02, -1.7114e-02,\n",
      "        -8.7224e-03, -3.1439e-02, -2.2899e-03,  3.9228e-02, -2.0310e-02,\n",
      "         3.4840e-02,  1.7478e-02, -5.6636e-03,  2.1206e-02,  7.6029e-03,\n",
      "         2.6843e-02, -9.9170e-03,  3.2315e-02,  1.0239e-02,  1.2266e-02,\n",
      "        -4.3607e-02, -1.5945e-03, -4.1460e-02,  2.2292e-02,  7.9599e-03,\n",
      "         3.7253e-02,  1.7076e-02,  1.4066e-02, -2.8210e-02,  3.7140e-02,\n",
      "         3.8897e-02,  3.1388e-02, -3.7580e-02,  4.2634e-02, -2.7214e-02,\n",
      "         3.4931e-02, -2.5598e-02, -5.5835e-03,  8.3988e-03,  2.2569e-02,\n",
      "        -6.5762e-03,  1.2878e-02,  2.8412e-02, -2.1962e-02, -2.2196e-02,\n",
      "        -1.8291e-02, -3.7446e-02,  2.0958e-02,  3.4629e-02, -4.0189e-02,\n",
      "        -2.8465e-02, -6.7563e-03, -3.5228e-02, -1.3178e-02,  2.7376e-02,\n",
      "        -3.6505e-02,  9.6425e-03, -3.8505e-02, -3.5175e-02,  2.4472e-02,\n",
      "        -2.9303e-04, -2.4456e-02, -1.9947e-03, -3.5710e-02, -3.6998e-02,\n",
      "         1.0814e-03, -1.4302e-02, -8.1771e-03,  2.3509e-02, -4.4086e-02,\n",
      "        -6.0948e-03,  2.0816e-02,  3.4353e-02, -1.4344e-02,  4.3040e-02,\n",
      "         4.3445e-02,  1.1666e-02,  1.1393e-02,  1.5811e-02,  2.2940e-02,\n",
      "        -4.0904e-02, -9.8304e-04,  3.6053e-02, -1.3012e-02, -2.2612e-02,\n",
      "        -8.2351e-03, -2.1675e-02, -3.4542e-02, -4.8833e-04,  5.7909e-03,\n",
      "         1.2883e-02,  2.3061e-02, -2.6718e-02, -4.1337e-02,  1.9644e-02,\n",
      "        -2.9915e-02, -2.8460e-02], requires_grad=True))\n",
      "('decoder.layers.1.ff.linear_1.weight', Parameter containing:\n",
      "tensor([[ 0.0338, -0.0409,  0.0289,  ...,  0.0062, -0.0104, -0.0157],\n",
      "        [ 0.0087, -0.0074,  0.0381,  ...,  0.0268, -0.0135, -0.0102],\n",
      "        [-0.0293,  0.0431,  0.0369,  ..., -0.0364,  0.0330, -0.0008],\n",
      "        ...,\n",
      "        [ 0.0417,  0.0466, -0.0298,  ...,  0.0011,  0.0459, -0.0110],\n",
      "        [ 0.0381, -0.0270, -0.0098,  ..., -0.0459, -0.0162, -0.0047],\n",
      "        [-0.0482, -0.0250, -0.0434,  ...,  0.0280,  0.0355,  0.0239]],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.1.ff.linear_1.bias', Parameter containing:\n",
      "tensor([ 0.0384, -0.0290, -0.0322,  ...,  0.0400, -0.0094, -0.0251],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.1.ff.linear_2.weight', Parameter containing:\n",
      "tensor([[ 0.0401,  0.0317,  0.0219,  ..., -0.0418, -0.0221,  0.0040],\n",
      "        [-0.0246, -0.0174, -0.0311,  ...,  0.0005,  0.0456, -0.0088],\n",
      "        [ 0.0191,  0.0344,  0.0123,  ...,  0.0021,  0.0353,  0.0345],\n",
      "        ...,\n",
      "        [-0.0259,  0.0252, -0.0130,  ...,  0.0184, -0.0049, -0.0404],\n",
      "        [-0.0161, -0.0016, -0.0363,  ...,  0.0374, -0.0124,  0.0422],\n",
      "        [ 0.0160, -0.0318, -0.0362,  ...,  0.0451,  0.0154,  0.0211]],\n",
      "       requires_grad=True))\n",
      "('decoder.layers.1.ff.linear_2.bias', Parameter containing:\n",
      "tensor([-4.0199e-03,  4.9445e-04, -3.9172e-03,  2.4727e-03, -7.2851e-03,\n",
      "         3.2565e-03,  1.3141e-02, -1.2643e-02,  1.8128e-02, -7.0269e-03,\n",
      "         4.2464e-04,  3.9034e-03,  1.6957e-02, -1.5771e-02, -1.0167e-02,\n",
      "         1.1240e-02,  1.7235e-02, -2.0048e-02, -6.9507e-03, -1.4854e-02,\n",
      "        -1.8692e-02,  1.8861e-02,  1.4123e-02,  1.5159e-02, -1.6544e-02,\n",
      "         5.7571e-03,  8.4974e-03, -1.5382e-02,  1.1530e-02, -1.0514e-02,\n",
      "        -1.7382e-02, -2.1732e-02,  9.8501e-03,  1.9825e-02,  2.0286e-02,\n",
      "        -9.1625e-03,  8.0442e-03,  1.4489e-02,  6.5699e-03,  1.5260e-03,\n",
      "         1.4166e-02, -8.5941e-03,  2.9449e-03,  3.5485e-03,  8.1282e-03,\n",
      "        -6.4223e-03,  1.1488e-02,  6.8673e-03,  1.5951e-02, -1.4181e-02,\n",
      "         8.1290e-03, -2.0260e-02,  1.3747e-02, -1.6542e-02, -2.2091e-02,\n",
      "         4.8278e-03, -6.5973e-03, -3.2997e-03, -5.8900e-03,  3.5182e-03,\n",
      "         2.1547e-03, -2.0211e-02, -1.2798e-02, -2.7538e-03, -1.1926e-02,\n",
      "         1.4928e-02,  1.9336e-02,  2.6905e-03, -1.4914e-03, -3.0465e-03,\n",
      "        -6.6079e-03, -1.1373e-02, -1.3177e-02,  5.6175e-03, -2.8976e-03,\n",
      "         9.6631e-03, -1.2687e-02, -1.4496e-02,  1.0473e-02, -4.9142e-03,\n",
      "        -2.2024e-03,  1.0632e-02,  3.1562e-03, -1.2723e-02,  1.2751e-02,\n",
      "         1.0730e-02,  5.9565e-03,  1.9626e-02,  2.2094e-02,  1.0407e-02,\n",
      "        -7.6022e-03, -2.7055e-05, -7.3672e-03,  1.6808e-02, -2.0677e-02,\n",
      "         4.7542e-04, -5.3157e-03, -1.1312e-02,  1.5082e-02,  1.7224e-02,\n",
      "        -5.2409e-03,  5.7623e-03,  1.6166e-03,  4.0550e-03,  8.9260e-03,\n",
      "         2.4608e-03,  1.4025e-02, -1.4796e-02, -1.4235e-02,  7.9003e-03,\n",
      "         1.5542e-02, -1.9148e-02,  1.9081e-02, -2.1613e-02,  2.7593e-03,\n",
      "        -5.6320e-03,  2.0658e-02,  1.5888e-02,  1.9595e-02, -1.2082e-02,\n",
      "        -1.6910e-02, -2.0334e-02,  1.1168e-02, -8.6940e-04,  1.7961e-02,\n",
      "        -8.1677e-03,  1.3020e-02, -3.5910e-03, -9.2135e-03,  2.1471e-02,\n",
      "         1.9926e-02,  1.2801e-02,  1.4551e-02,  6.0618e-03, -1.0529e-02,\n",
      "         2.1950e-02, -2.0453e-02,  1.4784e-02, -1.4081e-02,  2.1564e-02,\n",
      "         3.5225e-03, -5.8149e-03, -1.6047e-02,  1.3413e-02, -1.1337e-02,\n",
      "         1.5108e-02, -4.4502e-03,  1.0052e-02,  1.7180e-02,  1.9942e-02,\n",
      "         4.8717e-03, -1.9814e-02, -6.8282e-03, -6.5165e-05, -5.5513e-03,\n",
      "         1.9182e-02, -9.3982e-03,  1.5634e-02, -1.6869e-02, -3.7324e-03,\n",
      "        -1.8274e-02, -5.4519e-03,  2.0650e-02, -1.7744e-02,  2.6689e-03,\n",
      "        -1.3575e-02, -5.5121e-03, -4.2633e-03,  9.3554e-03,  1.6953e-02,\n",
      "         1.8315e-02, -2.0903e-02, -4.6363e-03, -1.1099e-02, -6.8866e-03,\n",
      "        -5.4197e-03, -2.0986e-02, -2.6764e-03,  8.7599e-03,  5.2748e-03,\n",
      "         6.1005e-03, -6.1615e-03, -9.1340e-03,  8.4477e-03,  1.7603e-03,\n",
      "         1.0896e-02,  1.2022e-02, -8.3808e-03,  2.1454e-02,  1.7656e-02,\n",
      "         8.6594e-03,  1.3527e-03,  1.2885e-02,  6.8660e-03, -1.5086e-02,\n",
      "        -5.4202e-03, -1.8641e-02,  6.8052e-03,  9.4074e-03,  2.0030e-02,\n",
      "         5.5149e-03,  1.0534e-04,  7.6480e-03, -1.5204e-04, -7.9349e-07,\n",
      "        -2.0287e-02,  2.1385e-02, -1.9136e-02,  1.1639e-02,  9.7419e-03,\n",
      "        -4.4113e-03,  1.8748e-02, -1.4318e-02, -1.4980e-02, -1.2158e-03,\n",
      "         5.2835e-03, -1.4733e-02,  2.7585e-03, -2.0830e-02, -1.8405e-02,\n",
      "         1.5495e-03, -1.1267e-02,  2.3357e-03,  2.0864e-02,  2.8660e-03,\n",
      "        -1.0960e-03,  1.2216e-02, -1.8672e-02,  1.7592e-03,  1.2764e-02,\n",
      "         1.2132e-02, -7.1912e-03,  1.6800e-02,  1.3069e-02, -1.3148e-02,\n",
      "         1.4573e-02,  2.8715e-03,  2.0803e-02, -7.2615e-03, -1.0161e-02,\n",
      "         1.2753e-02, -2.0779e-02,  1.7249e-02, -1.4401e-02, -1.9130e-02,\n",
      "        -2.0988e-02,  1.5178e-02,  1.3086e-02, -7.8106e-03,  1.7013e-02,\n",
      "         3.7028e-03,  7.2102e-03, -5.2252e-04,  9.5734e-03,  1.1858e-02,\n",
      "         1.4477e-02,  1.6781e-02,  8.1645e-03, -1.0204e-02, -2.1199e-02,\n",
      "         1.3487e-02,  1.4331e-02, -1.9201e-02,  1.6801e-02, -5.1640e-03,\n",
      "        -1.7426e-03, -3.8452e-03, -1.2795e-02,  1.0413e-02,  1.3966e-02,\n",
      "         1.8683e-02,  1.6941e-03,  9.3503e-03,  1.6162e-02, -6.3178e-03,\n",
      "         4.4133e-03, -3.4203e-03,  1.0484e-02, -7.1017e-04,  9.4598e-03,\n",
      "        -2.0204e-02, -2.0107e-02,  2.5083e-03,  8.3851e-03, -1.9862e-02,\n",
      "         4.8527e-03,  1.4231e-02, -1.7422e-02, -2.0956e-02, -1.8594e-02,\n",
      "        -2.9168e-03, -3.4994e-03, -2.6037e-03, -1.2180e-02,  1.5206e-02,\n",
      "         9.9711e-03, -2.1857e-02, -2.4316e-03,  1.5755e-02,  2.5229e-03,\n",
      "        -1.4600e-03,  6.4640e-03, -9.9567e-03, -7.6713e-03,  4.6481e-03,\n",
      "        -1.3432e-02, -9.6215e-04, -1.7980e-02, -4.3536e-03, -2.1856e-02,\n",
      "         1.3498e-02,  2.1612e-02, -2.1612e-02, -1.5483e-03,  1.5359e-02,\n",
      "         7.9671e-03,  6.7616e-03,  1.6998e-02, -2.1767e-02, -1.0319e-02,\n",
      "         1.0907e-03, -8.5077e-03, -1.7858e-02,  1.4256e-02,  2.0443e-02,\n",
      "        -5.3807e-03,  1.2593e-02,  1.6884e-02, -5.8211e-03, -1.6715e-02,\n",
      "        -1.5428e-03, -1.1626e-02,  1.0790e-02, -1.2740e-02,  1.6344e-03,\n",
      "         3.6901e-03,  1.3699e-03,  9.7039e-03,  2.0418e-02, -2.0491e-02,\n",
      "         8.6580e-03,  1.3925e-02, -3.6534e-03,  1.4250e-02,  6.7095e-03,\n",
      "         1.7591e-02,  1.2725e-02, -1.7933e-02,  3.7724e-03, -2.1177e-03,\n",
      "        -1.3914e-02, -1.0259e-02, -1.4303e-02,  1.9562e-02,  1.0596e-02,\n",
      "         1.4452e-02, -1.6153e-02,  1.1114e-02,  1.4265e-02,  5.0792e-05,\n",
      "         7.9900e-03,  1.0635e-02,  1.9737e-02,  7.2163e-03,  1.6895e-02,\n",
      "        -9.0557e-03, -1.9822e-02, -2.1515e-02,  8.5566e-03, -1.6989e-02,\n",
      "         2.0656e-02,  6.2071e-03, -1.3287e-02,  1.0540e-02,  5.2316e-03,\n",
      "         4.4203e-03,  6.2810e-03,  1.9001e-03, -1.6010e-03,  9.3815e-03,\n",
      "        -2.2253e-03,  1.5361e-02, -1.7092e-02,  4.2335e-03,  5.8510e-04,\n",
      "        -1.1602e-03, -1.6037e-02, -2.5539e-03,  4.6269e-03,  1.6918e-02,\n",
      "         1.5227e-02, -3.7441e-03, -5.9720e-03, -1.9809e-02,  9.7906e-03,\n",
      "         2.1430e-02,  9.0645e-03,  1.3000e-02, -1.1856e-02,  1.0904e-03,\n",
      "         1.7781e-02, -9.5568e-03, -9.6942e-03, -8.2937e-03,  2.1392e-02,\n",
      "        -1.7484e-02, -1.6180e-02, -2.0065e-02, -1.8979e-02, -1.3277e-02,\n",
      "        -1.6161e-02,  1.9679e-02, -5.1667e-03, -4.4583e-03, -1.7812e-02,\n",
      "         1.8093e-02,  1.4854e-02, -1.7533e-02,  1.4009e-03, -2.1413e-02,\n",
      "         1.7183e-02, -2.0979e-02, -8.5615e-04, -5.5006e-04, -1.2955e-02,\n",
      "        -7.8758e-03,  1.9492e-02, -5.8536e-03, -1.6261e-02,  6.4225e-03,\n",
      "        -2.0637e-02, -2.0941e-02,  7.3173e-03, -2.1080e-02, -1.3876e-02,\n",
      "        -9.3030e-03, -4.8558e-03, -1.3596e-02, -9.7747e-03,  1.3873e-02,\n",
      "         3.8516e-03, -1.8572e-03,  1.1640e-02,  1.0089e-04, -5.8198e-03,\n",
      "        -1.2926e-02,  1.9318e-02,  1.0793e-02,  2.0721e-02,  3.0270e-03,\n",
      "        -2.2447e-03, -4.3719e-03,  1.9900e-02, -1.0645e-02, -2.1897e-02,\n",
      "        -4.3418e-03,  1.0624e-03,  1.8304e-02,  1.6515e-02,  2.0251e-02,\n",
      "         7.7765e-03, -6.3551e-03,  2.0200e-02, -1.3868e-03,  1.6538e-02,\n",
      "         1.3912e-02,  2.0116e-03,  2.0886e-02,  2.1243e-02,  1.4439e-02,\n",
      "        -1.3165e-02,  1.9357e-02, -5.8192e-03, -9.5289e-03,  1.2441e-02,\n",
      "         2.1788e-02,  5.9322e-03,  1.5761e-02,  1.7161e-02, -1.4129e-02,\n",
      "         1.0596e-02,  1.5234e-02,  2.1129e-02, -2.0502e-02, -5.1833e-03,\n",
      "        -1.0743e-02, -2.1228e-02,  9.5807e-04,  1.2438e-02, -1.5239e-02,\n",
      "        -1.9277e-02, -5.1763e-03, -9.0465e-03, -5.5660e-04, -1.9258e-02,\n",
      "         4.2584e-03,  5.2707e-03,  2.3938e-03, -1.4657e-02, -7.2563e-03,\n",
      "        -1.4100e-05, -5.2961e-04,  1.5059e-02, -3.4171e-03,  3.8046e-03,\n",
      "         1.2397e-02,  2.0989e-02, -1.1071e-02, -1.3993e-02, -1.5584e-02,\n",
      "        -6.4148e-03,  1.1393e-02], requires_grad=True))\n",
      "('decoder.norm.alpha', Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True))\n",
      "('decoder.norm.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))\n",
      "('out.weight', Parameter containing:\n",
      "tensor([[-0.0394,  0.0363,  0.0085,  ..., -0.0412, -0.0297, -0.0322],\n",
      "        [-0.0073,  0.0395,  0.0371,  ...,  0.0039,  0.0277, -0.0005],\n",
      "        [-0.0262, -0.0039,  0.0038,  ...,  0.0034, -0.0005,  0.0301],\n",
      "        ...,\n",
      "        [ 0.0033,  0.0083,  0.0417,  ..., -0.0167, -0.0166, -0.0328],\n",
      "        [ 0.0031, -0.0297,  0.0212,  ..., -0.0226, -0.0298,  0.0249],\n",
      "        [-0.0211, -0.0018, -0.0210,  ..., -0.0293,  0.0129,  0.0034]],\n",
      "       requires_grad=True))\n",
      "('out.bias', Parameter containing:\n",
      "tensor([-0.0074,  0.0135,  0.0012,  ...,  0.0156,  0.0379, -0.0273],\n",
      "       requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for p in model.named_parameters():\n",
    "    print(p)\n",
    "\n",
    "# Viewing the model parameters which would be subjected to the optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (200) must match the size of tensor b (80) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Data\\tutorials\\Data Science\\Projects\\language-translator\\language_translation.ipynb Cell 56'\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000055?line=37'>38</a>\u001b[0m                 total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000055?line=38'>39</a>\u001b[0m                 temp \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000055?line=40'>41</a>\u001b[0m train_model(\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32md:\\Data\\tutorials\\Data Science\\Projects\\language-translator\\language_translation.ipynb Cell 56'\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(epochs, print_every)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000055?line=18'>19</a>\u001b[0m \u001b[39m# create function to make masks using mask code above\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000055?line=19'>20</a>\u001b[0m src_mask, trg_mask \u001b[39m=\u001b[39m create_masks(source, target_lang_input)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000055?line=21'>22</a>\u001b[0m preds \u001b[39m=\u001b[39m model(source, target_lang_input, src_mask, trg_mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000055?line=23'>24</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000055?line=25'>26</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(preds, target_lang_output\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat64))\n",
      "File \u001b[1;32md:\\Data\\tutorials\\Data Science\\Projects\\language-translator\\transformenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\Data\\tutorials\\Data Science\\Projects\\language-translator\\language_translation.ipynb Cell 53'\u001b[0m in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, trg, src_mask, trg_mask)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000052?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src, trg, src_mask, trg_mask):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000052?line=8'>9</a>\u001b[0m     e_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder\u001b[39m.\u001b[39;49mforward(src, src_mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000052?line=9'>10</a>\u001b[0m     d_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mforward(trg, e_outputs, src_mask, trg_mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000052?line=10'>11</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(d_output)\n",
      "\u001b[1;32md:\\Data\\tutorials\\Data Science\\Projects\\language-translator\\language_translation.ipynb Cell 52'\u001b[0m in \u001b[0;36mEncoder.forward\u001b[1;34m(self, src, mask)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000051?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src, mask):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000051?line=9'>10</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed(src)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000051?line=10'>11</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpe(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000051?line=11'>12</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000051?line=12'>13</a>\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i](x, mask)\n",
      "File \u001b[1;32md:\\Data\\tutorials\\Data Science\\Projects\\language-translator\\transformenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\Data\\tutorials\\Data Science\\Projects\\language-translator\\language_translation.ipynb Cell 42'\u001b[0m in \u001b[0;36mPositionalEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000041?line=24'>25</a>\u001b[0m \u001b[39m#add constant to embedding\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000041?line=25'>26</a>\u001b[0m seq_len \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000041?line=27'>28</a>\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39;49m Variable(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpe[:seq_len], requires_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/tutorials/Data%20Science/Projects/language-translator/language_translation.ipynb#ch0000041?line=29'>30</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (200) must match the size of tensor b (80) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "def train_model(epochs, print_every=10):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    start = time.time()\n",
    "    temp = start\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for en_train_sequence, target_lang_input, target_lang_output in zip(en_train_sequences, decoder_inputs_train, decoder_outputs_train):\n",
    "            source = torch.from_numpy(en_train_sequence)\n",
    "            target_lang_input = torch.from_numpy(target_lang_input)\n",
    "            target_lang_output = torch.from_numpy(target_lang_output)\n",
    "            target_lang_output = F.one_hot(target_lang_output.to(torch.int64), num_classes=hindi_vocab_size)\n",
    "            \n",
    "            # create function to make masks using mask code above\n",
    "            src_mask, trg_mask = create_masks(source, target_lang_input)\n",
    "            \n",
    "            preds = model(source, target_lang_input, src_mask, trg_mask)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = F.cross_entropy(preds, target_lang_output.to(torch.float64))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(loss.data)\n",
    "            \n",
    "            total_loss += loss.data.item()\n",
    "\n",
    "            if (epoch + 1) % print_every == 0:\n",
    "                loss_avg = total_loss / print_every\n",
    "                print(\"time = %dm, epoch %d, loss = %.3f, %ds per %d iters\" % ((time.time() - start) // 60,\n",
    "                epoch + 1, loss_avg, time.time() - temp, print_every))\n",
    "                total_loss = 0\n",
    "                temp = time.time()\n",
    "\n",
    "train_model(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('transformenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8108e91934df90e45fad20689b2a3fa321c318a803faee73c02e105c89b041dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
